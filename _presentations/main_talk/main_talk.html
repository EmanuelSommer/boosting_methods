<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>main_talk.utf8</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: left, middle, inverse, hide-logo






### Boosting methods for regression
&lt;!-- 
English title page
--&gt;

#### Emanuel Sommer
#### 2021-05-20


&lt;center&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;img src="img/maTUM_w.png" width="100"&gt;
&lt;br&gt;
&lt;br&gt;
Department of Mathematics
&lt;br&gt;
Technical University of Munich
&lt;/center&gt;


---
## Basic Notation

Training data set:

`$$\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}$$`

Response: `\(y_i \in \mathbb{R}\)` like the burnout rate or the charges.

Predictors: `\(x_i \in \mathbb{R}^m\)`

$$
y = (y_1, \dots, y_N)^T
\quad
x = (x_1, \dots, x_N)^T
$$

---
## Previously

.pull-left[
### CART

&lt;img src="img/tree_illustration.png" width="45%" style="display: block; margin: auto;" /&gt;

Let `\(\mathcal{T}\)` be the space of CART models.

`$$t(x, \gamma, R) = \sum_{j=1}^J \gamma_j I(x \in R_j) \quad \text{for  } t \in \mathcal{T}$$`
with `\(R_j\)` being distinct regions of the predictor space and `\(\gamma_j\)` the prediction w.r.t. the region.
]

--

.pull-right[
### Random forest

&lt;img src="img/rf_illustration.png" width="70%" style="display: block; margin: auto;" /&gt;

Build many deep trees in parallel and average over the individual predictions. Randomness is induced to decorrelate the trees.

]
---
background-image: url("https://media.giphy.com/media/cRH5deQTgTMR2/giphy.gif")
background-size: cover
class: center, bottom, hide-logo


---

## The core idea of boosting

**Sequentially** build weak learners that are ultimately combined in an additive fashion. Each additional learner should improve the total model.

&lt;img src="img/boost_illustration.png" width="50%" style="display: block; margin: auto;" /&gt;

**The general additive model:**

`$$\hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}$$`

with `\(\mathcal{F}\)` being the space of learning algorithms. From now on we use `\(\mathcal{T}\)`.

---

## The fitting approach

To fit such a model one uses **Forward Stagewise Additive Modeling**.

* Sequentially add `\(t \in \mathcal{T}\)` to the current model.
* Do not change the parameters of the previous trees.

`$$\phi_k(x) = \phi_{k-1}(x) + t_k(x, \gamma^{(k)}, R^{(k)})$$`

Each tree that is added should solve the following optimization problem:

`$$(\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R))$$`
where `\(L\)` is a convex and differentiable loss function.

---

## Connection with Gradient descent

**General minimization problem:**

$$
s_{opt} = argmin_sf(s) \quad s \in \mathbb{R}^N
$$

With `\(f\)` is differentiable and `\(\lambda \in \mathbb{R}\)` a scalar.

**Gradient descent in a nutshell:** 

$$
s^{(k)} = s^{(k-1)} - \lambda \bigg[\frac{\partial f(s)}{\partial s}\bigg]_{s=s^{(k-1)}}
$$
--

This looks quite similar to the structure of our boosting model:

`$$\phi_k(x) = \phi_{k-1}(x) + t_k(x, \gamma^{(k)}, R^{(k)})$$`

Corresponding parts:

* The loss function `\(L\)` to the function `\(f\)`
* The predictions `\(\phi_k(x)\)` to the input `\(s^{(k)}\)`
* The predictions of the tree `\(t_k\)` to the gradient of `\(f\)`

---

## Tree-based gradient boosting

**Algorithm:**

1.  Initialize `\(\phi_0(x)\)` as a singular node tree.

2.  For `\(k = 1\)` to `\(K\)` do:

    -   For `\(i = 1\)` to `\(N\)` compute:

        `\(g^{(k)}_{i} = \bigg[\frac{\partial L(y_i, \phi(x_i))}{\partial \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}\)`

    -   Fit a regression tree by least squares to the outcome vector `\(-g^{(k)}\)` in order to get the `\(J^{(k)}\)` distinct regions `\(\tilde{R}^{(k)}_j\)`.

    -   For each of these `\(J^{(k)}\)` regions perform a line search in order to compute the leaf predictions `\(\tilde{\gamma}^{(k)}_{j}\)`.

    -   Set `\(\phi_k(x) = \phi_{k-1}(x) + t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)\)` with `\(t \in \mathcal{T}\)`

&lt;!--
## Which loss to pick?

### `\(L_2\)` loss

âž– Sensitive to outliers.

âž• Simplifications possible as the gradient is the residuals.

.pull-left[
### Alternatives

* `\(L_1\)` loss
* Huber loss
]

.pull-right[
### Comparison

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/huber_loss.png" width="90%" style="display: block; margin: auto auto auto 0;" /&gt;

]
--&gt;
---
## Combat Overfitting

### Early stopping

Monitor the loss on one or many (CV) validation sets and stop the addition of trees when the validation loss increases.

--

### Shrinkage

Introduce a learning rate `\(\eta\)` that scales the contribution of the new model. 

`$$\phi_k(x) = \phi_{k-1}(x) + \eta * t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)$$`

--

### Subsampling

**Row-subsampling:** In each step use only a fraction of the training data.

**Column-subsampling:** In each step use only a fraction of the training features.

---
class: inverse, hide-logo, middle, center



&lt;img src="img/xgb_hist_annot.jpg" width="80%" style="display: block; margin: auto;" /&gt;

---

## An implementation: XGBoost

&lt;br&gt;
&lt;center&gt;
** A highly scalable end-to-end tree boosting system.**
&lt;/center&gt;
&lt;br&gt;

### Core strengths

* Strong regularization
* Parallelized tree building
* Extremely efficient on sparse data (default directions)
* Cache aware algorithm (clever data structure)

---
## XGBoost vs overfitting 

### Regularized Loss

`$$\mathcal{L}(\phi) = L(\phi) + \sum_{k=1}^K \Omega_{\alpha, \lambda, \nu}(t_k)$$`


Where `\(\Omega_{\alpha, \lambda, \nu}\)` is the penalizing term for model complexity.


* `\(\alpha\)` is a `\(L_1\)` regularization parameter on `\(\gamma\)`.
* `\(\lambda\)` is a `\(L_2\)` regularization parameter on `\(\gamma\)`.
* `\(\nu\)` is the minimal loss reduction for terminal partitions.
* Second order approximations of the loss allow for better parallelization.

--

.pull-left[
### Shrinkage

### Subsampling
]

.pull-right[
### Early stopping

### Maximum tree depth
]

---
background-image: url("https://media.giphy.com/media/BpGWitbFZflfSUYuZ9/giphy.gif")
background-size: cover
class: center, bottom, hide-logo


---

## Burnout data set

**Goal:** Predict the burnout rate of an employee.

.left-column[
&lt;br&gt;

**Number of predictors:** 

9

**Training data:** 

17301 rows

**Test data:** 

4325 rows

**Missing:**

14%
]

.right-column[
&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/burn_rate_raw.png" width="90%" style="display: block; margin: auto;" /&gt;
]
---

## Transformed outcome

The transformation with the empirical logit removes the bounds on the target.

`$$log(\frac{x+0.5}{1-x+0.5})$$`

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/burn_rate_trans.png" width="65%" style="display: block; margin: auto;" /&gt;

---
## Mental fatigue score

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/mfs_main.png" width="70%" style="display: block; margin: auto;" /&gt;

Proposed baseline model:

$$
\text{burn_rate}_i = \beta_0 + \beta_1 \text{ mental_fatigue_score}_i + \epsilon_i \quad \text{with }
\epsilon_i \sim N(0,\sigma^2)
$$


&lt;!--
## Two closely related features

&lt;br&gt;

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/designationVsRessources.png" width="80%" style="display: block; margin: auto;" /&gt;

Both features show relevant main effects.
--&gt;
---
## Prepare for takeoff

### Pre-processing / Feature Engineering

* All variables are used without normalization.
* Extract extra features `day of the week` and `month` of the date of joining.
* Dummy-encoding of all nominal variables.

--

### Resampling

**5-fold cross validation** is used for model evaluation e.g. hyperparameter tuning.

&lt;img src="img/cv5_illustration.png" width="70%" style="display: block; margin: auto;" /&gt;

---
## Tuning strategy

1. Large enough number of trees with a small learning rate.

2. Space-filling grid search over sensible ranges of the tree and the basic regularization hyperparameters. 

3. Refine the grid as often as needed and explore further regularization parameters if required.

4. Optional: Iterative searches for single parameters.

&lt;br&gt; 

--

**The ranges used here:**

.pull-left[
`#Trees`: `\(K \in [100,2000]\)`

`Max tree depth`: `\(max(J^{(k)}) \in [1,15]\)`

`Learning rate`: `\(\eta \in [1e^{-10},0.1]\)`
]

.pull-right[
`Loss reduction`: `\(\nu \in [1e^{-10},30]\)`

`Column-subsampling`: `\([1,\text{#features}]\)`

`Row-subsampling`:  `\([0.1,1]\)`
]
---
## Tuning the number of trees

Besides the standard value of six for the maximum tree depth also the influence of considering just stumps is displayed.

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/bout_tuning_stumpsVs6.png" width="90%" style="display: block; margin: auto;" /&gt;

ðŸ‘‰ Fix 1500 trees.

---
## Space-filling grid search

Tuning both the model with the raw outcome and the transformed one with space-filling grids came at a cost.

- 800 combinations.
- 6 million trees.
- 8 hours of parallel computation.

&lt;br&gt;

**The final parameters:**

.pull-left[
`#Trees`: 1500

`Max tree depth`: 4

`Learning rate`: 0.02
]

.pull-right[
`Loss reduction`: roughly 0

`Column-subsampling`: not enabled

`Row-subsampling`:  80%
]

---
## Raw vs transformed target

The two models behaved exactly the same while tuning and show no notable differences in any of the following visualizations so the focus will now be on the raw model.

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/bout_raw_vs_trans.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

---
## Feature importance

The feature importance is basically the relative frequency of the overall model to choose the feature in a split.

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/vip_burn.png" width="80%" style="display: block; margin: auto auto auto 0;" /&gt;

---
## Handling of missing values

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/miss_vals_burn2.png" width="75%" style="display: block; margin: auto auto auto 0;" /&gt;

The default directions of XGBoost seem to work really well.

---
## Final performance

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/final_perf_burn.png" width="80%" style="display: block; margin: auto auto auto 0;" /&gt;

---

## Insurance data set

**Goal:** Predict the medical costs (Charges) billed by health insurance.

.left-column[
&lt;br&gt;

**Number of predictors:** 

6

**Training data:** 

1071 rows

**Test data:** 

267 rows

**Missing:**

None
]


.right-column[
&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/charges_ins.png" width="90%" style="display: block; margin: auto;" /&gt;
]

---
## Prepare for takeoff

- Again all variables are used without normalization.
- Dummy-encoding of all nominal variables.
- Resampling setup with 5-fold CV is the same.
- The tuning strategy and ranges are unchanged.

--

&lt;br&gt;
.center[
&lt;img src="https://media.giphy.com/media/ZNHlhLQb1pm5FYXoWy/giphy.gif" width="300" height="300" /&gt;
]

---
## Tuning the number of trees

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/boost_ins_tune_plot1.png" width="90%" style="display: block; margin: auto;" /&gt;

So a closer look at the region around 500 is advisable.

---
## Tuning the number of trees

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/boost_ins_tune_plot2.png" width="90%" style="display: block; margin: auto;" /&gt;

ðŸ‘‰ Fix 600 trees.

---
## A glimpse at learning rates

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/boost_ins_tune_lrate.png" width="90%" style="display: block; margin: auto;" /&gt;

Thus a too small learning rate can also be a problem.

---
## Space-filling grid search


- 600 combinations.
- 900k trees.
- 30 minutes of parallel computation.

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/tuning_ins_parallelcoord.png" width="90%" style="display: block; margin: auto;" /&gt;

---
## The final parameters


`#Trees`: 600

`Max tree depth`: 3

`Learning rate`: 0.02

`Loss reduction`: 0.03

`Column-subsampling`: 7 of 8

`Row-subsampling`:  80%


---
## Feature importance

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/vip_ins.png" width="90%" style="display: block; margin: auto auto auto 0;" /&gt;


---
## Final Performance

&lt;img src="C:/Users/Emanuel/Documents/Uni/3. MA Semester/Seminar/boosting_methods/_pictures/final_perf_ins.png" width="80%" style="display: block; margin: auto auto auto 0;" /&gt;

---
## Pros 

- **Minimal pre-processing**

- **Flexible enough to detect complex non-linear patterns**

- **Robust to outliers and missing values**

- **Good generalization due to lots of regularization options**

- **Strong predictive power**

## Cons

- **Not easily explainable**

- **Computationally demanding (especially the hyperparameter tuning)**

- **Better if there are lots of observations**


---
class: inverse, center, middle, hide-logo


## Thank you and happy boosting!

Here the link to the bookdown project TBD

Basically everything regarding my work for this seminar can be found there. 



---
class: inverse, center, middle, hide-logo


## Discussion





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(img/TUM.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 80px;
  height: 80px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
