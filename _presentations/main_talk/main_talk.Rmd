---
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      after_body: logo_placer.html
    seal: false
---
class: left, middle, inverse, hide-logo

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
# Here the slides theme (TUM blue) is set and the fonts 
# feel free to change them
library(xaringanthemer)
style_mono_accent(
  base_color = "#0073C3",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```


### Boosting methods for regression
<!-- 
English title page
-->

#### Emanuel Sommer
#### `r Sys.Date()`


<center>
<br><br><br>
<img src="img/maTUM_w.png" width="100">
<br>
<br>
Department of Mathematics
<br>
Technical University of Munich
</center>


---
## Basic Notation

Training data set:

$$\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}$$

Response: $y_i \in \mathbb{R}$ like the burnout rate or the charges.

Predictors: $x_i \in \mathbb{R}^m$

$$
y = (y_1, \dots, y_N)^T
\quad
x = (x_1, \dots, x_N)^T
$$

---
## Previously

.pull-left[
### CART

```{r, echo=FALSE, out.width="45%", fig.align='center'}
knitr::include_graphics("img/tree_illustration.png")
```

Let $\mathcal{T}$ be the space of CART models.

$$t(x, \gamma, R) = \sum_{j=1}^J \gamma_j I(x \in R_j) \quad \text{for  } t \in \mathcal{T}$$
with $R_j$ being distinct regions of the predictor space and $\gamma_j$ the prediction w.r.t. the region.
]

--

.pull-right[
### Random forest

```{r, echo=FALSE, out.width="70%", fig.align='center'}
knitr::include_graphics("img/rf_illustration.png")
```

Build many deep trees in parallel and average over the individual predictions. Randomness is induced to decorrelate the trees.

]
---
background-image: url("https://media.giphy.com/media/cRH5deQTgTMR2/giphy.gif")
background-size: cover
class: center, bottom, hide-logo


---

## The core idea of boosting

**Sequentially** build weak learners that are ultimately combined in an additive fashion. Each additional learner should improve the total model.

```{r, echo=FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("img/boost_illustration.png")
```

**The general additive model:**

$$\hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}$$

with $\mathcal{F}$ being the space of learning algorithms. From now on we use $\mathcal{T}$.

---

## The fitting approach

To fit such a model one uses **Forward Stagewise Additive Modeling**.

* Sequentially add $t \in \mathcal{T}$ to the current model.
* Do not change the parameters of the previous trees.

$$\phi_k(x) = \phi_{k-1}(x) + t_k(x, \gamma^{(k)}, R^{(k)})$$

Each tree that is added should solve the following optimization problem:

$$(\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R))$$
where $L$ is a convex and differentiable loss function.

---

## Connection with Gradient descent

**General minimization problem:**

$$
s_{opt} = argmin_sf(s) \quad s \in \mathbb{R}^N
$$

With $f$ is differentiable and $\lambda \in \mathbb{R}$ a scalar.

**Gradient descent in a nutshell:** 

$$
s^{(k)} = s^{(k-1)} - \lambda \bigg[\frac{\partial f(s)}{\partial s}\bigg]_{s=s^{(k-1)}}
$$
--

This looks quite similar to the structure of our boosting model:

$$\phi_k(x) = \phi_{k-1}(x) + t_k(x, \gamma^{(k)}, R^{(k)})$$

Corresponding parts:

* The loss function $L$ to the function $f$
* The predictions $\phi_k(x)$ to the input $s^{(k)}$
* The predictions of the tree $t_k$ to the gradient of $f$

---

## Tree-based gradient boosting

**Algorithm:**

1.  Initialize $\phi_0(x)$ as a singular node tree.

2.  For $k = 1$ to $K$ do:

    -   For $i = 1$ to $N$ compute:

        $g^{(k)}_{i} = \bigg[\frac{\partial L(y_i, \phi(x_i))}{\partial \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}$

    -   Fit a regression tree by least squares to the outcome vector $-g^{(k)}$ in order to get the $J^{(k)}$ distinct regions $\tilde{R}^{(k)}_j$.

    -   For each of these $J^{(k)}$ regions perform a line search in order to compute the leaf predictions $\tilde{\gamma}^{(k)}_{j}$.

    -   Set $\phi_k(x) = \phi_{k-1}(x) + t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)$ with $t \in \mathcal{T}$

<!--
## Which loss to pick?

### $L_2$ loss

`r emo::ji("minus")` Sensitive to outliers.

`r emo::ji("plus")` Simplifications possible as the gradient is the residuals.

.pull-left[
### Alternatives

* $L_1$ loss
* Huber loss
]

.pull-right[
### Comparison

```{r, echo=FALSE, out.width="90%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/huber_loss.png"))
```

]
-->
---
## Combat Overfitting

### Early stopping

Monitor the loss on one or many (CV) validation sets and stop the addition of trees when the validation loss increases.

--

### Shrinkage

Introduce a learning rate $\eta$ that scales the contribution of the new model. 

$$\phi_k(x) = \phi_{k-1}(x) + \eta * t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)$$

--

### Subsampling

**Row-subsampling:** In each step use only a fraction of the training data.

**Column-subsampling:** In each step use only a fraction of the training features.

---
class: inverse, hide-logo, middle, center



```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("img/xgb_hist_annot.jpg")
```

---

## An implementation: XGBoost

<br>
<center>
** A highly scalable end-to-end tree boosting system.**
</center>
<br>

### Core strengths

* Strong regularization
* Parallelized tree building
* Extremely efficient on sparse data (default directions)
* Cache aware algorithm (clever data structure)

---
## XGBoost vs overfitting 

### Regularized Loss

$$\mathcal{L}(\phi) = L(\phi) + \sum_{k=1}^K \Omega_{\alpha, \lambda, \nu}(t_k)$$


Where $\Omega_{\alpha, \lambda, \nu}$ is the penalizing term for model complexity.


* $\alpha$ is a $L_1$ regularization parameter on $\gamma$.
* $\lambda$ is a $L_2$ regularization parameter on $\gamma$.
* $\nu$ is the minimal loss reduction for terminal partitions.
* Second order approximations of the loss allow for better parallelization.

--

.pull-left[
### Shrinkage

### Subsampling
]

.pull-right[
### Early stopping

### Maximum tree depth
]

---
background-image: url("https://media.giphy.com/media/BpGWitbFZflfSUYuZ9/giphy.gif")
background-size: cover
class: center, bottom, hide-logo


---
## R setup

.pull-left[
```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("img/tidyverse_hex.png")
```
]
.pull-right[
```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("img/tidymodels_hex.png")
```
]

<br>
Amazing book: **Tidy modeling with R**

---
## Burnout data set

**Goal:** Predict the burnout rate of an employee.

.left-column[
<br>

**Number of predictors:** 

9

**Training data:** 

17301 rows

**Test data:** 

4325 rows

**Missing:**

14%
]

.right-column[
```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/burn_rate_raw.png"))
```
]
---

## Transformed outcome

The transformation with the empirical logit removes the bounds on the target.

$$log(\frac{x+0.5}{1-x+0.5})$$

```{r, echo=FALSE, out.width="65%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/burn_rate_trans.png"))
```

---
## Mental fatigue score

```{r, echo=FALSE, out.width="70%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/mfs_main.png"))
```

Proposed baseline model:

$$
\text{burn_rate}_i = \beta_0 + \beta_1 \text{ mental_fatigue_score}_i + \epsilon_i \quad \text{with }
\epsilon_i \sim N(0,\sigma^2)
$$


<!--
## Two closely related features

<br>

```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/designationVsRessources.png"))
```

Both features show relevant main effects.
-->
---
## Prepare for takeoff

### Pre-processing / Feature Engineering

* All variables are used without normalization.
* Extract extra features `day of the week` and `month` of the date of joining.
* Dummy-encoding of all nominal variables.

--

### Resampling

**5-fold cross validation** is used for model evaluation e.g. hyperparameter tuning.

```{r, echo=FALSE, out.width="70%", fig.align='center'}
knitr::include_graphics("img/cv5_illustration.png")
```

---
## Tuning strategy

1. Large enough number of trees with a small learning rate.

2. Space-filling grid search over sensible ranges of the tree and the basic regularization hyperparameters. 

3. Refine the grid as often as needed and explore further regularization parameters if required.

4. Optional: Iterative searches for single parameters.

<br> 

--

**The ranges used here:**

.pull-left[
`#Trees`: $K \in [100,2000]$

`Max tree depth`: $max(J^{(k)}) \in [1,15]$

`Learning rate`: $\eta \in [1e^{-10},0.1]$
]

.pull-right[
`Loss reduction`: $\nu \in [1e^{-10},30]$

`Column-subsampling`: $[1,\text{#features}]$

`Row-subsampling`:  $[0.1,1]$
]
---
## Tuning the number of trees

Besides the standard value of six for the maximum tree depth also the influence of considering just stumps is displayed.

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/bout_tuning_stumpsVs6.png"))
```

`r emo::ji("backhand index pointing right")` Fix 1500 trees.

---
## Space-filling grid search

Tuning both the model with the raw outcome and the transformed one with space-filling grids came at a cost.

- 800 combinations.
- 6 million trees.
- 8 hours of parallel computation.

<br>

**The final parameters:**

.pull-left[
`#Trees`: 1500

`Max tree depth`: 4

`Learning rate`: 0.02
]

.pull-right[
`Loss reduction`: roughly 0

`Column-subsampling`: not enabled

`Row-subsampling`:  80%
]

---
## Raw vs transformed target

The two models behaved exactly the same while tuning and show no notable differences in any of the following visualizations so the focus will now be on the raw model.

```{r, echo=FALSE, out.width="70%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/bout_raw_vs_trans.png"))
```

---
## Feature importance

The feature importance is basically the gain w.r.t. the loss of the feature over all splits.

```{r, echo=FALSE, out.width="80%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/vip_burn.png"))
```

---
## Handling of missing values

```{r, echo=FALSE, out.width="75%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/miss_vals_burn2.png"))
```

The default directions of XGBoost seem to work well.

---
## Final performance

```{r, echo=FALSE, out.width="80%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/final_perf_burn.png"))
```

---

## Insurance data set

**Goal:** Predict the medical costs (Charges) billed by health insurance.

.left-column[
<br>

**Number of predictors:** 

6

**Training data:** 

1071 rows

**Test data:** 

267 rows

**Missing:**

None
]


.right-column[
```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/charges_ins.png"))
```
]

---
## Prepare for takeoff

- Again all variables are used without normalization.
- Dummy-encoding of all nominal variables.
- Resampling setup with 5-fold CV is the same.
- The tuning strategy and ranges are unchanged.

--

<br>
.center[
<img src="https://media.giphy.com/media/ZNHlhLQb1pm5FYXoWy/giphy.gif" width="300" height="300" />
]

---
## Tuning the number of trees

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/boost_ins_tune_plot1.png"))
```

So a closer look at the region around 500 is advisable.

---
## Tuning the number of trees

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/boost_ins_tune_plot2.png"))
```

`r emo::ji("backhand index pointing right")` Fix 600 trees.

---
## A glimpse at learning rates

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/boost_ins_tune_lrate.png"))
```

Thus a too small learning rate can also be a problem.

---
## Space-filling grid search


- 600 combinations.
- 900k trees.
- 30 minutes of parallel computation.

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/tuning_ins_parallelcoord.png"))
```

---
## The final parameters


`#Trees`: 600

`Max tree depth`: 3

`Learning rate`: 0.02

`Loss reduction`: 0.03

`Column-subsampling`: 7 of 8

`Row-subsampling`:  80%


---
## Feature importance

```{r, echo=FALSE, out.width="90%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/vip_ins.png"))
```

---
## Compressed tree visualization 

```{r, echo=FALSE, out.width="75%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/ins_multitree_xgb.png"))
```
---
## Final Performance

```{r, echo=FALSE, out.width="80%", fig.align='left'}
knitr::include_graphics(paste0(dirname(dirname(getwd())),"/_pictures/final_perf_ins.png"))
```

---
## Pros 

- **Minimal pre-processing**

- **Flexible enough to detect complex non-linear patterns**

- **Handling of missing values**

- **Integrated feature selection**

- **Good generalization due to lots of regularization options**

- **Strong predictive power**

## Cons

- **Not easily explainable**

- **Computationally demanding (especially the hyperparameter tuning)**

- **Better if there are lots of observations**


---
class: inverse, center, middle, hide-logo


## Thank you and happy boosting!

Here the link to the bookdown project TBD

Basically everything regarding my work for this seminar can be found there. 



---
class: inverse, center, middle, hide-logo


## Discussion





