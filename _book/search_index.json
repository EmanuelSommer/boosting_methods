[["index.html", "Boosting methods Chapter 1 Prerequisites", " Boosting methods Emanuel Sommer 2021-03-28 Chapter 1 Prerequisites Here some Notation/ Prerequisites TBD TBD TBD TBD TBD TBD "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Introduction to the whole setting (seminar topic/ intro to topic maybe mention kaggle wins) "],["theory.html", "Chapter 3 Theory of boosting 3.1 XGBoost", " Chapter 3 Theory of boosting Here comes the theory 3.1 XGBoost We will cite the great book!(Chen and Guestrin 2016) "],["eda.html", "Chapter 4 Expore the data 4.1 Train-test split 4.2 Visualize the data 4.3 Create recipe", " Chapter 4 Expore the data We use the following packages.(Wickham et al. 2019; Kuhn and Wickham 2020) library(tidyverse) library(tidymodels) 4.1 Train-test split set.seed(2) # ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) # ames_train &lt;- training(ames_split) # ames_test &lt;- testing(ames_split) 4.2 Visualize the data pair plots indiviual main effects correlations correlation plot individual distributions identify categorical features feature engineering 4.3 Create recipe # ames_rec &lt;- # recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + # Latitude + Longitude, data = ames_train) %&gt;% # step_log(Gr_Liv_Area, base = 10) %&gt;% # step_other(Neighborhood, threshold = 0.01) %&gt;% # step_dummy(all_nominal()) %&gt;% # step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% # step_ns(Latitude, Longitude, deg_free = 20) "],["modeling.html", "Chapter 5 Lets boost the models", " Chapter 5 Lets boost the models boooooooooooooooooooooost them (gif here?) library(xgboost) ## Warning: package &#39;xgboost&#39; was built under R version 4.0.3 ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice library(ranger) To do register parallel backend create models and workflows create resampling objects choose tuning method (racing/iterative or grid search) tune models and select best ones compare final models to test set save final model for 2. # Models: lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) rf_model &lt;- rand_forest(trees = 1000) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) boost_model &lt;- boost_tree() %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;regression&quot;) # Workflows: # lm_wflow &lt;- # workflow() %&gt;% # add_model(lm_model) %&gt;% # add_recipe() for 3. # Create Resampling objects # ames_folds &lt;- vfold_cv(ames_train, v = 10) "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion Lets wrap it up! "],["references.html", "Chapter 7 References", " Chapter 7 References Chen, Tianqi, and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. CoRR abs/1603.02754. http://arxiv.org/abs/1603.02754. Kuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles. https://www.tidymodels.org. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy DAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. Welcome to the tidyverse. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. "]]
