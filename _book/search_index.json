[["index.html", "Boosting methods Chapter 1 Prerequisites", " Boosting methods Emanuel Sommer 2021-04-08 Chapter 1 Prerequisites Here some Notation/ Prerequisites List: - CART - General Machine learining set up (test train split/CV) "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Introduction to the whole setting (seminar topic/ intro to topic maybe mention kaggle wins) Boosting is one of the most powerful learning ideas introduced in the last twenty years. [1] "],["theory.html", "Chapter 3 Theory 3.1 The powerful idea of gradient boosting 3.2 General gradient tree boosting 3.3 XGBoost a highly efficient implementation", " Chapter 3 Theory 3.1 The powerful idea of gradient boosting As roughly mentioned in the introduction section 2 the main idea of boosting is to sequentially build weak learners that form a powerful ensemble model. It is not totally clear from which field these methods emerged but some claim that the work of Freund and Schapire with respect to PAC learning in the 1990s were instrumental for their growth.[1] PAC learning can be considered one field within the broader field of learning theory that tries to find generalization bounds for algorithms that are probably approximately correct (PAC). [2] This section will firstly cover the general setup of gradient boosting as the most prominent method to train forward stagewise additive models. Secondly tree based gradient boosting and finally a very efficient and robust tree based gradient boosting algorithm namely XGBoost will be discussed in detail. 3.1.1 Forward Stagewise Additive Modeling In the setting of the dataset \\(\\mathcal{D} = \\{(y_i,x_i)\\ | i \\in [N]\\}\\) with \\(x_i \\in \\mathbb{R}^m\\) and \\(y_i \\in \\mathbb{R}\\) boosting is fitting the following additive, still quite general, model. \\[\\begin{equation} \\hat{y_i} = \\phi(x_i) = \\sum_{k=1}^{K} f_k(x_i), \\quad f_k \\in \\mathcal{F} \\tag{3.1} \\end{equation}\\] Where \\(\\mathcal{F}\\) is the space of learning algorithms that will be narrowed down later on. Additive expansions like this are at the core of many other powerful machine learning algorithms like Neural Networks or Wavelets. [1] The formulation [ref?](eq:additiveModel) leads to so called forward stagewise additive modeling which basically means that one sequentially adds \\(f \\in \\mathcal{F}\\) to the current model \\(\\phi_k\\) without changing anything about the previous models. [1] The algorithm is shown below. Algorithm 1: Forward Stagewise Additive Modeling Initialize \\(\\phi_0(x) = 0\\) For \\(k = 1\\) to \\(K\\) do: \\((\\beta_k,\\gamma_k) = argmin_{\\beta,\\gamma}\\sum_{i=1}^N L(y_i,\\phi_{k-1}(x_i) + \\beta f(x_i,\\gamma))\\) \\(\\phi_k(x) = \\phi_{k-1}(x) + \\beta_k f(x, \\gamma_k)\\) Where \\(\\gamma\\) parameterizes the learner \\(f \\in \\mathcal{F}\\) and the \\(\\beta_k\\) are the expansion coefficients. \\(L\\) should be a differentiable loss function. For example for the basic \\(L_2\\) loss the expression to be minimized simplifies to the following: \\[ L_2(y_i,\\phi_{k-1}(x_i) + \\beta f(x_i,\\gamma)) = (y_i - \\phi_{k-1}(x_i) - \\beta f(x_i,\\gamma))^2 \\] As \\(y_i - \\phi_{k-1}(x_i)\\) is just the residual of the previous model, so the next model that is added corresponds to the model that best approximates the residuals of the current model. Although the \\(L_2\\) loss has many very nice properties like the above it lacks robustness against outliers. Therefore two alternative losses for boosting in the regression setting are worth considering. 3.1.2 Robust loss functions for regression As the \\(L_2\\) loss squares the residuals, observations with large absolute residuals are overly important in the minimization step. This effect can be reduced intuitively by just using the \\(L_1\\) loss i.e. minimize over the sum over just the absolute residuals. To do this is indeed a valid approach and can reduce the influence of outliers greatly and thus make the final model more robust. Another good choice could be the Huber loss which tries to get the best of \\(L_1\\) and \\(L_2\\) loss. [1] \\[\\begin{equation} L_{Huber}(y,f(x)) = \\bigg\\{\\begin{array}{11} L_2(y,f(x)) &amp; |y-f(x)| \\leq \\delta \\\\ 2\\delta |y-f(x)| - \\delta^2 &amp; otherwise. \\\\ \\end{array} \\tag{3.2} \\end{equation}\\] In Figure 3.1 is a comparison of the three different losses discussed so far. Figure 3.1: Comparison of different regression loss functions[1] These alternative loss criteria are more robust but make the fitting i.e. the minimization much more complex as they do not yield such simplifications like the \\(L_2\\) loss. The next step in the journey of exploring boosting is to narrow down the argument spaces of Algorithm 1 (Forward Stagewise Additive Modeling) and to specify a subset of the general space of learning algorithms. This subset will be the space of Classification and Regression Tree (CART) models and in this case as the focus is on a regression task the space of regression trees. This choice is by no means arbitrary as in practice tree based boosting algorithms have proven countless of times that they provide very robust and accurate models but still other learners might be chosen. The next section will explore how one can actually fit such an Forward Stagewise Additive Model when using regression trees as the learner class. 3.2 General gradient tree boosting first some stuff from elements (more general) 3.2.1 numerical optimization 3.2.2 gradient tree boosting algorithm from elements 3.2.3 problem of right sized trees 3.2.4 regularisation 3.2.5 Off shelf performance Figure 3.2: Example of an additive tree ensamble[3] reference the figure: 3.2 3.3 XGBoost a highly efficient implementation use the whole paper We will cite the great paper![3] for details about hyperparameters use [4] "],["eda.html", "Chapter 4 Expore the data 4.1 Train-test split 4.2 Visualize the data 4.3 Create recipe", " Chapter 4 Expore the data We use the following packages.[5, 6] library(tidyverse) library(tidymodels) 4.1 Train-test split set.seed(2) # ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) # ames_train &lt;- training(ames_split) # ames_test &lt;- testing(ames_split) 4.2 Visualize the data pair plots indiviual main effects correlations correlation plot individual distributions identify categorical features feature engineering 4.3 Create recipe one for lm (dummify categorical features) and one for the tree based approaches # ames_rec &lt;- # recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + # Latitude + Longitude, data = ames_train) %&gt;% # step_log(Gr_Liv_Area, base = 10) %&gt;% # step_other(Neighborhood, threshold = 0.01) %&gt;% # step_dummy(all_nominal()) %&gt;% # step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% # step_ns(Latitude, Longitude, deg_free = 20) "],["modeling.html", "Chapter 5 Lets boost the models", " Chapter 5 Lets boost the models boooooooooooooooooooooost them (gif here?) Cite additional model packages.[8] library(xgboost) library(ranger) To do register parallel backend create models and workflows create resampling objects choose tuning method (racing/iterative or grid search) tune models and select best ones compare final models to test set save final model cite [9] library(doParallel) # Create a cluster object and then register: cl &lt;- makePSOCKcluster(2) registerDoParallel(cl) # Put at the end: stopCluster(cl) for 2. set engine specific arguments in the set_engine() function use tune() for parameters that should be tuned (optional tune(id_name)) # Models: lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) rf_model &lt;- rand_forest(trees = 1000) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) boost_model &lt;- boost_tree() %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;regression&quot;) # Workflows: # lm_wflow &lt;- # workflow() %&gt;% # add_model(lm_model) %&gt;% # add_recipe() for 3. # Create Resampling objects # set.seed(2) # ames_folds &lt;- vfold_cv(ames_train, v = 10) for fitting resampling objects without tuning: # set.seed(2) # rf_res &lt;- rf_wflow %&gt;% # fit_resamples(resamples = ames_folds) # # collect the metrics during resampling with # collect_metrics() for 4. and following # show the parameters to be tuned with the range # dials::parameters() # get and modify the parameters to be tuned with: # name() # wflow_param %&gt;% pull_dials_object(&quot;threshold&quot;) # parameters(ames_rec) %&gt;% # update(threshold = threshold(c(0.8, 1.0))) # finalize for data dependent params # updated_param &lt;- # workflow() %&gt;% # add_model(rf_spec) %&gt;% # add_recipe(pca_rec) %&gt;% # parameters() %&gt;% # finalize(ames_train) grids: grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2)) space filling: grid_latin_hypercube(size = 15, original = FALSE) then tune_grid() function instead of fit_resamples roc_res &lt;- metric_set(roc_auc) set.seed(99) mlp_reg_tune &lt;- mlp_wflow %&gt;% tune_grid( cell_folds, grid = mlp_param %&gt;% grid_regular(levels = 3), metrics = roc_res ) autoplot(mlp_reg_tune) + theme(legend.position = &quot;top&quot;) show_best(mlp_reg_tune) # for spacefilling mlp_sfd_tune &lt;- mlp_wflow %&gt;% tune_grid( cell_folds, grid = 20, # Pass in the parameter object to use the appropriate range: param_info = mlp_param, metrics = roc_res ) select_best() logistic_param &lt;- tibble( num_comp = 0, epochs = 125, hidden_units = 1, penalty = 1 ) final_mlp_wflow &lt;- mlp_wflow %&gt;% finalize_workflow(logistic_param) final_mlp_fit &lt;- final_mlp_wflow %&gt;% fit(cells) Racing: library(finetune) # if used to be cited set.seed(99) mlp_sfd_race &lt;- mlp_wflow %&gt;% tune_race_anova( cell_folds, grid = 20, param_info = mlp_param, metrics = roc_res, control = control_race(verbose_elim = TRUE) ) Here no iterative search as big parameter space, could be done after grid search or for single parameters. (If then Simulated Annealing) "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion Lets wrap it up! "],["references.html", "Chapter 7 References", " Chapter 7 References [1] Hastie, T., Tibshirani, R. and Friedman, J. (2009). The elements of statistical learning (12th printing). Springer New York. [2] Wolf, M. M. (2020). Lecture notes in mathematical foundations of supervised learning. [3] Chen, T. and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. CoRR abs/1603.02754. [4] Boehmke, B. and Greenwell, B. (2019). Hands-on machine learning with r. [5] Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K. and Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software 4 1686. [6] Kuhn, M. and Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. [7] Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y. and Li, Y. (2021). Xgboost: Extreme gradient boosting. [8] Wright, M. N. and Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software 77 17. [9] Corporation, M. and Weston, S. (2019). doParallel: Foreach parallel adaptor for the parallel package. "]]
