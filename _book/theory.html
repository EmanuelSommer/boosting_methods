<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Theory | Boosting methods: Theory and application in R</title>
  <meta name="description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Theory | Boosting methods: Theory and application in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  <meta name="github-repo" content="EmanuelSommer/boosting_methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Theory | Boosting methods: Theory and application in R" />
  
  <meta name="twitter:description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  

<meta name="author" content="Emanuel Sommer" />


<meta name="date" content="2021-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="eda.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Boosting methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>3</b> Theory</a>
<ul>
<li class="chapter" data-level="3.1" data-path="theory.html"><a href="theory.html#the-powerful-idea-of-gradient-boosting"><i class="fa fa-check"></i><b>3.1</b> The powerful idea of gradient boosting</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="theory.html"><a href="theory.html#forward-stagewise-additive-modeling"><i class="fa fa-check"></i><b>3.1.1</b> Forward Stagewise Additive Modeling</a></li>
<li class="chapter" data-level="3.1.2" data-path="theory.html"><a href="theory.html#robust-loss-functions-for-regression"><i class="fa fa-check"></i><b>3.1.2</b> Robust loss functions for regression</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="theory.html"><a href="theory.html#general-gradient-tree-boosting"><i class="fa fa-check"></i><b>3.2</b> General gradient tree boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="theory.html"><a href="theory.html#numOpt"><i class="fa fa-check"></i><b>3.2.1</b> Numerical optimization</a></li>
<li class="chapter" data-level="3.2.2" data-path="theory.html"><a href="theory.html#single-tree-depth"><i class="fa fa-check"></i><b>3.2.2</b> Single tree depth</a></li>
<li class="chapter" data-level="3.2.3" data-path="theory.html"><a href="theory.html#combOver"><i class="fa fa-check"></i><b>3.2.3</b> Combat overfitting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="theory.html"><a href="theory.html#xgboost-a-highly-efficient-implementation"><i class="fa fa-check"></i><b>3.3</b> XGBoost a highly efficient implementation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="theory.html"><a href="theory.html#regularized-loss"><i class="fa fa-check"></i><b>3.3.1</b> Regularized loss</a></li>
<li class="chapter" data-level="3.3.2" data-path="theory.html"><a href="theory.html#shrinkage-and-subsampling"><i class="fa fa-check"></i><b>3.3.2</b> Shrinkage and subsampling</a></li>
<li class="chapter" data-level="3.3.3" data-path="theory.html"><a href="theory.html#even-more-tweaks"><i class="fa fa-check"></i><b>3.3.3</b> Even more tweaks</a></li>
<li class="chapter" data-level="3.3.4" data-path="theory.html"><a href="theory.html#hyperparameters-overview"><i class="fa fa-check"></i><b>3.3.4</b> Hyperparameters overview</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>4</b> Explore the data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="eda.html"><a href="eda.html#burnout-data"><i class="fa fa-check"></i><b>4.1</b> Burnout data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="eda.html"><a href="eda.html#train-test-split"><i class="fa fa-check"></i><b>4.1.1</b> Train-test split</a></li>
<li class="chapter" data-level="4.1.2" data-path="eda.html"><a href="eda.html#quick-general-overview"><i class="fa fa-check"></i><b>4.1.2</b> Quick general overview</a></li>
<li class="chapter" data-level="4.1.3" data-path="eda.html"><a href="eda.html#what-about-the-outcome-variable"><i class="fa fa-check"></i><b>4.1.3</b> What about the outcome variable?</a></li>
<li class="chapter" data-level="4.1.4" data-path="eda.html"><a href="eda.html#distribution-and-main-effects-of-the-predictors"><i class="fa fa-check"></i><b>4.1.4</b> Distribution and main effects of the predictors</a></li>
<li class="chapter" data-level="4.1.5" data-path="eda.html"><a href="eda.html#relationships-between-the-predictors"><i class="fa fa-check"></i><b>4.1.5</b> Relationships between the predictors</a></li>
<li class="chapter" data-level="4.1.6" data-path="eda.html"><a href="eda.html#some-feature-engineering"><i class="fa fa-check"></i><b>4.1.6</b> Some feature engineering</a></li>
<li class="chapter" data-level="4.1.7" data-path="eda.html"><a href="eda.html#create-the-recipe"><i class="fa fa-check"></i><b>4.1.7</b> Create the recipe</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eda.html"><a href="eda.html#insurence-data"><i class="fa fa-check"></i><b>4.2</b> Insurence data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="eda.html"><a href="eda.html#train-test-split-1"><i class="fa fa-check"></i><b>4.2.1</b> Train-test split</a></li>
<li class="chapter" data-level="4.2.2" data-path="eda.html"><a href="eda.html#a-general-overview"><i class="fa fa-check"></i><b>4.2.2</b> A general overview</a></li>
<li class="chapter" data-level="4.2.3" data-path="eda.html"><a href="eda.html#what-about-the-outcome"><i class="fa fa-check"></i><b>4.2.3</b> What about the outcome?</a></li>
<li class="chapter" data-level="4.2.4" data-path="eda.html"><a href="eda.html#distribution-and-main-effects-of-the-predictors-1"><i class="fa fa-check"></i><b>4.2.4</b> Distribution and main effects of the predictors</a></li>
<li class="chapter" data-level="4.2.5" data-path="eda.html"><a href="eda.html#relationships-between-the-predictors-1"><i class="fa fa-check"></i><b>4.2.5</b> Relationships between the predictors</a></li>
<li class="chapter" data-level="4.2.6" data-path="eda.html"><a href="eda.html#create-the-recipe-1"><i class="fa fa-check"></i><b>4.2.6</b> Create the recipe</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>5</b> Let’s boost the models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modeling.html"><a href="modeling.html#burnout-data-1"><i class="fa fa-check"></i><b>5.1</b> Burnout data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="modeling.html"><a href="modeling.html#baseline-models"><i class="fa fa-check"></i><b>5.1.1</b> Baseline models</a></li>
<li class="chapter" data-level="5.1.2" data-path="modeling.html"><a href="modeling.html#model-specification"><i class="fa fa-check"></i><b>5.1.2</b> Model specification</a></li>
<li class="chapter" data-level="5.1.3" data-path="modeling.html"><a href="modeling.html#tuning"><i class="fa fa-check"></i><b>5.1.3</b> Tuning</a></li>
<li class="chapter" data-level="5.1.4" data-path="modeling.html"><a href="modeling.html#evaluate-and-understand-the-model"><i class="fa fa-check"></i><b>5.1.4</b> Evaluate and understand the model</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modeling.html"><a href="modeling.html#insurance-data"><i class="fa fa-check"></i><b>5.2</b> Insurance data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="modeling.html"><a href="modeling.html#baseline-models-1"><i class="fa fa-check"></i><b>5.2.1</b> Baseline models</a></li>
<li class="chapter" data-level="5.2.2" data-path="modeling.html"><a href="modeling.html#model-specification-1"><i class="fa fa-check"></i><b>5.2.2</b> Model specification</a></li>
<li class="chapter" data-level="5.2.3" data-path="modeling.html"><a href="modeling.html#tuning-1"><i class="fa fa-check"></i><b>5.2.3</b> Tuning</a></li>
<li class="chapter" data-level="5.2.4" data-path="modeling.html"><a href="modeling.html#evaluate-and-understand-the-model-1"><i class="fa fa-check"></i><b>5.2.4</b> Evaluate and understand the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a>
<ul>
<li class="chapter" data-level="6.1" data-path="conclusion.html"><a href="conclusion.html#pros"><i class="fa fa-check"></i><b>6.1</b> Pros</a></li>
<li class="chapter" data-level="6.2" data-path="conclusion.html"><a href="conclusion.html#cons"><i class="fa fa-check"></i><b>6.2</b> Cons</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>7</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Boosting methods: Theory and application in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theory" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Theory</h1>
<div id="the-powerful-idea-of-gradient-boosting" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> The powerful idea of gradient boosting</h2>
<p>As roughly mentioned in the introduction section <a href="intro.html#intro">2</a> the main idea of boosting is to sequentially build weak learners that form a powerful ensemble model. With weak learners models with high bias and low variance are meant that perform at least a little better than guessing. This already shows that the sequential approach of gradient boosting with weak learners stands in strong contrast to bagged ensembles like random forest. There many models with low bias and high variance are fitted in a parallel fashion and the variance is then reduced by averaging over the models.<span class="citation">[<a href="references.html#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> It is not totally clear from which field boosting methods emerged but some claim that the work of Freund and Schapire with respect to PAC learning in the 1990s were instrumental for their growth.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> PAC learning can be considered one field within the broader field of learning theory that tries to find generalization bounds for algorithms that are probably approximately correct (PAC).<span class="citation">[<a href="references.html#ref-pacbounds" role="doc-biblioref">3</a>]</span> This section will first cover the general setup of gradient boosting as the most prominent method to train forward stagewise additive models. Secondly tree-based gradient boosting and finally a very efficient and robust tree-based gradient boosting algorithm namely XGBoost will be discussed in detail.</p>
<div id="forward-stagewise-additive-modeling" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Forward Stagewise Additive Modeling</h3>
<p>In the setting of the data set <span class="math inline">\(\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}\)</span> with predictors <span class="math inline">\(x_i \in \mathbb{R}^m\)</span> and target <span class="math inline">\(y_i \in \mathbb{R}\)</span> boosting is fitting the following additive, still quite general, model.</p>
<span class="math display" id="eq:additiveModel">\[\begin{equation}
  \hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}
  \tag{3.1}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\mathcal{F}\)</span> is the space of learning algorithms that will be narrowed down later on. Additive expansions like this are at the core of many other powerful machine learning algorithms like Neural Networks or Wavelets.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<p>The formulation <a href="theory.html#eq:additiveModel">(3.1)</a> leads to so called forward stagewise additive modeling which basically means that one sequentially adds <span class="math inline">\(f \in \mathcal{F}\)</span> to the current model <span class="math inline">\(\phi_k\)</span> without changing anything about the previous models.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The algorithm is shown below.</p>
<hr />
<p><strong>Algorithm 1</strong>: Forward Stagewise Additive Modeling <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\phi_0(x) = 0\)</span></p></li>
<li><p>For <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(K\)</span> do:</p>
<ul>
<li><span class="math inline">\((\beta_k,\gamma_k) = argmin_{\beta,\gamma}\sum_{i=1}^N L(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma))\)</span></li>
<li><span class="math inline">\(\phi_k(x) = \phi_{k-1}(x) + \beta_k f(x, \gamma_k)\)</span></li>
</ul></li>
</ol>
<p>Where <span class="math inline">\(\gamma\)</span> parameterizes the learner <span class="math inline">\(f \in \mathcal{F}\)</span> and the <span class="math inline">\(\beta_k\)</span> are the expansion coefficients. <span class="math inline">\(L\)</span> should be a differentiable loss function.</p>
<hr />
<p>For example for the basic <span class="math inline">\(L_2\)</span> loss the expression to be minimized simplifies to the following:</p>
<p><span class="math display">\[
L_2(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma)) = (y_i - \phi_{k-1}(x_i) - \beta f(x_i,\gamma))^2
\]</span></p>
<p>As <span class="math inline">\(y_i - \phi_{k-1}(x_i)\)</span> is just the residual of the previous model, the next model that is added corresponds to the model that best approximates the residuals of the current model. Although the <span class="math inline">\(L_2\)</span> loss has many very nice properties like the above, it lacks robustness against outliers. Therefore two alternative losses for boosting in the regression setting are worth considering.</p>
</div>
<div id="robust-loss-functions-for-regression" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Robust loss functions for regression</h3>
<p>As the <span class="math inline">\(L_2\)</span> loss squares the residuals, observations with large absolute residuals are overly important in the minimization step. This effect can be reduced intuitively by just using the <span class="math inline">\(L_1\)</span> loss i.e. minimize over the sum over just the absolute residuals. To do this is indeed a valid approach and can reduce the influence of outliers greatly and thus make the final model more robust. Another good choice could be the <strong>Huber</strong> loss which tries to get the best of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<span class="math display" id="eq:huberLoss">\[\begin{equation}
  L_{Huber}(y,f(x)) = \begin{cases}L_2(y,f(x)) &amp; |y-f(x)| \leq \delta \\
  2\delta |y-f(x)| - \delta^2 &amp; otherwise.
  \end{cases}
  \tag{3.2}
\end{equation}\]</span>
<p>In Figure <a href="theory.html#fig:lossComp">3.1</a> is a comparison of the three different losses discussed so far.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<div class="figure" style="text-align: center"><span id="fig:lossComp"></span>
<img src="_pictures/huber_loss.png" alt="Comparison of different regression loss functions." width="70%" />
<p class="caption">
Figure 3.1: Comparison of different regression loss functions.
</p>
</div>
<p>These alternative loss criteria are more robust but make the fitting i.e. the minimization much more complex as they do not yield such simplifications like the <span class="math inline">\(L_2\)</span> loss.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The next step in the journey of exploring boosting is to narrow down the argument spaces of <strong>Algorithm 1</strong> (Forward Stagewise Additive Modeling) and to specify a subset of the general space of learning algorithms. This subset will be the space of Classification and Regression Tree (CART) models and in this case as the focus is on a regression task the space of regression trees. This choice is by no means arbitrary as in practice tree-based boosting algorithms have proven countless of times that they provide very robust and accurate models but still other learners might be chosen.<span class="citation">[<a href="references.html#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> The next subsection will explore how one can actually fit such a forward stagewise additive model when using regression trees as the learner class.</p>
</div>
</div>
<div id="general-gradient-tree-boosting" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> General gradient tree boosting</h2>
<p>From now on there is the switch from the space of learning algorithms <span class="math inline">\(\mathcal{F}\)</span> to the space of regression trees <span class="math inline">\(\mathcal{T}\)</span>. Such a regression tree can be formally expressed by:</p>
<span class="math display" id="eq:treeDef">\[\begin{equation}
  t(x, \gamma, R) = \sum_{j=1}^J \gamma_j I(x \in R_j) \quad \text{for  } t \in \mathcal{T}
  \tag{3.3}
\end{equation}\]</span>
<p>With <span class="math inline">\(R_j\)</span> being <span class="math inline">\(J\)</span> distinct regions of the predictor space usually attained by recursive binary splitting. Moreover these regions correspond to the leafs of the tree and the number of leafs <span class="math inline">\(J\)</span> or the depth of the trees are most often hyperparameters (not trained). The <span class="math inline">\(\gamma_j \in \mathbb{R}\)</span> are the predictions for a given x if x is contained in the region <span class="math inline">\(R_j\)</span>. While it is quite easy to get the <span class="math inline">\(\gamma_j\)</span> for the regions given, most often by computing <span class="math inline">\(\gamma_j = \frac{1}{|\{x \in R_j\}|} \sum_{\{x \in R_j\}} x\)</span> , it is a much harder problem to get good distinct regions. The above mentioned recursive binary splitting is an approximation to do this and works in a top down greedy fashion.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> From now on we assume that we have an efficient way of fitting such trees to a metric outcome variable e.g. by recursive binary splitting.</p>
<p>A nice graphical example of an additive model based on trees is displayed in the figure <a href="theory.html#fig:exampleAdditiveTree">3.2</a> below.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<div class="figure" style="text-align: center"><span id="fig:exampleAdditiveTree"></span>
<img src="_pictures/boosting_easy.png" alt="Example of an additive tree ensamble." width="70%" />
<p class="caption">
Figure 3.2: Example of an additive tree ensamble.
</p>
</div>
<p>Having now the new space <span class="math inline">\(\mathcal{T}\)</span> for the general boosting model <a href="theory.html#eq:additiveModel">(3.1)</a> one can write down the optimization problem that has to be solved in each step of the forward stagewise process of fitting the model.</p>
<span class="math display" id="eq:oneStepTreeBoost">\[\begin{equation}
  (\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R))
  \tag{3.4}
\end{equation}\]</span>
<p>This can be estimated fast and quite straight forward if there is a simplification like the one seen for the <span class="math inline">\(L_2\)</span> loss. But in the more general case of an arbitrary differentiable convex loss function like the Huber loss techniques from numerical optimization are needed to derive fast algorithms.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<div id="numOpt" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Numerical optimization</h3>
<p>According to <strong>Algorithm 1</strong> the goal in order to fit the boosting model is to minimize over the full loss of the training data <span class="math inline">\(\mathcal{D}\)</span> which is the sum over all observation losses.</p>
<p><span class="math display">\[
L(\phi) = \sum_{i=1}^N L(y_i, \phi(x_i))
\]</span></p>
<p>And thus the <span class="math inline">\(\hat{\phi}\)</span> additive boosting model we try to get is the following.</p>
<p><span class="math display">\[
\hat{\phi} = argmin_{\phi} L(\phi)
\]</span></p>
<p>Now we basically follow the spirit of the general gradient descent algorithm for differentiable functions. In the general case we minimize a function <span class="math inline">\(f(x)\)</span> by stepping iteratively along the direction of the steepest descent i.e. the negative gradient. The step length can then either be a constant small scalar or be determined by a line search.</p>
<p>In the setting of the additive boosting model <span class="math inline">\(\phi\)</span> can be viewed as a vector of dimension <span class="math inline">\(N\)</span> that contains the prediction according to <span class="math inline">\(\phi\)</span> of the corresponding observation i.e. <span class="math inline">\(\phi = (\phi(x_1),...,\phi(x_N))\)</span>. So the loss function <span class="math inline">\(L(\phi)\)</span> corresponds to the <span class="math inline">\(f(x)\)</span> in the general gradient descent algorithm. Numerical optimization, here gradient descent, then solves for <span class="math inline">\(\hat{\phi}\)</span> by a sum of vectors of the same dimension as the <span class="math inline">\(\phi\)</span>.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The result of the sum <span class="math inline">\(\phi_K\)</span> (<a href="theory.html#eq:numOptSol">(3.5)</a>) can be viewed as the current proposal for the optimal <span class="math inline">\(\hat{\phi}\)</span> after <span class="math inline">\(K\)</span> optimization steps and each <span class="math inline">\(h^{(k)}\)</span> is the proposed improvement step.</p>
<span class="math display" id="eq:numOptSol">\[\begin{equation}
  \phi_K = \sum_{k=0}^K h^{(k)} \quad h^{(k)} \in \mathbb{R}^N 
  \tag{3.5}
\end{equation}\]</span>
<p>While <span class="math inline">\(h^{(0)}\)</span> is just an initial guess the subsequent <span class="math inline">\(h^{(k)}\)</span> are again the prediction vectors of the corresponding model out of <span class="math inline">\(\mathcal{T}\)</span> i.e. <span class="math inline">\(h^{(k)}_{i} = t_k(x_i)\)</span> with <span class="math inline">\(t \in \mathcal{T}\)</span>. This means that each <span class="math inline">\(\phi_k = \phi_{k-1} + h^{(k)}\)</span>. The <span class="math inline">\(h^{(k)}\)</span> are calculated via the gradient which finally comes into play. As mentioned above one minimizes the loss the most by going towards the direction of the steepest descent. For <a href="theory.html#eq:numOptSol">(3.5)</a> follows from its additive formulation and defining <span class="math inline">\(g^{(k)}\)</span> as the gradient of <span class="math inline">\(L(\phi_k)\)</span> evaluated for <span class="math inline">\(\phi_{k-1}\)</span> the update <span class="math inline">\(h^{(k)} = -\lambda_k g^{(k)}\)</span>. As we assumed the loss to be differentiable we see that <span class="math inline">\(g^{(k)}\)</span> is well defined. Here <span class="math inline">\(\lambda_k\)</span> is the usual step length for gradient descent methods. It is the solution of the line search <span class="math inline">\(\lambda_k = argmin_{\lambda} L(\phi_{k-1} - \lambda g^{(k)})\)</span>. This <span class="math inline">\(\lambda_k\)</span> almost exactly corresponds to the <span class="math inline">\(\beta_k\)</span> in <strong>Algorithm 1</strong> although here the optimization is performed for every region of the tree separately.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<p>With these insights it is clear that the tree predictions correspond to the negative gradient <span class="math inline">\(-g^{(k)}\)</span>. Of course the predictions are not independent as the prediction is constant for each leaf of the tree. So the new optimization proposed by numerical optimization via gradient boosting is given in <a href="theory.html#eq:oneStepTreeBoostnew">(3.6)</a> below.</p>
<span class="math display" id="eq:oneStepTreeBoostnew">\[\begin{equation}
  (\tilde{\gamma}^{(k)},\tilde{R}^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N [-g^{(k)}_{i} -  t(x_i,\gamma,R)] ^2
  \tag{3.6}
\end{equation}\]</span>
<p>In words this just means fitting a regression tree by least squares to the negative gradients that were evaluated with the current predictions. The solution regions will not exactly match the ones from <a href="theory.html#eq:oneStepTreeBoost">(3.4)</a> but should be very similar.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> After having estimated the regions one estimates the parameters <span class="math inline">\(\gamma\)</span> by solving the line search <a href="theory.html#eq:gammaLineSearch">(3.7)</a>.</p>
<span class="math display" id="eq:gammaLineSearch">\[\begin{equation}
  \tilde{\gamma}^{(k)}_{j} = argmin_{\gamma^{(k)}_{j}} \sum_{x \in R^{(k)}_{j}} L(y_i,\phi_{k-1}(x_i) + \gamma^{(k)}_{j})
  \tag{3.7}
\end{equation}\]</span>
<p>Putting all of this back together with <strong>Algorithm 1</strong> results in <strong>Algorithm 2</strong> that covers a general algorithm for tree-based gradient boosting.</p>
<hr />
<p><strong>Algorithm 2</strong>: Tree-based gradient boosting <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\phi_0(x)\)</span> as a singular node tree.</p></li>
<li><p>For <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(K\)</span> do:</p>
<ul>
<li><p>For <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(N\)</span> compute:</p>
<p><span class="math inline">\(g^{(k)}_{i} = \bigg[\frac{\partial L(y_i, \phi(x_i))}{\partial \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}\)</span></p></li>
<li><p>Fit a regression tree by least squares to the outcome vector <span class="math inline">\(-g^{(k)}\)</span> in order to get the <span class="math inline">\(J^{(k)}\)</span> distinct regions <span class="math inline">\(\tilde{R}^{(k)}_j\)</span>.</p></li>
<li><p>For each of these <span class="math inline">\(J^{(k)}\)</span> regions perform a line search in order to compute the leaf predictions <span class="math inline">\(\tilde{\gamma}^{(k)}_{j}\)</span> exactly like in <a href="theory.html#eq:gammaLineSearch">(3.7)</a>.</p></li>
<li><p>Set <span class="math inline">\(\phi_k(x) = \phi_{k-1}(x) + t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)\)</span> with <span class="math inline">\(t \in \mathcal{T}\)</span></p></li>
</ul></li>
</ol>
<hr />
<p>The only unknowns in this algorithm are now the differentiable loss function and the hyperparameters like the <span class="math inline">\(J^{(k)}\)</span> and the <span class="math inline">\(K\)</span>. While choices for the hyperparameters are discussed further below, the following table <a href="theory.html#tab:lossGradients">3.1</a> displays the gradients for the losses discussed so far.</p>
<table>
<caption><span id="tab:lossGradients">Table 3.1: </span> Gradients of the discussed losses <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></caption>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th>Loss</th>
<th>Gradient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(L_2\)</span>: <span class="math inline">\((y_i - \phi(x_i))^2\)</span></td>
<td><span class="math inline">\(2(y_i - \phi(x_i))\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(L_1\)</span>: <span class="math inline">\(|y_i - \phi(x_i)|\)</span></td>
<td><span class="math inline">\(sign(y_i - \phi(x_i))\)</span></td>
</tr>
<tr class="odd">
<td>Huber loss <a href="theory.html#eq:huberLoss">(3.2)</a></td>
<td><p><span class="math inline">\(y_i - \phi(x_i)\)</span> for <span class="math inline">\(|y_i - \phi(x_i)| \leq \delta\)</span></p>
<p><span class="math inline">\(\delta sign(y_i - \phi(x_i))\)</span> otherwise, with <span class="math inline">\(\delta\)</span> quantile of <span class="math inline">\(|y_i - \phi(x_i)|\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div id="single-tree-depth" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Single tree depth</h3>
<p>The question which <span class="math inline">\(J^{(k)}\)</span> should be used at each iteration is now shortly discussed. Basically using the tree depth (<span class="math inline">\(log_2(J^{(k)}) = Depth^{(k)}\)</span>) two means allowing only the main effects and no interactions. A value of 3 already allows all two way interaction effects and so on. As one stacks these models additive and does not restrict oneself to a single one, the building of a large tree and then pruning it back at each iteration as the regular CART algorithms do would be a computational overkill. Instead it has proven to be sufficient and efficient in practice to set the values <span class="math inline">\(Depth^{(k)}\)</span> to a constant <span class="math inline">\(Depth \approx 6\)</span>.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> Also decision stumps (<span class="math inline">\(Depth = 1\)</span>) could be used but may require a lot more iterations.</p>
</div>
<div id="combOver" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Combat overfitting</h3>
<p>No learning algorithm could be fully covered without treating the good old problem of overfitting. In the setting of boosting the experienced eye could have spotted the problem of overfitting already in the definition of the additive model <a href="theory.html#eq:additiveModel">(3.1)</a>. There the number <span class="math inline">\(K\)</span> of models that form the ensemble was introduced but was not discussed further till now. Of course one can arbitrarily fit or better say remember some given training data in order to minimize the loss with such an additive model by letting <span class="math inline">\(K\)</span> be arbitrarily large. This comes from the fact that the loss reduces usually after each iteration over <span class="math inline">\(K\)</span>.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The easiest way to prevent overfitting is to have a validation set at hand which is disjoint from the training data. Having such a validation set at hand can be used to monitor the loss on the unseen data. In the case the loss would rise again on the validation data one can consider to stop the algorithm and to use the current iteration number for the final parameter <span class="math inline">\(K\)</span>. This approach is often called early stopping. Besides that there are methods that regularize the individual trees that are fitted in each iteration. Two of those will be discussed now.</p>
<div id="shrinkage" class="section level4" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> Shrinkage</h4>
<p>Shrinkage basically refers to just introducing a learning rate <span class="math inline">\(\eta\)</span>. This learning rate <span class="math inline">\(\eta\)</span> scales the contribution of the new model. In general the learning rate should be in <span class="math inline">\((0,1]\)</span> but in practice it has been shown that rather small values like <span class="math inline">\(\eta &lt; 0.1\)</span> work very good.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> As almost everything in life this does not come for free. A smaller learning rate usually comes with a computational cost as with lower <span class="math inline">\(\eta\)</span> a larger <span class="math inline">\(K\)</span> is required. Those two hyperparameters represent a trade-off in a way. In practice it is advisable to use a small learning rate <span class="math inline">\(\eta\)</span> and adjust the <span class="math inline">\(K\)</span> accordingly which in this case would mean to make the <span class="math inline">\(K\)</span> large enough until one reaches an early stopping point. Still it is good to keep in mind that a <span class="math inline">\(\eta\)</span> that is too small can lead to an immense and unnecessary computational effort. All in all using shrinkage the update step in <strong>Algorithm 2</strong> changes to the following.</p>
<span class="math display" id="eq:shrinkage">\[\begin{equation}
  \phi_k(x) = \phi_{k-1}(x) + \eta * t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)
  \tag{3.8}
\end{equation}\]</span>
</div>
<div id="subsampling" class="section level4" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> Subsampling</h4>
<p>The other method to regularize the trees will be subsampling. There are two different kinds of subsampling. The first one is row-subsampling which basically means just using a random fraction of the training data in each iteration when fitting the new tree. Common values are <span class="math inline">\(\frac{1}{2}\)</span> but for a larger training set the value can be chosen to be smaller. This does not only help to prevent overfitting and thus achieving a better predictive performance but reduces also the computational effort in each iteration.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The approach is very similar to the dropout technique in the setting of deep neural networks.</p>
<p>Moreover one can apply column-subsampling or feature-subsampling. Here only a fraction of the features or an explicit number of the features are used in each iteration. This is the exact same method and intention as the one applied in random forest models. Again the technique not only boosts performance on unseen data but also reduces the computational time.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span> Still it must be noted that by using one or both subsampling methods one introduces one or two additional hyperparameters to the model that have to be tuned.</p>
<p>Having the <strong>Algorithm 2</strong> for tree-based gradient boosting alongside some regularization techniques it is time to look at a very popular, efficient and powerful open source implementation.</p>
</div>
</div>
</div>
<div id="xgboost-a-highly-efficient-implementation" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> XGBoost a highly efficient implementation</h2>
<p>The XGBoost algorithm is a highly scalable, efficient and successful implementation and optimization of <strong>Algorithm 2</strong> introduced by Chen and Guestrin in 2016.<span class="citation">[<a href="references.html#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> It has gained a lot of spotlight when it was integral for many winning submissions at kaggle machine learning challenges. In the following the most important tweaks that are proposed to <strong>Algorithm 2</strong> are covered.</p>
<div id="regularized-loss" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Regularized loss</h3>
<p>Instead of just minimizing a convex and differentiable loss <span class="math inline">\(L\)</span> XGBoost minimizes the following regularized loss.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<span class="math display" id="eq:regLoss">\[\begin{equation}
  \begin{split}
  \mathcal{L}(\phi) = L(\phi) + \sum_{k=1}^K \Omega(t_k) \\
  \text{where } \Omega(t) = \nu J + \frac{1}{2} \lambda ||\gamma||^2 \quad t \in \mathcal{T}
  \end{split}
  \tag{3.9}
\end{equation}\]</span>
<p>Here <span class="math inline">\(J\)</span> and <span class="math inline">\(\gamma\)</span> again parameterize the regression trees <span class="math inline">\(t \in \mathcal{T}\)</span> as defined in <a href="theory.html#eq:treeDef">(3.3)</a>. As evident from the formula both hyperparameters <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\lambda\)</span> favor when increased less complex models at each iteration. This is again a measure against overfitting.</p>
<p>This regularized objective leads to a slightly modified version of the minimization problem <a href="theory.html#eq:oneStepTreeBoost">(3.4)</a> that has to be solved in each iteration.</p>
<span class="math display" id="eq:oneStepXGBoost">\[\begin{equation}
  (\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R)) + \Omega(t(x_i,\gamma,R))
  \tag{3.10}
\end{equation}\]</span>
<p>Instead of the first order approach, that has been introduced in <a href="theory.html#numOpt">3.2.1</a>, XGBoost uses a second order approximation to further simplify the objective. This means that besides the first order gradient <span class="math inline">\(g^{(k)}\)</span> of the loss function evaluated at <span class="math inline">\(\phi_{k-1}\)</span> also the second order gradient <span class="math inline">\(z^{(k)}_i = \bigg[\frac{\partial^2 L(y_i, \phi(x_i))}{\partial^2 \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}\)</span> is used. By neglecting constants one reaches the approximate new objective below.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<span class="math display" id="eq:oneappStepXGBoost">\[\begin{equation}
\begin{split}
  \tilde{\mathcal{L}}^{(k)} &amp; = \sum_{i=1}^N [g^{(k)}_i t^{(k)}(x_i) + \frac{1}{2} t^{(k)}(x_i)^2] + \Omega(t^{(k)}) \\
  &amp; = \sum_{j=1}^J [(\sum_{\{i|x_i \in R^{(k)}_j\}} g^{(k)}_i) \gamma^{(k)}_j + \frac{1}{2} (\sum_{\{i|x_i \in R^{(k)}_j\}} z^{(k)}_i + \lambda) (\gamma^{(k)}_j)^2 ] + \nu J
\end{split}
\tag{3.11}
\end{equation}\]</span>
<p>This representation is used as it can be decomposed by the <span class="math inline">\(J\)</span> leafs of the tree. One can then define a scoring function for split finding that only depends on <span class="math inline">\(g^{(k)}\)</span> and <span class="math inline">\(z^{(k)}\)</span>. This approach speeds up the split finding and allows further parallel computations with respect to the leafs.</p>
</div>
<div id="shrinkage-and-subsampling" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Shrinkage and subsampling</h3>
<p>As discussed above in <a href="theory.html#combOver">3.2.3</a> shrinkage and subsampling are great ways to regularize the model and prevent overfitting. XGBoost implements both shrinkage and subsampling. It enables the user to use column as well as row subsampling. The authors claim that in practice column subsampling has prevented overfitting more than row subsampling.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
</div>
<div id="even-more-tweaks" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Even more tweaks</h3>
<p>Besides the changes above the authors also implemented an effective approximate algorithm for split finding for the tree building step. A very nice attribute of this approximate procedure is that the algorithm is sparsity aware i.e. the processing of missing values or 0 values is very efficient. This is done via a learned default direction in each split. Moreover they used very efficient data structures to allow a lot of parallel processing.<span class="citation">[<a href="references.html#ref-xgboostPaper" role="doc-biblioref">4</a>]</span> It is written in C++ but has APIs in various languages like in R via the <code>xgboost</code> package.<span class="citation">[<a href="references.html#ref-xgboost_package" role="doc-biblioref">5</a>]</span></p>
</div>
<div id="hyperparameters-overview" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Hyperparameters overview</h3>
<p>As the <strong>tidymodels</strong> framework will be used in the applied part, the according names from the parsnip interface for tree-based boosting i.e. <code>boost_tree</code> are discussed below.<span class="citation">[<a href="references.html#ref-tidymodels" role="doc-biblioref">6</a>]</span></p>
<table>
<caption><span id="tab:xgboostHyper">Table 3.2: </span> XGBoost hyperparameters overview.</caption>
<colgroup>
<col width="14%" />
<col width="16%" />
<col width="69%" />
</colgroup>
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>Notation above</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>trees</code></td>
<td><span class="math inline">\(K\)</span></td>
<td>Number of trees in the ensemble.</td>
</tr>
<tr class="even">
<td><code>tree_depth</code></td>
<td><span class="math inline">\(max(log_2J^{(k)})\)</span></td>
<td>Maximum depth of the trees.</td>
</tr>
<tr class="odd">
<td><code>learn_rate</code></td>
<td><span class="math inline">\(\eta\)</span></td>
<td>Learning rate (shrinkage).</td>
</tr>
<tr class="even">
<td><code>min_n</code></td>
<td></td>
<td>Minimum number of observations in terminal region <span class="math inline">\(R^{(k)}_j\)</span>.</td>
</tr>
<tr class="odd">
<td><code>lambda</code></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(L_2\)</span> regularization parameter on <span class="math inline">\(\gamma\)</span> as in <a href="theory.html#eq:oneappStepXGBoost">(3.11)</a> (default = 0).</td>
</tr>
<tr class="even">
<td><code>alpha</code></td>
<td></td>
<td><span class="math inline">\(L_1\)</span> regularization parameter on <span class="math inline">\(\gamma\)</span> (default = 0).</td>
</tr>
<tr class="odd">
<td><code>loss_reduction</code></td>
<td><span class="math inline">\(\nu\)</span></td>
<td>Minimal loss reduction for terminal partitions. (default = 0)</td>
</tr>
<tr class="even">
<td><code>mtry</code></td>
<td></td>
<td>Proportion or number of columns for column subsampling.</td>
</tr>
<tr class="odd">
<td><code>sample_size</code></td>
<td></td>
<td>Proportion of observations for row subsampling.</td>
</tr>
<tr class="even">
<td><code>stop_iter</code></td>
<td></td>
<td>Allowed number of rounds without improvement before early stopping.</td>
</tr>
</tbody>
</table>
<p>Note that the <span class="math inline">\(L_1\)</span> penalization term would be added to the <span class="math inline">\(\Omega\)</span> term in the loss <span class="math inline">\(\mathcal{L}\)</span> of XGBoost.</p>
<p>This closes the chapter on the theory of boosting methods with a focus on tree-based gradient boosting. The discussed tools will be put to the test in the subsequent sections where real world data will be analyzed with them.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="eda.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["boosting_methods.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
