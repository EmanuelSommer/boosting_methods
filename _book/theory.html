<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Theory | Boosting methods</title>
  <meta name="description" content="Chapter 3 Theory | Boosting methods: Theory and Application (xgboost)" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Theory | Boosting methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 3 Theory | Boosting methods: Theory and Application (xgboost)" />
  <meta name="github-repo" content="EmanuelSommer/boosting_methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Theory | Boosting methods" />
  
  <meta name="twitter:description" content="Chapter 3 Theory | Boosting methods: Theory and Application (xgboost)" />
  

<meta name="author" content="Emanuel Sommer" />


<meta name="date" content="2021-04-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="eda.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Boosting methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="theory.html"><a href="theory.html"><i class="fa fa-check"></i><b>3</b> Theory</a>
<ul>
<li class="chapter" data-level="3.1" data-path="theory.html"><a href="theory.html#the-powerful-idea-of-gradient-boosting"><i class="fa fa-check"></i><b>3.1</b> The powerful idea of gradient boosting</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="theory.html"><a href="theory.html#forward-stagewise-additive-modeling"><i class="fa fa-check"></i><b>3.1.1</b> Forward Stagewise Additive Modeling</a></li>
<li class="chapter" data-level="3.1.2" data-path="theory.html"><a href="theory.html#robust-loss-functions-for-regression"><i class="fa fa-check"></i><b>3.1.2</b> Robust loss functions for regression</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="theory.html"><a href="theory.html#general-gradient-tree-boosting"><i class="fa fa-check"></i><b>3.2</b> General gradient tree boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="theory.html"><a href="theory.html#numerical-optimization"><i class="fa fa-check"></i><b>3.2.1</b> numerical optimization</a></li>
<li class="chapter" data-level="3.2.2" data-path="theory.html"><a href="theory.html#gradient-tree-boosting-algorithm-from-elements"><i class="fa fa-check"></i><b>3.2.2</b> gradient tree boosting algorithm from elements</a></li>
<li class="chapter" data-level="3.2.3" data-path="theory.html"><a href="theory.html#problem-of-right-sized-trees"><i class="fa fa-check"></i><b>3.2.3</b> problem of right sized trees</a></li>
<li class="chapter" data-level="3.2.4" data-path="theory.html"><a href="theory.html#regularisation"><i class="fa fa-check"></i><b>3.2.4</b> regularisation</a></li>
<li class="chapter" data-level="3.2.5" data-path="theory.html"><a href="theory.html#off-shelf-performance"><i class="fa fa-check"></i><b>3.2.5</b> Off shelf performance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="theory.html"><a href="theory.html#xgboost-a-highly-efficient-implementation"><i class="fa fa-check"></i><b>3.3</b> XGBoost a highly efficient implementation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>4</b> Expore the data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="eda.html"><a href="eda.html#train-test-split"><i class="fa fa-check"></i><b>4.1</b> Train-test split</a></li>
<li class="chapter" data-level="4.2" data-path="eda.html"><a href="eda.html#visualize-the-data"><i class="fa fa-check"></i><b>4.2</b> Visualize the data</a></li>
<li class="chapter" data-level="4.3" data-path="eda.html"><a href="eda.html#create-recipe"><i class="fa fa-check"></i><b>4.3</b> Create recipe</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>5</b> Let’s boost the models</a></li>
<li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="chapter" data-level="7" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>7</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Boosting methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theory" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Theory</h1>
<div id="the-powerful-idea-of-gradient-boosting" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> The powerful idea of gradient boosting</h2>
<p>As roughly mentioned in the introduction section <a href="intro.html#intro">2</a> the main idea of boosting is to sequentially build ‘weak’ learners that form a powerful ensemble model. It is not totally clear from which field these methods emerged but some claim that the work of Freund and Schapire with respect to PAC learning in the 1990s were instrumental for their growth.<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> PAC learning can be considered one field within the broader field of learning theory that tries to find generalization bounds for algorithms that are probably approximately correct (PAC). <span class="citation">[<a href="references.html#ref-pacbounds" role="doc-biblioref">2</a>]</span> This section will firstly cover the general setup of gradient boosting as the most prominent method to train forward stagewise additive models. Secondly tree based gradient boosting and finally a very efficient and robust tree based gradient boosting algorithm namely XGBoost will be discussed in detail.</p>
<div id="forward-stagewise-additive-modeling" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Forward Stagewise Additive Modeling</h3>
<p>In the setting of the dataset <span class="math inline">\(\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}\)</span> with <span class="math inline">\(x_i \in \mathbb{R}^m\)</span> and <span class="math inline">\(y_i \in \mathbb{R}\)</span> boosting is fitting the following additive, still quite general, model.</p>
<span class="math display" id="eq:additiveModel">\[\begin{equation}
  \hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}
  \tag{3.1}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\mathcal{F}\)</span> is the space of learning algorithms that will be narrowed down later on. Additive expansions like this are at the core of many other powerful machine learning algorithms like Neural Networks or Wavelets. <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<p>The formulation <span class="citation">[<a href="#ref-ref" role="doc-biblioref"><strong>ref?</strong></a>]</span>(eq:additiveModel) leads to so called forward stagewise additive modeling which basically means that one sequentially adds <span class="math inline">\(f \in \mathcal{F}\)</span> to the current model <span class="math inline">\(\phi_k\)</span> without changing anything about the previous models. <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span> The algorithm is shown below.</p>
<hr />
<p><strong>Algorithm 1</strong>: Forward Stagewise Additive Modeling</p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\phi_0(x) = 0\)</span></p></li>
<li><p>For <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(K\)</span> do:</p>
<ul>
<li><span class="math inline">\((\beta_k,\gamma_k) = argmin_{\beta,\gamma}\sum_{i=1}^N L(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma))\)</span></li>
<li><span class="math inline">\(\phi_k(x) = \phi_{k-1}(x) + \beta_k f(x, \gamma_k)\)</span></li>
</ul></li>
</ol>
<p>Where <span class="math inline">\(\gamma\)</span> parameterizes the learner <span class="math inline">\(f \in \mathcal{F}\)</span> and the <span class="math inline">\(\beta_k\)</span> are the expansion coefficients. <span class="math inline">\(L\)</span> should be a differentiable loss function.</p>
<hr />
<p>For example for the basic <span class="math inline">\(L_2\)</span> loss the expression to be minimized simplifies to the following:</p>
<p><span class="math display">\[
L_2(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma)) = (y_i - \phi_{k-1}(x_i) - \beta f(x_i,\gamma))^2
\]</span></p>
<p>As <span class="math inline">\(y_i - \phi_{k-1}(x_i)\)</span> is just the residual of the previous model, so the next model that is added corresponds to the model that best approximates the residuals of the current model. Although the <span class="math inline">\(L_2\)</span> loss has many very nice properties like the above it lacks robustness against outliers. Therefore two alternative losses for boosting in the regression setting are worth considering.</p>
</div>
<div id="robust-loss-functions-for-regression" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Robust loss functions for regression</h3>
<p>As the <span class="math inline">\(L_2\)</span> loss squares the residuals, observations with large absolute residuals are overly important in the minimization step. This effect can be reduced intuitively by just using the <span class="math inline">\(L_1\)</span> loss i.e. minimize over the sum over just the absolute residuals. To do this is indeed a valid approach and can reduce the influence of outliers greatly and thus make the final model more robust. Another good choice could be the <strong>Huber</strong> loss which tries to get the best of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss. <span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span></p>
<span class="math display" id="eq:huberLoss">\[\begin{equation}
  L_{Huber}(y,f(x)) = \bigg\{\begin{array}{11}
  L_2(y,f(x)) &amp; |y-f(x)| \leq \delta \\
  2\delta |y-f(x)| - \delta^2 &amp; otherwise. \\
  \end{array}
  \tag{3.2}
\end{equation}\]</span>
<p>In Figure <a href="theory.html#fig:lossComp">3.1</a> is a comparison of the three different losses discussed so far.</p>
<div class="figure" style="text-align: center"><span id="fig:lossComp"></span>
<img src="_pictures/huber_loss.png" alt="Comparison of different regression loss functions[@elements]" width="70%" />
<p class="caption">
Figure 3.1: Comparison of different regression loss functions<span class="citation">[<a href="references.html#ref-elements" role="doc-biblioref">1</a>]</span>
</p>
</div>
<p>These alternative loss criteria are more robust but make the fitting i.e. the minimization much more complex as they do not yield such simplifications like the <span class="math inline">\(L_2\)</span> loss. The next step in the journey of exploring boosting is to narrow down the argument spaces of Algorithm 1 (Forward Stagewise Additive Modeling) and to specify a subset of the general space of learning algorithms. This subset will be the space of Classification and Regression Tree (CART) models and in this case as the focus is on a regression task the space of regression trees. This choice is by no means arbitrary as in practice tree based boosting algorithms have proven countless of times that they provide very robust and accurate models but still other learners might be chosen. The next section will explore how one can actually fit such an Forward Stagewise Additive Model when using regression trees as the learner class.</p>
</div>
</div>
<div id="general-gradient-tree-boosting" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> General gradient tree boosting</h2>
<p>first some stuff from elements (more general)</p>
<div id="numerical-optimization" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> numerical optimization</h3>
</div>
<div id="gradient-tree-boosting-algorithm-from-elements" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> gradient tree boosting algorithm from elements</h3>
</div>
<div id="problem-of-right-sized-trees" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> problem of right sized trees</h3>
</div>
<div id="regularisation" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> regularisation</h3>
</div>
<div id="off-shelf-performance" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Off shelf performance</h3>
<div class="figure" style="text-align: center"><span id="fig:exampleAdditiveTree"></span>
<img src="_pictures/boosting_easy.png" alt="Example of an additive tree ensamble[@xgboost_paper]" width="70%" />
<p class="caption">
Figure 3.2: Example of an additive tree ensamble<span class="citation">[<a href="references.html#ref-xgboost_paper" role="doc-biblioref">3</a>]</span>
</p>
</div>
<p>reference the figure: <a href="theory.html#fig:exampleAdditiveTree">3.2</a></p>
</div>
</div>
<div id="xgboost-a-highly-efficient-implementation" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> XGBoost a highly efficient implementation</h2>
<p>use the whole paper</p>
<p>We will cite the great paper!<span class="citation">[<a href="references.html#ref-xgboost_paper" role="doc-biblioref">3</a>]</span></p>
<p>for details about hyperparameters use <span class="citation">[<a href="references.html#ref-HandsOnMLwithR" role="doc-biblioref">4</a>]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="eda.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["boosting_methods.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
