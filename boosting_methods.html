<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Boosting methods: Theory and application in R</title>
  <meta name="description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Boosting methods: Theory and application in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  <meta name="github-repo" content="EmanuelSommer/boosting_methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Boosting methods: Theory and application in R" />
  
  <meta name="twitter:description" content="Boosting methods: Theory and Application in R (XGBoost)" />
  

<meta name="author" content="Emanuel Sommer" />


<meta name="date" content="2021-06-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Boosting methods: Theory and application in R</h1>
<p class="author"><em>Emanuel Sommer</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-06-19</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#prerequisites"><span class="toc-section-number">1</span> Prerequisites</a></li>
<li><a href="#intro"><span class="toc-section-number">2</span> Introduction</a></li>
<li><a href="#theory"><span class="toc-section-number">3</span> Theory</a>
<ul>
<li><a href="#the-powerful-idea-of-gradient-boosting"><span class="toc-section-number">3.1</span> The powerful idea of gradient boosting</a>
<ul>
<li><a href="#forward-stagewise-additive-modeling"><span class="toc-section-number">3.1.1</span> Forward Stagewise Additive Modeling</a></li>
<li><a href="#robust-loss-functions-for-regression"><span class="toc-section-number">3.1.2</span> Robust loss functions for regression</a></li>
</ul></li>
<li><a href="#general-gradient-tree-boosting"><span class="toc-section-number">3.2</span> General gradient tree boosting</a>
<ul>
<li><a href="#numOpt"><span class="toc-section-number">3.2.1</span> Numerical optimization</a></li>
<li><a href="#single-tree-depth"><span class="toc-section-number">3.2.2</span> Single tree depth</a></li>
<li><a href="#combOver"><span class="toc-section-number">3.2.3</span> Combat overfitting</a></li>
</ul></li>
<li><a href="#xgboost-a-highly-efficient-implementation"><span class="toc-section-number">3.3</span> XGBoost a highly efficient implementation</a>
<ul>
<li><a href="#regularized-loss"><span class="toc-section-number">3.3.1</span> Regularized loss</a></li>
<li><a href="#shrinkage-and-subsampling"><span class="toc-section-number">3.3.2</span> Shrinkage and subsampling</a></li>
<li><a href="#even-more-tweaks"><span class="toc-section-number">3.3.3</span> Even more tweaks</a></li>
<li><a href="#hyperparameters-overview"><span class="toc-section-number">3.3.4</span> Hyperparameters overview</a></li>
</ul></li>
</ul></li>
<li><a href="#eda"><span class="toc-section-number">4</span> Explore the data</a>
<ul>
<li><a href="#burnout-data"><span class="toc-section-number">4.1</span> Burnout data</a>
<ul>
<li><a href="#train-test-split"><span class="toc-section-number">4.1.1</span> Train-test split</a></li>
<li><a href="#quick-general-overview"><span class="toc-section-number">4.1.2</span> Quick general overview</a></li>
<li><a href="#what-about-the-outcome-variable"><span class="toc-section-number">4.1.3</span> What about the outcome variable?</a></li>
<li><a href="#distribution-and-main-effects-of-the-predictors"><span class="toc-section-number">4.1.4</span> Distribution and main effects of the predictors</a></li>
<li><a href="#relationships-between-the-predictors"><span class="toc-section-number">4.1.5</span> Relationships between the predictors</a></li>
<li><a href="#some-feature-engineering"><span class="toc-section-number">4.1.6</span> Some feature engineering</a></li>
<li><a href="#create-the-recipe"><span class="toc-section-number">4.1.7</span> Create the recipe</a></li>
</ul></li>
<li><a href="#insurence-data"><span class="toc-section-number">4.2</span> Insurence data</a>
<ul>
<li><a href="#train-test-split-1"><span class="toc-section-number">4.2.1</span> Train-test split</a></li>
<li><a href="#a-general-overview"><span class="toc-section-number">4.2.2</span> A general overview</a></li>
<li><a href="#what-about-the-outcome"><span class="toc-section-number">4.2.3</span> What about the outcome?</a></li>
<li><a href="#distribution-and-main-effects-of-the-predictors-1"><span class="toc-section-number">4.2.4</span> Distribution and main effects of the predictors</a></li>
<li><a href="#relationships-between-the-predictors-1"><span class="toc-section-number">4.2.5</span> Relationships between the predictors</a></li>
<li><a href="#create-the-recipe-1"><span class="toc-section-number">4.2.6</span> Create the recipe</a></li>
</ul></li>
</ul></li>
<li><a href="#modeling"><span class="toc-section-number">5</span> Let’s boost the models</a>
<ul>
<li><a href="#burnout-data-1"><span class="toc-section-number">5.1</span> Burnout data</a>
<ul>
<li><a href="#baseline-models"><span class="toc-section-number">5.1.1</span> Baseline models</a></li>
<li><a href="#model-specification"><span class="toc-section-number">5.1.2</span> Model specification</a></li>
<li><a href="#tuning"><span class="toc-section-number">5.1.3</span> Tuning</a></li>
<li><a href="#evaluate-and-understand-the-model"><span class="toc-section-number">5.1.4</span> Evaluate and understand the model</a></li>
</ul></li>
<li><a href="#insurance-data"><span class="toc-section-number">5.2</span> Insurance data</a>
<ul>
<li><a href="#baseline-models-1"><span class="toc-section-number">5.2.1</span> Baseline models</a></li>
<li><a href="#model-specification-1"><span class="toc-section-number">5.2.2</span> Model specification</a></li>
<li><a href="#tuning-1"><span class="toc-section-number">5.2.3</span> Tuning</a></li>
<li><a href="#evaluate-and-understand-the-model-1"><span class="toc-section-number">5.2.4</span> Evaluate and understand the model</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion"><span class="toc-section-number">6</span> Conclusion</a>
<ul>
<li><a href="#pros"><span class="toc-section-number">6.1</span> Pros</a></li>
<li><a href="#cons"><span class="toc-section-number">6.2</span> Cons</a></li>
</ul></li>
<li><a href="#references"><span class="toc-section-number">7</span> References</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Boosting methods: Theory and application in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="prerequisites" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Prerequisites</h1>
<p>The reader should have some basic knowledge of the following topics:</p>
<ul>
<li><p>Statistical learning</p>
<ul>
<li><p>Train/ test split</p></li>
<li><p>Cross-validation</p></li>
<li><p>Overfitting</p></li>
<li><p>Gradient descent</p></li>
<li><p>Bias variance trade-off</p></li>
<li><p>Hyperparameters</p></li>
</ul></li>
<li><p>Regression trees and random forest models</p></li>
<li><p>Basic R knowledge (only for the applied part)</p></li>
</ul>
<!--chapter:end:index.Rmd-->
</div>
<div id="intro" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Introduction</h1>
<p>This bookdown project shows my work for the main master’s seminar from my <em>M.Sc. Mathematics in Data Science</em> at the TUM. The topic of the seminar is <em>Statistical Methods and Models</em>. During the seminar I was supervised by Prof. Claudia Czado and Özge Sahin. My topic and thus covered in this project are boosting methods. Those are unarguably one of the hottest machine learning algorithms for tabular data to date.</p>
<blockquote>
<p>“Boosting is one of the most powerful learning ideas introduced in the last
twenty years.” <span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
</blockquote>
<p>Hereby the focus will lie on the regression and not the more often discussed classification setting. The main idea of boosting is to sequentially build models from some class of base learners to finally combine them to a powerful ensemble model. As the base learners regression trees will be chosen in this project.</p>
<p>The next chapter will cover the theory behind boosting and especially tree-based gradient boosting. Besides a very prominent, efficient and successful implementation of tree-based gradient boosting namely XGBoost will be discussed in this chapter. It gained a lot of attention when it was integral to many winning submissions in machine learning competitions on the platform Kaggle and comes with some interesting tweaks to the general algorithm.</p>
<p>The application of the discussed boosting models to two real world data sets with the use of the programming language R are the content of the subsequent chapters. This will be divided into exploratory data analysis and the modeling itself. Notably the framework of <code>tidymodels</code> is used in the practical part.</p>
<!--chapter:end:01-intro.Rmd-->
</div>
<div id="theory" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Theory</h1>
<div id="the-powerful-idea-of-gradient-boosting" class="section level2" number="3.1">
<h2 number="3.1"><span class="header-section-number">3.1</span> The powerful idea of gradient boosting</h2>
<p>As roughly mentioned in the introduction section @ref(intro) the main idea of boosting is to sequentially build weak learners that form a powerful ensemble model. With weak learners models with high bias and low variance are meant that perform at least a little better than guessing. This already shows that the sequential approach of gradient boosting with weak learners stands in strong contrast to bagged ensembles like random forest. There many models with low bias and high variance are fitted in a parallel fashion and the variance is then reduced by averaging over the models.<span class="citation">[<a href="#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> It is not totally clear from which field boosting methods emerged but some claim that the work of Freund and Schapire with respect to PAC learning in the 1990s were instrumental for their growth.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> PAC learning can be considered one field within the broader field of learning theory that tries to find generalization bounds for algorithms that are probably approximately correct (PAC).<span class="citation">[<a href="#ref-pacbounds" role="doc-biblioref">3</a>]</span> This section will first cover the general setup of gradient boosting as the most prominent method to train forward stagewise additive models. Secondly tree-based gradient boosting and finally a very efficient and robust tree-based gradient boosting algorithm namely XGBoost will be discussed in detail.</p>
<div id="forward-stagewise-additive-modeling" class="section level3" number="3.1.1">
<h3 number="3.1.1"><span class="header-section-number">3.1.1</span> Forward Stagewise Additive Modeling</h3>
<p>In the setting of the data set <span class="math inline">\(\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}\)</span> with predictors <span class="math inline">\(x_i \in \mathbb{R}^m\)</span> and target <span class="math inline">\(y_i \in \mathbb{R}\)</span> boosting is fitting the following additive, still quite general, model.</p>
<span class="math display">\[\begin{equation}
  \hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}
  (\#eq:additiveModel)
\end{equation}\]</span>
<p>Where <span class="math inline">\(\mathcal{F}\)</span> is the space of learning algorithms that will be narrowed down later on. Additive expansions like this are at the core of many other powerful machine learning algorithms like Neural Networks or Wavelets.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<p>The formulation @ref(eq:additiveModel) leads to so called forward stagewise additive modeling which basically means that one sequentially adds <span class="math inline">\(f \in \mathcal{F}\)</span> to the current model <span class="math inline">\(\phi_k\)</span> without changing anything about the previous models.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> The algorithm is shown below.</p>
<hr />
<p><strong>Algorithm 1</strong>: Forward Stagewise Additive Modeling <span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\phi_0(x) = 0\)</span></p></li>
<li><p>For <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(K\)</span> do:</p>
<ul>
<li><span class="math inline">\((\beta_k,\gamma_k) = argmin_{\beta,\gamma}\sum_{i=1}^N L(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma))\)</span></li>
<li><span class="math inline">\(\phi_k(x) = \phi_{k-1}(x) + \beta_k f(x, \gamma_k)\)</span></li>
</ul></li>
</ol>
<p>Where <span class="math inline">\(\gamma\)</span> parameterizes the learner <span class="math inline">\(f \in \mathcal{F}\)</span> and the <span class="math inline">\(\beta_k\)</span> are the expansion coefficients. <span class="math inline">\(L\)</span> should be a differentiable loss function.</p>
<hr />
<p>For example for the basic <span class="math inline">\(L_2\)</span> loss the expression to be minimized simplifies to the following:</p>
<p><span class="math display">\[
L_2(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma)) = (y_i - \phi_{k-1}(x_i) - \beta f(x_i,\gamma))^2
\]</span></p>
<p>As <span class="math inline">\(y_i - \phi_{k-1}(x_i)\)</span> is just the residual of the previous model, the next model that is added corresponds to the model that best approximates the residuals of the current model. Although the <span class="math inline">\(L_2\)</span> loss has many very nice properties like the above, it lacks robustness against outliers. Therefore two alternative losses for boosting in the regression setting are worth considering.</p>
</div>
<div id="robust-loss-functions-for-regression" class="section level3" number="3.1.2">
<h3 number="3.1.2"><span class="header-section-number">3.1.2</span> Robust loss functions for regression</h3>
<p>As the <span class="math inline">\(L_2\)</span> loss squares the residuals, observations with large absolute residuals are overly important in the minimization step. This effect can be reduced intuitively by just using the <span class="math inline">\(L_1\)</span> loss i.e. minimize over the sum over just the absolute residuals. To do this is indeed a valid approach and can reduce the influence of outliers greatly and thus make the final model more robust. Another good choice could be the <strong>Huber</strong> loss which tries to get the best of <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<span class="math display">\[\begin{equation}
  L_{Huber}(y,f(x)) = \begin{cases}L_2(y,f(x)) &amp; |y-f(x)| \leq \delta \\
  2\delta |y-f(x)| - \delta^2 &amp; otherwise.
  \end{cases}
  (\#eq:huberLoss)
\end{equation}\]</span>
<p>In Figure @ref(fig:lossComp) is a comparison of the three different losses discussed so far.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<div class="figure" style="text-align: center">
<img src="_pictures/huber_loss.png" alt="Comparison of different regression loss functions." width="70%" />
<p class="caption">
(#fig:lossComp)Comparison of different regression loss functions.
</p>
</div>
<p>These alternative loss criteria are more robust but make the fitting i.e. the minimization much more complex as they do not yield such simplifications like the <span class="math inline">\(L_2\)</span> loss.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> The next step in the journey of exploring boosting is to narrow down the argument spaces of <strong>Algorithm 1</strong> (Forward Stagewise Additive Modeling) and to specify a subset of the general space of learning algorithms. This subset will be the space of Classification and Regression Tree (CART) models and in this case as the focus is on a regression task the space of regression trees. This choice is by no means arbitrary as in practice tree-based boosting algorithms have proven countless of times that they provide very robust and accurate models but still other learners might be chosen.<span class="citation">[<a href="#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> The next subsection will explore how one can actually fit such a forward stagewise additive model when using regression trees as the learner class.</p>
</div>
</div>
<div id="general-gradient-tree-boosting" class="section level2" number="3.2">
<h2 number="3.2"><span class="header-section-number">3.2</span> General gradient tree boosting</h2>
<p>From now on there is the switch from the space of learning algorithms <span class="math inline">\(\mathcal{F}\)</span> to the space of regression trees <span class="math inline">\(\mathcal{T}\)</span>. Such a regression tree can be formally expressed by:</p>
<span class="math display">\[\begin{equation}
  t(x, \gamma, R) = \sum_{j=1}^J \gamma_j I(x \in R_j) \quad \text{for  } t \in \mathcal{T}
  (\#eq:treeDef)
\end{equation}\]</span>
<p>With <span class="math inline">\(R_j\)</span> being <span class="math inline">\(J\)</span> distinct regions of the predictor space usually attained by recursive binary splitting. Moreover these regions correspond to the leafs of the tree and the number of leafs <span class="math inline">\(J\)</span> or the depth of the trees are most often hyperparameters (not trained). The <span class="math inline">\(\gamma_j \in \mathbb{R}\)</span> are the predictions for a given x if x is contained in the region <span class="math inline">\(R_j\)</span>. While it is quite easy to get the <span class="math inline">\(\gamma_j\)</span> for the regions given, most often by computing <span class="math inline">\(\gamma_j = \frac{1}{|\{x \in R_j\}|} \sum_{\{x \in R_j\}} x\)</span> , it is a much harder problem to get good distinct regions. The above mentioned recursive binary splitting is an approximation to do this and works in a top down greedy fashion.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> From now on we assume that we have an efficient way of fitting such trees to a metric outcome variable e.g. by recursive binary splitting.</p>
<p>A nice graphical example of an additive model based on trees is displayed in the figure @ref(fig:exampleAdditiveTree) below.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<div class="figure" style="text-align: center">
<img src="_pictures/boosting_easy.png" alt="Example of an additive tree ensamble." width="70%" />
<p class="caption">
(#fig:exampleAdditiveTree)Example of an additive tree ensamble.
</p>
</div>
<p>Having now the new space <span class="math inline">\(\mathcal{T}\)</span> for the general boosting model @ref(eq:additiveModel) one can write down the optimization problem that has to be solved in each step of the forward stagewise process of fitting the model.</p>
<span class="math display">\[\begin{equation}
  (\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R))
  (\#eq:oneStepTreeBoost)
\end{equation}\]</span>
<p>This can be estimated fast and quite straight forward if there is a simplification like the one seen for the <span class="math inline">\(L_2\)</span> loss. But in the more general case of an arbitrary differentiable convex loss function like the Huber loss techniques from numerical optimization are needed to derive fast algorithms.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<div id="numOpt" class="section level3" number="3.2.1">
<h3 number="3.2.1"><span class="header-section-number">3.2.1</span> Numerical optimization</h3>
<p>According to <strong>Algorithm 1</strong> the goal in order to fit the boosting model is to minimize over the full loss of the training data <span class="math inline">\(\mathcal{D}\)</span> which is the sum over all observation losses.</p>
<p><span class="math display">\[
L(\phi) = \sum_{i=1}^N L(y_i, \phi(x_i))
\]</span></p>
<p>And thus the <span class="math inline">\(\hat{\phi}\)</span> additive boosting model we try to get is the following.</p>
<p><span class="math display">\[
\hat{\phi} = argmin_{\phi} L(\phi)
\]</span></p>
<p>Now we basically follow the spirit of the general gradient descent algorithm for differentiable functions. In the general case we minimize a function <span class="math inline">\(f(x)\)</span> by stepping iteratively along the direction of the steepest descent i.e. the negative gradient. The step length can then either be a constant small scalar or be determined by a line search.</p>
<p>In the setting of the additive boosting model <span class="math inline">\(\phi\)</span> can be viewed as a vector of dimension <span class="math inline">\(N\)</span> that contains the prediction according to <span class="math inline">\(\phi\)</span> of the corresponding observation i.e. <span class="math inline">\(\phi = (\phi(x_1),...,\phi(x_N))\)</span>. So the loss function <span class="math inline">\(L(\phi)\)</span> corresponds to the <span class="math inline">\(f(x)\)</span> in the general gradient descent algorithm. Numerical optimization, here gradient descent, then solves for <span class="math inline">\(\hat{\phi}\)</span> by a sum of vectors of the same dimension as the <span class="math inline">\(\phi\)</span>.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> The result of the sum <span class="math inline">\(\phi_K\)</span> (@ref(eq:numOptSol)) can be viewed as the current proposal for the optimal <span class="math inline">\(\hat{\phi}\)</span> after <span class="math inline">\(K\)</span> optimization steps and each <span class="math inline">\(h^{(k)}\)</span> is the proposed improvement step.</p>
<span class="math display">\[\begin{equation}
  \phi_K = \sum_{k=0}^K h^{(k)} \quad h^{(k)} \in \mathbb{R}^N 
  (\#eq:numOptSol)
\end{equation}\]</span>
<p>While <span class="math inline">\(h^{(0)}\)</span> is just an initial guess the subsequent <span class="math inline">\(h^{(k)}\)</span> are again the prediction vectors of the corresponding model out of <span class="math inline">\(\mathcal{T}\)</span> i.e. <span class="math inline">\(h^{(k)}_{i} = t_k(x_i)\)</span> with <span class="math inline">\(t \in \mathcal{T}\)</span>. This means that each <span class="math inline">\(\phi_k = \phi_{k-1} + h^{(k)}\)</span>. The <span class="math inline">\(h^{(k)}\)</span> are calculated via the gradient which finally comes into play. As mentioned above one minimizes the loss the most by going towards the direction of the steepest descent. For @ref(eq:numOptSol) follows from its additive formulation and defining <span class="math inline">\(g^{(k)}\)</span> as the gradient of <span class="math inline">\(L(\phi_k)\)</span> evaluated for <span class="math inline">\(\phi_{k-1}\)</span> the update <span class="math inline">\(h^{(k)} = -\lambda_k g^{(k)}\)</span>. As we assumed the loss to be differentiable we see that <span class="math inline">\(g^{(k)}\)</span> is well defined. Here <span class="math inline">\(\lambda_k\)</span> is the usual step length for gradient descent methods. It is the solution of the line search <span class="math inline">\(\lambda_k = argmin_{\lambda} L(\phi_{k-1} - \lambda g^{(k)})\)</span>. This <span class="math inline">\(\lambda_k\)</span> almost exactly corresponds to the <span class="math inline">\(\beta_k\)</span> in <strong>Algorithm 1</strong> although here the optimization is performed for every region of the tree separately.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<p>With these insights it is clear that the tree predictions correspond to the negative gradient <span class="math inline">\(-g^{(k)}\)</span>. Of course the predictions are not independent as the prediction is constant for each leaf of the tree. So the new optimization proposed by numerical optimization via gradient boosting is given in @ref(eq:oneStepTreeBoostnew) below.</p>
<span class="math display">\[\begin{equation}
  (\tilde{\gamma}^{(k)},\tilde{R}^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N [-g^{(k)}_{i} -  t(x_i,\gamma,R)] ^2
  (\#eq:oneStepTreeBoostnew)
\end{equation}\]</span>
<p>In words this just means fitting a regression tree by least squares to the negative gradients that were evaluated with the current predictions. The solution regions will not exactly match the ones from @ref(eq:oneStepTreeBoost) but should be very similar.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> After having estimated the regions one estimates the parameters <span class="math inline">\(\gamma\)</span> by solving the line search @ref(eq:gammaLineSearch).</p>
<span class="math display">\[\begin{equation}
  \tilde{\gamma}^{(k)}_{j} = argmin_{\gamma^{(k)}_{j}} \sum_{x \in R^{(k)}_{j}} L(y_i,\phi_{k-1}(x_i) + \gamma^{(k)}_{j})
  (\#eq:gammaLineSearch)
\end{equation}\]</span>
<p>Putting all of this back together with <strong>Algorithm 1</strong> results in <strong>Algorithm 2</strong> that covers a general algorithm for tree-based gradient boosting.</p>
<hr />
<p><strong>Algorithm 2</strong>: Tree-based gradient boosting <span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\phi_0(x)\)</span> as a singular node tree.</p></li>
<li><p>For <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(K\)</span> do:</p>
<ul>
<li><p>For <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(N\)</span> compute:</p>
<p><span class="math inline">\(g^{(k)}_{i} = \bigg[\frac{\partial L(y_i, \phi(x_i))}{\partial \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}\)</span></p></li>
<li><p>Fit a regression tree by least squares to the outcome vector <span class="math inline">\(-g^{(k)}\)</span> in order to get the <span class="math inline">\(J^{(k)}\)</span> distinct regions <span class="math inline">\(\tilde{R}^{(k)}_j\)</span>.</p></li>
<li><p>For each of these <span class="math inline">\(J^{(k)}\)</span> regions perform a line search in order to compute the leaf predictions <span class="math inline">\(\tilde{\gamma}^{(k)}_{j}\)</span> exactly like in @ref(eq:gammaLineSearch).</p></li>
<li><p>Set <span class="math inline">\(\phi_k(x) = \phi_{k-1}(x) + t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)\)</span> with <span class="math inline">\(t \in \mathcal{T}\)</span></p></li>
</ul></li>
</ol>
<hr />
<p>The only unknowns in this algorithm are now the differentiable loss function and the hyperparameters like the <span class="math inline">\(J^{(k)}\)</span> and the <span class="math inline">\(K\)</span>. While choices for the hyperparameters are discussed further below, the following table @ref(tab:lossGradients) displays the gradients for the losses discussed so far.</p>
<table>
<caption>(#tab:lossGradients) Gradients of the discussed losses <span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></caption>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th>Loss</th>
<th>Gradient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(L_2\)</span>: <span class="math inline">\((y_i - \phi(x_i))^2\)</span></td>
<td><span class="math inline">\(2(y_i - \phi(x_i))\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(L_1\)</span>: <span class="math inline">\(|y_i - \phi(x_i)|\)</span></td>
<td><span class="math inline">\(sign(y_i - \phi(x_i))\)</span></td>
</tr>
<tr class="odd">
<td>Huber loss @ref(eq:huberLoss)</td>
<td><p><span class="math inline">\(y_i - \phi(x_i)\)</span> for <span class="math inline">\(|y_i - \phi(x_i)| \leq \delta\)</span></p>
<p><span class="math inline">\(\delta sign(y_i - \phi(x_i))\)</span> otherwise, with <span class="math inline">\(\delta\)</span> quantile of <span class="math inline">\(|y_i - \phi(x_i)|\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div id="single-tree-depth" class="section level3" number="3.2.2">
<h3 number="3.2.2"><span class="header-section-number">3.2.2</span> Single tree depth</h3>
<p>The question which <span class="math inline">\(J^{(k)}\)</span> should be used at each iteration is now shortly discussed. Basically using the tree depth (<span class="math inline">\(log_2(J^{(k)}) = Depth^{(k)}\)</span>) of one means allowing only the main effects and no interactions. A value of 2 already allows all two way interaction effects and so on. As one stacks these models additive and does not restrict oneself to a single one, the building of a large tree and then pruning it back at each iteration as the regular CART algorithms do would be a computational overkill. Instead it has proven to be sufficient and efficient in practice to set the values <span class="math inline">\(Depth^{(k)}\)</span> to a constant <span class="math inline">\(Depth \approx 6\)</span>.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> Also decision stumps (<span class="math inline">\(Depth = 1\)</span>) could be used but may require a lot more iterations.</p>
</div>
<div id="combOver" class="section level3" number="3.2.3">
<h3 number="3.2.3"><span class="header-section-number">3.2.3</span> Combat overfitting</h3>
<p>No learning algorithm could be fully covered without treating the good old problem of overfitting. In the setting of boosting the experienced eye could have spotted the problem of overfitting already in the definition of the additive model @ref(eq:additiveModel). There the number <span class="math inline">\(K\)</span> of models that form the ensemble was introduced but was not discussed further till now. Of course one can arbitrarily fit or better say remember some given training data in order to minimize the loss with such an additive model by letting <span class="math inline">\(K\)</span> be arbitrarily large. This comes from the fact that the loss reduces usually after each iteration over <span class="math inline">\(K\)</span>.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> The easiest way to prevent overfitting is to have a validation set at hand which is disjoint from the training data. Having such a validation set at hand can be used to monitor the loss on the unseen data. In the case the loss would rise again on the validation data one can consider to stop the algorithm and to use the current iteration number for the final parameter <span class="math inline">\(K\)</span>. This approach is often called early stopping. Besides that there are methods that regularize the individual trees that are fitted in each iteration. Two of those will be discussed now.</p>
<div id="shrinkage" class="section level4" number="3.2.3.1">
<h4 number="3.2.3.1"><span class="header-section-number">3.2.3.1</span> Shrinkage</h4>
<p>Shrinkage basically refers to just introducing a learning rate <span class="math inline">\(\eta\)</span>. This learning rate <span class="math inline">\(\eta\)</span> scales the contribution of the new model. In general the learning rate should be in <span class="math inline">\((0,1]\)</span> but in practice it has been shown that rather small values like <span class="math inline">\(\eta &lt; 0.1\)</span> work very good.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> As almost everything in life this does not come for free. A smaller learning rate usually comes with a computational cost as with lower <span class="math inline">\(\eta\)</span> a larger <span class="math inline">\(K\)</span> is required. Those two hyperparameters represent a trade-off in a way. In practice it is advisable to use a small learning rate <span class="math inline">\(\eta\)</span> and adjust the <span class="math inline">\(K\)</span> accordingly which in this case would mean to make the <span class="math inline">\(K\)</span> large enough until one reaches an early stopping point. Still it is good to keep in mind that a <span class="math inline">\(\eta\)</span> that is too small can lead to an immense and unnecessary computational effort. All in all using shrinkage the update step in <strong>Algorithm 2</strong> changes to the following.</p>
<span class="math display">\[\begin{equation}
  \phi_k(x) = \phi_{k-1}(x) + \eta * t(x,\tilde {\gamma}^{(k)}_{j},\tilde{R}^{(k)}_j)
  (\#eq:shrinkage)
\end{equation}\]</span>
</div>
<div id="subsampling" class="section level4" number="3.2.3.2">
<h4 number="3.2.3.2"><span class="header-section-number">3.2.3.2</span> Subsampling</h4>
<p>The other method to regularize the trees will be subsampling. There are two different kinds of subsampling. The first one is row-subsampling which basically means just using a random fraction of the training data in each iteration when fitting the new tree. Common values are <span class="math inline">\(\frac{1}{2}\)</span> but for a larger training set the value can be chosen to be smaller. This does not only help to prevent overfitting and thus achieving a better predictive performance but reduces also the computational effort in each iteration.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span> The approach is very similar to the dropout technique in the setting of deep neural networks.</p>
<p>Moreover one can apply column-subsampling or feature-subsampling. Here only a fraction of the features or an explicit number of the features are used in each iteration. This is the exact same method and intention as the one applied in random forest models. Again the technique not only boosts performance on unseen data but also reduces the computational time.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span> Still it must be noted that by using one or both subsampling methods one introduces one or two additional hyperparameters to the model that have to be tuned.</p>
<p>Having the <strong>Algorithm 2</strong> for tree-based gradient boosting alongside some regularization techniques it is time to look at a very popular, efficient and powerful open source implementation.</p>
</div>
</div>
</div>
<div id="xgboost-a-highly-efficient-implementation" class="section level2" number="3.3">
<h2 number="3.3"><span class="header-section-number">3.3</span> XGBoost a highly efficient implementation</h2>
<p>The XGBoost algorithm is a highly scalable, efficient and successful implementation and optimization of <strong>Algorithm 2</strong> introduced by Chen and Guestrin in 2016.<span class="citation">[<a href="#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span> It has gained a lot of spotlight when it was integral for many winning submissions at kaggle machine learning challenges. In the following the most important tweaks that are proposed to <strong>Algorithm 2</strong> are covered.</p>
<div id="regularized-loss" class="section level3" number="3.3.1">
<h3 number="3.3.1"><span class="header-section-number">3.3.1</span> Regularized loss</h3>
<p>Instead of just minimizing a convex and differentiable loss <span class="math inline">\(L\)</span> XGBoost minimizes the following regularized loss.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<span class="math display">\[\begin{equation}
  \begin{split}
  \mathcal{L}(\phi) = L(\phi) + \sum_{k=1}^K \Omega(t_k) \\
  \text{where } \Omega(t) = \nu J + \frac{1}{2} \lambda ||\gamma||^2 \quad t \in \mathcal{T}
  \end{split}
  (\#eq:regLoss)
\end{equation}\]</span>
<p>Here <span class="math inline">\(J\)</span> and <span class="math inline">\(\gamma\)</span> again parameterize the regression trees <span class="math inline">\(t \in \mathcal{T}\)</span> as defined in @ref(eq:treeDef). As evident from the formula both hyperparameters <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\lambda\)</span> favor when increased less complex models at each iteration. This is again a measure against overfitting.</p>
<p>This regularized objective leads to a slightly modified version of the minimization problem @ref(eq:oneStepTreeBoost) that has to be solved in each iteration.</p>
<span class="math display">\[\begin{equation}
  (\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R)) + \Omega(t(x_i,\gamma,R))
  (\#eq:oneStepXGBoost)
\end{equation}\]</span>
<p>Instead of the first order approach, that has been introduced in @ref(numOpt), XGBoost uses a second order approximation to further simplify the objective. This means that besides the first order gradient <span class="math inline">\(g^{(k)}\)</span> of the loss function evaluated at <span class="math inline">\(\phi_{k-1}\)</span> also the second order gradient <span class="math inline">\(z^{(k)}_i = \bigg[\frac{\partial^2 L(y_i, \phi(x_i))}{\partial^2 \phi(x_i)}\bigg]_{\phi = \phi_{k-1}}\)</span> is used. By neglecting constants one reaches the approximate new objective below.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
<span class="math display">\[\begin{equation}
\begin{split}
  \tilde{\mathcal{L}}^{(k)} &amp; = \sum_{i=1}^N [g^{(k)}_i t^{(k)}(x_i) + \frac{1}{2} t^{(k)}(x_i)^2] + \Omega(t^{(k)}) \\
  &amp; = \sum_{j=1}^J [(\sum_{\{i|x_i \in R^{(k)}_j\}} g^{(k)}_i) \gamma^{(k)}_j + \frac{1}{2} (\sum_{\{i|x_i \in R^{(k)}_j\}} z^{(k)}_i + \lambda) (\gamma^{(k)}_j)^2 ] + \nu J
\end{split}
(\#eq:oneappStepXGBoost)
\end{equation}\]</span>
<p>This representation is used as it can be decomposed by the <span class="math inline">\(J\)</span> leafs of the tree. One can then define a scoring function for split finding that only depends on <span class="math inline">\(g^{(k)}\)</span> and <span class="math inline">\(z^{(k)}\)</span>. This approach speeds up the split finding and allows further parallel computations with respect to the leafs.</p>
</div>
<div id="shrinkage-and-subsampling" class="section level3" number="3.3.2">
<h3 number="3.3.2"><span class="header-section-number">3.3.2</span> Shrinkage and subsampling</h3>
<p>As discussed above in @ref(combOver) shrinkage and subsampling are great ways to regularize the model and prevent overfitting. XGBoost implements both shrinkage and subsampling. It enables the user to use column as well as row subsampling. The authors claim that in practice column subsampling has prevented overfitting more than row subsampling.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span></p>
</div>
<div id="even-more-tweaks" class="section level3" number="3.3.3">
<h3 number="3.3.3"><span class="header-section-number">3.3.3</span> Even more tweaks</h3>
<p>Besides the changes above the authors also implemented an effective approximate algorithm for split finding for the tree building step. A very nice attribute of this approximate procedure is that the algorithm is sparsity aware i.e. the processing of missing values or 0 values is very efficient. This is done via a learned default direction in each split. Moreover they used very efficient data structures to allow a lot of parallel processing.<span class="citation">[<a href="#ref-xgboostPaper" role="doc-biblioref">4</a>]</span> It is written in C++ but has APIs in various languages like in R via the <code>xgboost</code> package.<span class="citation">[<a href="#ref-xgboost_package" role="doc-biblioref">5</a>]</span></p>
</div>
<div id="hyperparameters-overview" class="section level3" number="3.3.4">
<h3 number="3.3.4"><span class="header-section-number">3.3.4</span> Hyperparameters overview</h3>
<p>As the <strong>tidymodels</strong> framework will be used in the applied part, the according names from the parsnip interface for tree-based boosting i.e. <code>boost_tree</code> are shown below.<span class="citation">[<a href="#ref-tidymodels" role="doc-biblioref">6</a>]</span></p>
<table>
<caption>(#tab:xgboostHyper) XGBoost hyperparameters overview.</caption>
<colgroup>
<col width="14%" />
<col width="16%" />
<col width="69%" />
</colgroup>
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>Notation above</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>trees</code></td>
<td><span class="math inline">\(K\)</span></td>
<td>Number of trees in the ensemble.</td>
</tr>
<tr class="even">
<td><code>tree_depth</code></td>
<td><span class="math inline">\(max(log_2J^{(k)})\)</span></td>
<td>Maximum depth of the trees.</td>
</tr>
<tr class="odd">
<td><code>learn_rate</code></td>
<td><span class="math inline">\(\eta\)</span></td>
<td>Learning rate (shrinkage).</td>
</tr>
<tr class="even">
<td><code>min_n</code></td>
<td></td>
<td>Minimum number of observations in terminal region <span class="math inline">\(R^{(k)}_j\)</span>.</td>
</tr>
<tr class="odd">
<td><code>lambda</code></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(L_2\)</span> regularization parameter on <span class="math inline">\(\gamma\)</span> as in @ref(eq:oneappStepXGBoost) (default = 0).</td>
</tr>
<tr class="even">
<td><code>alpha</code></td>
<td></td>
<td><span class="math inline">\(L_1\)</span> regularization parameter on <span class="math inline">\(\gamma\)</span> (default = 0).</td>
</tr>
<tr class="odd">
<td><code>loss_reduction</code></td>
<td><span class="math inline">\(\nu\)</span></td>
<td>Minimal loss reduction for terminal partitions. (default = 0)</td>
</tr>
<tr class="even">
<td><code>mtry</code></td>
<td></td>
<td>Proportion or number of columns for column subsampling.</td>
</tr>
<tr class="odd">
<td><code>sample_size</code></td>
<td></td>
<td>Proportion of observations for row subsampling.</td>
</tr>
<tr class="even">
<td><code>stop_iter</code></td>
<td></td>
<td>Allowed number of rounds without improvement before early stopping.</td>
</tr>
</tbody>
</table>
<p>Note that the <span class="math inline">\(L_1\)</span> penalization term would be added to the <span class="math inline">\(\Omega\)</span> term in the loss <span class="math inline">\(\mathcal{L}\)</span> of XGBoost.</p>
<p>This closes the chapter on the theory of boosting methods with a focus on tree-based gradient boosting. The discussed tools will be put to the test in the subsequent sections where real world data will be analyzed with them.</p>
<!--chapter:end:02-theory.Rmd-->
</div>
</div>
</div>
<div id="eda" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Explore the data</h1>
<p>Before one starts with the actual modeling it is crucial to get to know the data and to bring it to the correct format. This process of getting familiar with the data is well known as Exploratory Data Analysis (EDA). To do this many packages are used.<span class="citation">[<a href="#ref-tidymodels" role="doc-biblioref">6</a>–<a href="#ref-ggtext" role="doc-biblioref">9</a>, <a href="#ref-viridisLite" role="doc-biblioref">10</a>, <a href="#ref-patchwork" role="doc-biblioref">11</a>–<a href="#ref-plotly" role="doc-biblioref">16</a>]</span> The most important ones will be loaded below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># general data handling tools</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels) <span class="co"># data modeling and preprocessing</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># color palettes</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridis)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork) <span class="co"># composing of ggplots</span></span></code></pre></div>
<p>Before the fun can begin a quick outline of the steps performed on each data set.</p>
<ul>
<li>A general overview of the classes of the features and a visualization to detect any missing values.</li>
<li>The distribution and the main effect on the outcome variable of each feature.</li>
<li>The pairwise relationships of the predictors.</li>
<li>Optionally some feature engineering.</li>
<li>Using the gained knowledge to fix all pre-processing and feature engineering steps by using a <code>recipes::recipe</code>.</li>
</ul>
<p>If one is mainly interested in the models themselves one can just have a look at the recipes and skip the rest of this section.</p>
<p>Some might ask why no interactions of predictors are covered in this EDA. If one would use a standard OLS, lasso or ridge regression it would be very important to have a look at them but as the focus here is on tree-based gradient boosting one already includes interactions if one is not restrictive to regression stumps. So let’s start with the first data set!</p>
<div id="burnout-data" class="section level2" number="4.1">
<h2 number="4.1"><span class="header-section-number">4.1</span> Burnout data</h2>
<p>The data is from the machine learning challenge <em>HackerEarth Machine Learning Challenge: Are your employees burning out?</em>. And can be downloaded here: <a href="https://www.kaggle.com/blurredmachine/are-your-employees-burning-out?select=train.csv" class="uri">https://www.kaggle.com/blurredmachine/are-your-employees-burning-out?select=train.csv</a></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>burnout_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;_data/burn_out_train.csv&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># convert colnames to snake_case</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(burnout_data) <span class="ot">&lt;-</span> <span class="fu">tolower</span>(</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  stringr<span class="sc">::</span><span class="fu">str_replace_all</span>(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">colnames</span>(burnout_data),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; &quot;</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;_&quot;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># omit missing values in the outcome variable</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>burnout_data <span class="ot">&lt;-</span> burnout_data[<span class="sc">!</span><span class="fu">is.na</span>(burnout_data<span class="sc">$</span>burn_rate),]</span></code></pre></div>
<div id="train-test-split" class="section level3" number="4.1.1">
<h3 number="4.1.1"><span class="header-section-number">4.1.1</span> Train-test split</h3>
<p>To not allow information leakage the train-test split is performed at the very start of the whole analysis.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>burnout_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(burnout_data, <span class="at">prop =</span> <span class="fl">0.80</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>burnout_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(burnout_split)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>burnout_test  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(burnout_split)</span></code></pre></div>
<p>The training data set contains 17300 rows and 9 variables.</p>
<p>The test data set contains 4326 observations and naturally also 9 variables.</p>
</div>
<div id="quick-general-overview" class="section level3" number="4.1.2">
<h3 number="4.1.2"><span class="header-section-number">4.1.2</span> Quick general overview</h3>
<p>First look at the classes of the variables.</p>
<table>
<thead>
<tr class="header">
<th align="left">column</th>
<th align="left">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">employee_id</td>
<td align="left">character</td>
</tr>
<tr class="even">
<td align="left">date_of_joining</td>
<td align="left">Date</td>
</tr>
<tr class="odd">
<td align="left">gender</td>
<td align="left">character</td>
</tr>
<tr class="even">
<td align="left">company_type</td>
<td align="left">character</td>
</tr>
<tr class="odd">
<td align="left">wfh_setup_available</td>
<td align="left">character</td>
</tr>
<tr class="even">
<td align="left">designation</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">resource_allocation</td>
<td align="left">numeric</td>
</tr>
<tr class="even">
<td align="left">mental_fatigue_score</td>
<td align="left">numeric</td>
</tr>
<tr class="odd">
<td align="left">burn_rate</td>
<td align="left">numeric</td>
</tr>
</tbody>
</table>
<p>A general visualization of the whole data set to detect missing values below.</p>
<p><img src="boosting_methods_files/figure-html/visdat-1.png" width="672" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># percentage of missing values in the training data set</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">rowSums</span>(<span class="fu">is.na</span>(burnout_train)) <span class="sc">&gt;</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.1419075</code></pre>
<p>As we know that XGBoost can handle missing values we do not have to be concerned. Although one could of course think about imputation or even removal.</p>
</div>
<div id="what-about-the-outcome-variable" class="section level3" number="4.1.3">
<h3 number="4.1.3"><span class="header-section-number">4.1.3</span> What about the outcome variable?</h3>
<p><code>burn_rate</code>: For each employee telling the rate of burnout should be in <span class="math inline">\([0,1]\)</span>. The greater the score the worse the burnout (0 means no burnout at all). As the variable is continuous we have a regression task. Yet it has bounds which has to be treated with when predicting.</p>
<p>The five point summary below shows that the full range is covered and no invalid values are in the data.</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.3200  0.4500  0.4531  0.5900  1.0000</code></pre>
<p>Now the distribution of the outcome.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The distribution of the outcome is very much symmetrical and bell shaped around 0.5 and the whole defined region <span class="math inline">\([0,1]\)</span> is covered quite well. Actually by overlaying a normal distribution with the sample mean <span class="math inline">\(\hat{\mu}\)</span> and the sample standard deviation <span class="math inline">\(\hat{\sigma}^2\)</span> as the parameters one can clearly see that the outcome almost perfectly follows a normal distribution. One could further fit a Q-Q-plot to visualize the normality. <strong>BUT</strong> of course here there is a bounded domain while the normal distribution has the whole <span class="math inline">\(\mathbb{R}\)</span> as domain. This bounded domain does not interfere with the boosted model as tree-based models do not superimpose a distribution assumptions upon the target variable. Nevertheless one can transform the outcome with the empirical logit <span class="math inline">\(log(\frac{y_i+0.5}{1-y_i+0.5})\)</span>.By doing this one removes the bounds on the target. One can then re-transform the predictions in the end by applying <span class="math inline">\(\frac{2}{exp(-y)+1}-0.5\)</span>. Here to see whether this transformation changes the behavior or improves the boosting model
one will have a look not only at the untransformed target <code>burn_rate</code> but also at the transformed one <code>burn_rate_trans</code>. The focus will be on the untransformed modeling as the low pre-processing strength of such boosting models should be emphasized. Below is the distribution of the transformed one.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add transformed outcome</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>burnout_train<span class="sc">$</span>burn_rate_trans <span class="ot">&lt;-</span> <span class="fu">log</span>((burnout_train<span class="sc">$</span>burn_rate <span class="sc">+</span> <span class="fl">0.5</span>) <span class="sc">/</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                                       (<span class="fl">1.5</span> <span class="sc">-</span> burnout_train<span class="sc">$</span>burn_rate))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>burnout_test<span class="sc">$</span>burn_rate_trans <span class="ot">&lt;-</span> <span class="fu">log</span>((burnout_test<span class="sc">$</span>burn_rate <span class="sc">+</span> <span class="fl">0.5</span>) <span class="sc">/</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                                       (<span class="fl">1.5</span> <span class="sc">-</span> burnout_test<span class="sc">$</span>burn_rate))</span></code></pre></div>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The transformed outcome basically resembles exactly the same properties as the untransformed one but the nice thing is that the bounds were removed. The further EDA will be based on the untransformed variable but the implications are the same due to the choice of the transformation via the empirical logit.</p>
</div>
<div id="distribution-and-main-effects-of-the-predictors" class="section level3" number="4.1.4">
<h3 number="4.1.4"><span class="header-section-number">4.1.4</span> Distribution and main effects of the predictors</h3>
<div id="employee-id" class="section level4" number="4.1.4.1">
<h4 number="4.1.4.1"><span class="header-section-number">4.1.4.1</span> Employee ID</h4>
<p><code>employee_id</code> is just an ID variable and thus is not useful for any prediction model. But one has to check for duplicates.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TRUE if there are NO duplicates</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>burnout_train <span class="sc">%&gt;%</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(employee_id) <span class="sc">%&gt;%</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">n =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nrow</span>() <span class="sc">==</span> <span class="fu">nrow</span>(burnout_train)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Thus there are no duplicates which is good.</p>
</div>
<div id="date-of-joining" class="section level4" number="4.1.4.2">
<h4 number="4.1.4.2"><span class="header-section-number">4.1.4.2</span> Date of joining</h4>
<p><code>date_of_joining</code> is the date the employee has joined the company. Thus a continuous variable that most likely needs some kind of feature engineering.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Although there is a lot of variation no major trends in hirings are visible from this plot. Overall the variable seems to be quite equally distributed over the year 2008.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>In its raw form the variable <code>date_of_joining</code> seems not to have a notable main effect on the outcome variable. Nevertheless the feature will be used in the model and as tree-based models have an in-built feature selection one can see after the fitting if the feature was helpful overall. The feature will not be included just as an integer (the default format how Dates are represented) but rather some more features like weekday or month will be extracted from the raw variable further down the road.</p>
</div>
<div id="gender" class="section level4" number="4.1.4.3">
<h4 number="4.1.4.3"><span class="header-section-number">4.1.4.3</span> Gender</h4>
<p><code>gender</code> represents the gender of the employee. Definitely a categorical variable.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># have a look at the discrete distribution</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">factor</span>(burnout_train<span class="sc">$</span>gender))</span></code></pre></div>
<pre><code>## Female   Male 
##   9039   8261</code></pre>
<p>The two classes are well balanced. Now a look at the main effect of the feature.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>For both classes the distributions are very similar and symmetrical. It seems like the male employees have overall a slightly higher risk of having a higher burn score i.e. a burnout.</p>
</div>
<div id="company-type" class="section level4" number="4.1.4.4">
<h4 number="4.1.4.4"><span class="header-section-number">4.1.4.4</span> Company type</h4>
<p><code>company_type</code> is a binary categorical variable that indicates whether the company is a service or product company.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># have a look at the discrete distribution</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">factor</span>(burnout_train<span class="sc">$</span>company_type))</span></code></pre></div>
<pre><code>## Product Service 
##    6002   11298</code></pre>
<p>In this case the classes are not fully balanced but each class is still well represented. Now a look at the main effect of the feature.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>For both classes the distributions are almost identical and symmetrical. From an univariate point of view no notable main effect is visible from these visualizations.</p>
</div>
<div id="work-from-home-setup" class="section level4" number="4.1.4.5">
<h4 number="4.1.4.5"><span class="header-section-number">4.1.4.5</span> Work from home setup</h4>
<p><code>wfh_setup_available</code> indicates whether a working from home setup is available for the employee. So this is again a binary variable.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># have a look at the discrete distribution</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">factor</span>(burnout_train<span class="sc">$</span>wfh_setup_available))</span></code></pre></div>
<pre><code>##   No  Yes 
## 7982 9318</code></pre>
<p>The two classes are well balanced. Now a look at the main effect of the feature.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Again both distributions are quite similar i.e. bell shaped and symmetrical. Here quite a main effect is visible. A work from home setup most likely has a positive influence on the wellbeing and thus lowers the risk for a high burn rate.</p>
</div>
<div id="designation" class="section level4" number="4.1.4.6">
<h4 number="4.1.4.6"><span class="header-section-number">4.1.4.6</span> Designation</h4>
<p><code>designation</code> A rate within <span class="math inline">\([0,5]\)</span> that represents the designation in the company for the employee. High values indicate a greater amount of designation.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># unique values of the feature</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">unique</span>(burnout_train<span class="sc">$</span>designation)</span></code></pre></div>
<pre><code>## [1] 0 2 3 1 4 5</code></pre>
<p>As the feature has a natural ordering this variable will be treated as an ordinal one i.e. be encoded with the integers and not by one-hot-encoding.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Here clearly the more extreme levels of designation are less represented in the data. This makes total sense w.r.t. the meaning of the variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>A strong main effect is visible in the plot. The plot also further strengthens the hypothesis that we should treat the feature as ordinal. A higher level of designation seems to have an influence on the risk of having a burnout. For example employees from the training data set with a level of designation below 3 never even achieved a maximal burn score of one.</p>
</div>
<div id="resource-allocation" class="section level4" number="4.1.4.7">
<h4 number="4.1.4.7"><span class="header-section-number">4.1.4.7</span> Resource allocation</h4>
<p><code>resource_allocation</code> A rate within <span class="math inline">\([1,10]\)</span> that represents the resource allocation to the employee. High values indicate more resources allocated to the employee.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># unique values of the feature</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">unique</span>(burnout_train<span class="sc">$</span>resource_allocation)</span></code></pre></div>
<pre><code>##  [1]  2  4  3  6  5  8  1  7 NA 10  9</code></pre>
<p>Here again the question is whether one should encode this variable as a categorical or an ordinal categorical feature. In this case as there are quite some levels and again a natural ordering the variable will be encoded as a continuous integer score.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>A similar behavior as the one of the previous variable is visible. But here there are some missing values (NA’s).</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>A strong main effect is visible in the plot. The plot again further strengthens the hypothesis that we should treat this feature as ordinal. A higher amount of resources assigned to an employee seems to have a positive influence on the risk of having a burnout. The missing values do not seem to have some structure as they replicate the base distribution of the outcome variable.</p>
</div>
<div id="mental-fatigue-score" class="section level4" number="4.1.4.8">
<h4 number="4.1.4.8"><span class="header-section-number">4.1.4.8</span> Mental fatigue score</h4>
<p><code>mental_fatigue_score</code> is the level of mental fatigue the employee is facing.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># number of unique values</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(burnout_train<span class="sc">$</span>mental_fatigue_score)) </span></code></pre></div>
<pre><code>## [1] 102</code></pre>
<p>This variable will without a question be treated in a continuous way.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Although there is a very slight skew towards a higher mental fatigue score the overall distribution is still more or less bell shaped and quite symmetrical. Moreover the whole allowed range is covered and the bounds are not violated. Next the main effect of the variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>This scatterplot shows drastic results! The mental fatigue score has an almost perfect linear relationship with the outcome variable. This is also underlined by the very high pearson correlation. This indicates that mental fatigue score will be a most important predictor. If a communication with the data collector would be possible it would be important to check whether the two scores have common confounding variables as then one would have to question the practical usability of this predictor. This comes from the fact that no model would be needed if it was as hard to collect the data about the predictors as the outcome data. Moreover there are 1569 missing values in the feature so for those the model has to rely on the other maybe more weak predictors. It should be noted that when evaluating the final model one should consider to compare its performance to a trivial model (like a single intercept model). When constructing such a trivial model one could and maybe should also use this variable (when available) to get a trivial prediction by scaling the <code>mental_fatigue_score</code> feature by a simple scalar.</p>
</div>
</div>
<div id="relationships-between-the-predictors" class="section level3" number="4.1.5">
<h3 number="4.1.5"><span class="header-section-number">4.1.5</span> Relationships between the predictors</h3>
<p>An exploration of the relationships between the predictors could also be done by having a look at a correlation and scatterplot matrix. This approach is much quicker than looking at each pairwise relationship individually but also not as precise as the one presented here. Especially for a lot of features such a matrix can get too big to grasp the subtle details. If this is necessary depends on the use case. A very good option if one wants an initial overview is the function <code>GGally::ggpairs</code>. So in the following each pairwise relationship will be covered.</p>
<div id="date-of-joining-vs.-the-others" class="section level4" number="4.1.5.1">
<h4 number="4.1.5.1"><span class="header-section-number">4.1.5.1</span> Date of joining vs. the others</h4>
<p><img src="boosting_methods_files/figure-html/ggpairs-1.png" width="90%" /></p>
<p>No major relationship can be detected here.</p>
</div>
<div id="gender-vs.-the-remaining" class="section level4" number="4.1.5.2">
<h4 number="4.1.5.2"><span class="header-section-number">4.1.5.2</span> Gender vs. the remaining</h4>
<p><strong>Contingency tables for the comparison of two binary features:</strong></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gender vs Company type</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(burnout_train<span class="sc">$</span>gender, burnout_train<span class="sc">$</span>company_type)</span></code></pre></div>
<pre><code>##         
##          Product Service
##   Female    3086    5953
##   Male      2916    5345</code></pre>
<p>No huge tendency visible.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gender vs Work from home setup</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(burnout_train<span class="sc">$</span>gender, burnout_train<span class="sc">$</span>wfh_setup_available)</span></code></pre></div>
<pre><code>##         
##            No  Yes
##   Female 3878 5161
##   Male   4104 4157</code></pre>
<p>Slightly more women have a work from home setup available.</p>
<p><strong>Now the ordinal variables:</strong></p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>It has to be noted again that female and male emplyees are almost equally represented in the data set. Thus one can see from the above plot that the biggest difference in distribution is for the levels 1 and 4 with opposing effects. While male employees more often have a quite high designation of 4 females are the much more frequent employee with designation level 1.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Here a major shift in distribution is visible towards men getting more resources allocated to them. This reflects the society that still promotes men much more often to high paying jobs that most often come with resource responsibility.</p>
<p><strong>Now the mental fatigue score:</strong></p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>This is of course very similar to the main effect of the <code>gender</code> variable as the outcome and the feature <code>mental_fatigue_score</code> are highly linearly correlated.</p>
</div>
<div id="company-type-vs.-the-remaining" class="section level4" number="4.1.5.3">
<h4 number="4.1.5.3"><span class="header-section-number">4.1.5.3</span> Company type vs. the remaining</h4>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Company type vs Work from home setup</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(burnout_train<span class="sc">$</span>company_type, burnout_train<span class="sc">$</span>wfh_setup_available)</span></code></pre></div>
<pre><code>##          
##             No  Yes
##   Product 2805 3197
##   Service 5177 6121</code></pre>
<p>No notable trend.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>No trend here either.</p>
</div>
<div id="work-from-home-setup-vs.-the-remaining" class="section level4" number="4.1.5.4">
<h4 number="4.1.5.4"><span class="header-section-number">4.1.5.4</span> Work from home setup vs. the remaining</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>A work from home setup is way more often available for employees with a lower designation (<span class="math inline">\(\leq 2\)</span>).</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>The same structure as in the previous comparison is visible here again. Employees with a lower amount of resources allocated to them have more often a work from home setup available. This could be due to the fewer responsibilities they have in the business.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Again this is of course very similar to the main effect of the <code>wfh_setup_available</code> variable as the outcome and the feature <code>mental_fatigue_score</code> are highly linearly correlated.</p>
</div>
<div id="designation-vs-the-remaining" class="section level4" number="4.1.5.5">
<h4 number="4.1.5.5"><span class="header-section-number">4.1.5.5</span> Designation vs the remaining</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Here a strong quite linear relationship is visible. This is sensible as often more resource responsibility is given to employees with high designation.</p>
<p>The last two relationships will be omitted here as both for the variable <code>designation</code> as well as for <code>resource_allocation</code> the comparison with <code>mental_fatigue_score</code> will be very similar to the main effect of the two variables. This comes again from the high correlation of the latter with the outcome.</p>
<p>Overall some stronger and mainly less strong relationships between the predictors could be detected. Not like in ordinary least squares regression for gradient tree boosting no decorrelation and normalization of the features is needed. But before one fixes the pre-processing one can try to extract some more information from some features through some feature engineering.</p>
</div>
</div>
<div id="some-feature-engineering" class="section level3" number="4.1.6">
<h3 number="4.1.6"><span class="header-section-number">4.1.6</span> Some feature engineering</h3>
<p>The only variable that allows for reasonable feature engineering is the date of joining predictor. One can try to extract some underlying patterns and see if an effect on the outcome is visible.</p>
<p>First extract the day of the week:</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>No main effect is visible here. So try the month next.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Again no main effect is visible.</p>
<p>Nevertheless one can include those two variables into the model because as mentioned before tree-based models actually perform a feature selection at each split. So including these just comes at a small computational cost. This minimal pre-processing that is needed when dealing with tree-based models is actually one of its biggest strengths. It is very robust against any kind of weird selection of features with different scales for example. This is one of the reasons, beside the strong predictive power, for the heavy use of such models in data mining applications.<span class="citation">[<a href="#ref-elements" role="doc-biblioref">1</a>]</span></p>
</div>
<div id="create-the-recipe" class="section level3" number="4.1.7">
<h3 number="4.1.7"><span class="header-section-number">4.1.7</span> Create the recipe</h3>
<p>A recipe is an object that defines a series of steps for data pre-processing and feature engineering.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="do">### recipe for xgboost (nominal variables must be dummy variables)</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># define outcome, predictors and training data set</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>burnout_rec_boost <span class="ot">&lt;-</span> <span class="fu">recipe</span>(burn_rate <span class="sc">~</span> date_of_joining <span class="sc">+</span> gender <span class="sc">+</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>                            company_type <span class="sc">+</span> wfh_setup_available <span class="sc">+</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                            designation <span class="sc">+</span> resource_allocation <span class="sc">+</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>                            mental_fatigue_score,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> burnout_train) <span class="sc">%&gt;%</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># extract the date features day of the week and month</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_date</span>(date_of_joining, <span class="at">features =</span> <span class="fu">c</span>(<span class="st">&quot;dow&quot;</span>, <span class="st">&quot;month&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># dummify all nominal features</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># encode the date as integers</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_mutate</span>(<span class="at">date_of_joining =</span> <span class="fu">as.integer</span>(date_of_joining))</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="do">### recipe for xgboost (nominal variables must be dummy variables)</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="do">### HERE the TRANSFORMED target</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>burnout_rec_boost_trans <span class="ot">&lt;-</span> <span class="fu">recipe</span>(burn_rate_trans <span class="sc">~</span> date_of_joining <span class="sc">+</span> gender <span class="sc">+</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                            company_type <span class="sc">+</span> wfh_setup_available <span class="sc">+</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>                            designation <span class="sc">+</span> resource_allocation <span class="sc">+</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>                            mental_fatigue_score,</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> burnout_train) <span class="sc">%&gt;%</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># extract the date features day of the week and month</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_date</span>(date_of_joining, <span class="at">features =</span> <span class="fu">c</span>(<span class="st">&quot;dow&quot;</span>, <span class="st">&quot;month&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># dummify all nominal features</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># encode the date as integers</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_mutate</span>(<span class="at">date_of_joining =</span> <span class="fu">as.integer</span>(date_of_joining))</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="do">### recipe for a random forest model for comparison (no dummy encoding needed)</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="co"># same as above without dummification but here the na&#39;s have to be</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="co"># imputed (here via knn)</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>burnout_rec_rf <span class="ot">&lt;-</span> <span class="fu">recipe</span>(burn_rate <span class="sc">~</span> date_of_joining <span class="sc">+</span> gender <span class="sc">+</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>                         company_type <span class="sc">+</span> wfh_setup_available <span class="sc">+</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>                         designation <span class="sc">+</span> resource_allocation <span class="sc">+</span></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>                         mental_fatigue_score,</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> burnout_train) <span class="sc">%&gt;%</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_string2factor</span>(<span class="fu">all_nominal</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_impute_knn</span>(resource_allocation, <span class="at">neighbors =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_impute_knn</span>(mental_fatigue_score, <span class="at">neighbors =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_date</span>(date_of_joining, <span class="at">features =</span> <span class="fu">c</span>(<span class="st">&quot;dow&quot;</span>, <span class="st">&quot;month&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_mutate</span>(<span class="at">date_of_joining =</span> <span class="fu">as.integer</span>(date_of_joining))</span></code></pre></div>
</div>
</div>
<div id="insurence-data" class="section level2" number="4.2">
<h2 number="4.2"><span class="header-section-number">4.2</span> Insurence data</h2>
<p>The insurance data set is part of the book <em>Machine Learning with R</em> by Brett Lantz. It can be downloaded here:
<a href="https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv" class="uri">https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv</a></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>insurance_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;_data/insurance.csv&quot;</span>)</span></code></pre></div>
<div id="train-test-split-1" class="section level3" number="4.2.1">
<h3 number="4.2.1"><span class="header-section-number">4.2.1</span> Train-test split</h3>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>ins_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(insurance_data, <span class="at">prop =</span> <span class="fl">0.80</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>ins_train <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(ins_split)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>ins_test  <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(ins_split)</span></code></pre></div>
<p>The training data set contains 1070 rows and 7 variables.</p>
<p>The test data set contains 268 observations and naturally also 7 variables.</p>
</div>
<div id="a-general-overview" class="section level3" number="4.2.2">
<h3 number="4.2.2"><span class="header-section-number">4.2.2</span> A general overview</h3>
<p>A general visualization of the whole data set in order to detect missing values.</p>
<p><img src="boosting_methods_files/figure-html/visdatIns-1.png" width="672" /></p>
<p>So there are no missing values!</p>
</div>
<div id="what-about-the-outcome" class="section level3" number="4.2.3">
<h3 number="4.2.3"><span class="header-section-number">4.2.3</span> What about the outcome?</h3>
<p><code>charges</code>: Individual medical costs billed by health insurance.</p>
<p>The five point summary below shows that the no invalid values i.e. negative ones are in the data.</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1122    4741    9289   13316   16640   62593</code></pre>
<p>Now the distribution of the outcome.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>Not like the <code>burn_rate</code> previously this target distribution is not at all symmetrical but highly right skewed. A natural thing to do would be a log transformation of the outcome. The resulting <code>log10_charges</code> outcome variable is shown below.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Although such a transformation of the outcome variable is not needed for tree-based modeling it can make the job of the algorithm somewhat easier.</p>
</div>
<div id="distribution-and-main-effects-of-the-predictors-1" class="section level3" number="4.2.4">
<h3 number="4.2.4"><span class="header-section-number">4.2.4</span> Distribution and main effects of the predictors</h3>
<div id="age" class="section level4" number="4.2.4.1">
<h4 number="4.2.4.1"><span class="header-section-number">4.2.4.1</span> Age</h4>
<p><code>age</code>: The age of the insurance contractor. This is naturally a continuous variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>A wide range of ages is covered. Notably there is a peak at roughly 18 which means that many fresh adults were observed in this data set.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>There seems to be a strong main effect although it does not seem to be linear. The general trend is that older contractors generally accumulate more medical costs. This is very intuitive.</p>
</div>
<div id="sex" class="section level4" number="4.2.4.2">
<h4 number="4.2.4.2"><span class="header-section-number">4.2.4.2</span> Sex</h4>
<p><code>sex</code>: The insurance contractors gender. Here either female or male. This means it is a binary variable and will be treated as such.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">as.factor</span>(ins_train<span class="sc">$</span>sex))</span></code></pre></div>
<pre><code>## female   male 
##    544    526</code></pre>
<p>The classes are very well balanced. Now the main effect.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>No notable difference can be detected here.</p>
</div>
<div id="body-mass-index" class="section level4" number="4.2.4.3">
<h4 number="4.2.4.3"><span class="header-section-number">4.2.4.3</span> Body mass index</h4>
<p><code>bmi</code>: The body mass index is providing an understanding of the body composition. It is a ratio composed out of the weight which is divided by the height <span class="math inline">\(\frac{kg}{m^2}\)</span>. Ideally the ratio is between 18.5 and 24.9. The variable is obviously a continuous variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>The distribution is bell-shaped and symmetrical roughly around a bmi of 30 which is above the ideal range. Actually only a small amount of the data falls into the normal range here. Moreover the right tail is heavier than the left one. Now a look at the main effect of the variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>With some fantasy one can grasp some non-linear patterns on the right side of the plot but beside that no strong main effect is visible here.</p>
</div>
<div id="number-of-children" class="section level4" number="4.2.4.4">
<h4 number="4.2.4.4"><span class="header-section-number">4.2.4.4</span> Number of children</h4>
<p><code>children</code>: The number of children or dependents covered by the health insurance.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># unique values of the feature</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">unique</span>(ins_train<span class="sc">$</span>children)</span></code></pre></div>
<pre><code>## [1] 0 1 2 3 4 5</code></pre>
<p>This could be treated as categorical but as there is a natural ordering it will be encoded by the integers so the treatment is like the one of a continuous feature.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>As one would also think the more children the lower the number of observed values. Especially the numbers greater than 3 are not well represented. If encoded by one-hot-encoding one would have to think about removing these then near-zero-variance variables. But as they will be encoded in a continuous way this is no problem at all. A look at the main effects can now strengthen or weaken this hypothesis of a natural ordering.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>This plot is quite similar to the main effect plot for the <code>age</code> feature. As most likely (will be checked later) the age is positively correlated with the number of children one can observe a rise of the minimal observed charges towards a greater amount of children. The upper two boxplots are built with just a few observations so they should not be interpreted in great detail. Overall there seems to be some kind of main effect.</p>
</div>
<div id="smoking" class="section level4" number="4.2.4.5">
<h4 number="4.2.4.5"><span class="header-section-number">4.2.4.5</span> Smoking</h4>
<p><code>smoker</code>: Is the contractor smoking? Of course a binary variable.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">as.factor</span>(ins_train<span class="sc">$</span>smoker))</span></code></pre></div>
<pre><code>##  no yes 
## 849 221</code></pre>
<p>The classes are not balanced but the class of the smokers is still represented with a good amount of observations. Now a look at the main effect.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>This main effect is as drastic as it is intuitive. Smoking seems to definitely increases the charges. This means that this variable has probably a lot of predictive power.</p>
</div>
<div id="region" class="section level4" number="4.2.4.6">
<h4 number="4.2.4.6"><span class="header-section-number">4.2.4.6</span> Region</h4>
<p><code>region</code>: The beneficiary’s residential area in the US. Either northeast, southeast, southwest or northwest. This definitely is a categorical variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>The four regions are balanced. Now to the main effect.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>No important main effect is detectable from this plot.</p>
</div>
</div>
<div id="relationships-between-the-predictors-1" class="section level3" number="4.2.5">
<h3 number="4.2.5"><span class="header-section-number">4.2.5</span> Relationships between the predictors</h3>
<div id="age-vs-the-others" class="section level4" number="4.2.5.1">
<h4 number="4.2.5.1"><span class="header-section-number">4.2.5.1</span> Age vs the others</h4>
<p>First the continuous one: <code>bmi</code></p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>No relationship detectable. The pearson correlation is with 0.089 also low.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>The most interesting take away from these four plots is that the hypothesis about the age of the contractors with children seem to be okish except for the ones with more than 3 children. But again this counterintuitive behavior could also be due to the few samples. At this point one might think about encoding the children variable as a categorical but in the following it will be left continuous.</p>
</div>
<div id="sex-vs-the-remaining" class="section level4" number="4.2.5.2">
<h4 number="4.2.5.2"><span class="header-section-number">4.2.5.2</span> Sex vs the remaining</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>No notable differences.</p>
<p>Contingency table for the binary variable <code>smoker</code>:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sex vs smoker</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(ins_train<span class="sc">$</span>sex, ins_train<span class="sc">$</span>smoker)</span></code></pre></div>
<pre><code>##         
##           no yes
##   female 448  96
##   male   401 125</code></pre>
<p>Slightly more men smoke but the difference is smallish.</p>
</div>
<div id="bmi-vs-the-remaining" class="section level4" number="4.2.5.3">
<h4 number="4.2.5.3"><span class="header-section-number">4.2.5.3</span> BMI vs the remaining</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>The most notable fact here is that southeast of the US seems to be a little more overweight than the rest.</p>
</div>
<div id="children-vs-the-remaining" class="section level4" number="4.2.5.4">
<h4 number="4.2.5.4"><span class="header-section-number">4.2.5.4</span> Children vs the remaining</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>In relative terms actually the contractors with three children smoke the most but the other levels seem quite balanced.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p>Here no trend is visible.</p>
</div>
<div id="smoker-vs-region" class="section level4" number="4.2.5.5">
<h4 number="4.2.5.5"><span class="header-section-number">4.2.5.5</span> Smoker vs Region</h4>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>So the southeast is not only the most overweight region but also the one with the most smokers in relative terms.</p>
<p>This concludes the tour of the pairwise relationships. Of course such an in detail look at all pairwise relationships for both data sets was only possible because there are quite few predictors and is not always needed in this extend. Besides a crystal clear understanding of the data one sees that there is not much room left for feature engineering for the insurance data set. Thus one can go on and define the recipe.</p>
</div>
</div>
<div id="create-the-recipe-1" class="section level3" number="4.2.6">
<h3 number="4.2.6"><span class="header-section-number">4.2.6</span> Create the recipe</h3>
<p>Transformations on the outcome variable are not good practice within a recipe thus this will be done now before hand by adding a new feature i.e. <code>log10_charges</code> to the train and test data set.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add the log transformed outcome variable to the data</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>ins_train<span class="sc">$</span>log10_charges <span class="ot">&lt;-</span> <span class="fu">log10</span>(ins_train<span class="sc">$</span>charges)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>ins_test<span class="sc">$</span>log10_charges <span class="ot">&lt;-</span> <span class="fu">log10</span>(ins_test<span class="sc">$</span>charges)</span></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="do">### recipe for xgboost (nominal variables must be dummy variables)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># define outcome, predictors and training data set</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>ins_rec_boost <span class="ot">&lt;-</span> <span class="fu">recipe</span>(log10_charges <span class="sc">~</span> age <span class="sc">+</span> sex <span class="sc">+</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                            bmi <span class="sc">+</span> children <span class="sc">+</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                            smoker <span class="sc">+</span> region,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> ins_train) <span class="sc">%&gt;%</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># dummify all nominal features (sex, smoker, region)</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>())</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="do">### recipe for a random forest model for comparison (no dummy encoding needed)</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># same as above without dummification </span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>ins_rec_rf <span class="ot">&lt;-</span> <span class="fu">recipe</span>(log10_charges <span class="sc">~</span> age <span class="sc">+</span> sex <span class="sc">+</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>                         bmi <span class="sc">+</span> children <span class="sc">+</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>                         smoker <span class="sc">+</span> region,</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> ins_train)</span></code></pre></div>
<p>Having now all the recipes ready one can proceed with modeling. Finally!</p>
<!--chapter:end:03-eda.Rmd-->
</div>
</div>
</div>
<div id="modeling" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Let’s boost the models</h1>
<p>Beside the XGBoost model also a random forest model will be fitted here for comparison. The whole modeling approach and procedure is closely following the principles that are outlined in the <strong>great</strong> book <em>Tidy modeling with R</em> by Max Kuhn and Julia Silge.</p>
<p>For the modeling three additional packages are required.<span class="citation">[<a href="#ref-xgboost_package" role="doc-biblioref">5</a>, <a href="#ref-ranger_package" role="doc-biblioref">17</a>, <a href="#ref-vipPack" role="doc-biblioref">18</a>]</span></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(xgboost) <span class="co"># for the xgboost model</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger) <span class="co"># for the random forest model</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># + the vip package but it will not be loaded into</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the namespace</span></span></code></pre></div>
<p>For parallel computations the <code>doParallel</code> package is used.<span class="citation">[<a href="#ref-doParallel_package" role="doc-biblioref">19</a>]</span>
This is most useful for the tuning part.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a cluster object and then register: </span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makePSOCKcluster</span>(<span class="dv">2</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(cl)</span></code></pre></div>
<p>First one has to set a <strong>metric</strong> for the evaluation of the final performance. As one knows from the EDA that not a lot of outliers are present in the data one can leave the default for XGBoost as the optimization metric for regression which is the root mean squared error (RMSE) which basically corresponds to the <span class="math inline">\(L_2\)</span> loss. Besides also the mean average error (MAE) which is kind of the <span class="math inline">\(L_1\)</span> norm will be covered but the optimization and tuning will focus on the RMSE.</p>
<div id="burnout-data-1" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> Burnout data</h2>
<div id="baseline-models" class="section level3" number="5.1.1">
<h3 number="5.1.1"><span class="header-section-number">5.1.1</span> Baseline models</h3>
<p>The first thing is to set up the trivial <strong>baseline models</strong>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the trivial intercept only model:</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>bout_predict_trivial_mean <span class="ot">&lt;-</span> <span class="cf">function</span>(new_data) {</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="fu">mean</span>(burnout_train<span class="sc">$</span>burn_rate), <span class="fu">nrow</span>(new_data))</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the trivial scoring of mental fatigue score (if missing intercept model)</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>bout_predict_trivial_mfs <span class="ot">&lt;-</span> <span class="cf">function</span>(new_data) {</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># these two scoring parameters are correspondint to the </span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># simple linear regression model containing only one predictor</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># i.e. mfs</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> new_data[[<span class="st">&quot;mental_fatigue_score&quot;</span>]] <span class="sc">*</span> <span class="fl">0.097</span> <span class="sc">-</span> <span class="fl">0.1</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>  pred[<span class="fu">is.na</span>(pred)] <span class="ot">&lt;-</span> <span class="fu">mean</span>(burnout_train<span class="sc">$</span>burn_rate)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>  pred</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The predictions of these baseline models on the test data set will be compared with the predictions of the tree-based models that will be constructed.</p>
</div>
<div id="model-specification" class="section level3" number="5.1.2">
<h3 number="5.1.2"><span class="header-section-number">5.1.2</span> Model specification</h3>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Models:</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest model for comparison</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>bout_rf_model <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>(<span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mtry =</span> <span class="fu">tune</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost model </span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>bout_boost_model <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(<span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>                               <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>                               <span class="at">loss_reduction =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>                               <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>                               <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>                               <span class="at">sample_size =</span> <span class="fu">tune</span>(),</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stop_iter =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Workflows: model + recipe</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forest workflow</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>bout_rf_wflow <span class="ot">&lt;-</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(bout_rf_model) <span class="sc">%&gt;%</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(burnout_rec_rf)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost workflow with the untransformed target</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>bout_boost_wflow <span class="ot">&lt;-</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(bout_boost_model) <span class="sc">%&gt;%</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(burnout_rec_boost)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost workflow with the transformed target (empirical logit)</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>bout_boost_wflow_trans <span class="ot">&lt;-</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(bout_boost_model) <span class="sc">%&gt;%</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(burnout_rec_boost_trans)</span></code></pre></div>
</div>
<div id="tuning" class="section level3" number="5.1.3">
<h3 number="5.1.3"><span class="header-section-number">5.1.3</span> Tuning</h3>
<p>For the hyperparameter tuning one needs validation sets to monitor the models on unseen data. To do this 5-fold cross validation (CV) is used here.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Resampling object</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>burnout_folds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(burnout_train, <span class="at">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Now adjust or check the hyperparameter ranges that the tuning will use. First the Random forest model.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Have a look at the hyperparameters that have to be tuned and finalize them</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>bout_rf_params <span class="ot">&lt;-</span> bout_rf_wflow <span class="sc">%&gt;%</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>()</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>bout_rf_params </span></code></pre></div>
<pre><code>## Collection of 2 parameters for tuning
## 
##  identifier  type    object
##        mtry  mtry nparam[?]
##       trees trees nparam[+]
## 
## Model parameters needing finalization:
##    # Randomly Selected Predictors (&#39;mtry&#39;)
## 
## See `?dials::finalize` or `?dials::update.parameters` for more information.</code></pre>
<p>This shows that the <code>mtry</code> hyperparameter has to be adjusted depending on the data. Moreover with the <code>dials::update</code> function one can manually set the ranges that should be used for tuning.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># default range for tuning is 1 up to 2000 for the trees argument</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="co"># set the lower bound of the range to 100. Then finalize the</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters using the training data.</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>bout_rf_params <span class="ot">&lt;-</span> bout_rf_params <span class="sc">%&gt;%</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">trees =</span> <span class="fu">trees</span>(<span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">2000</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(burnout_train)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>bout_rf_params</span></code></pre></div>
<pre><code>## Collection of 2 parameters for tuning
## 
##  identifier  type    object
##        mtry  mtry nparam[+]
##       trees trees nparam[+]</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>bout_rf_params <span class="sc">%&gt;%</span> <span class="fu">pull_dials_object</span>(<span class="st">&quot;mtry&quot;</span>)</span></code></pre></div>
<pre><code>## # Randomly Selected Predictors (quantitative)
## Range: [1, 10]</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>bout_rf_params <span class="sc">%&gt;%</span> <span class="fu">pull_dials_object</span>(<span class="st">&quot;trees&quot;</span>)</span></code></pre></div>
<pre><code>## # Trees (quantitative)
## Range: [100, 2000]</code></pre>
<p>Now this parameter object is ready for tuning and the same steps have to be performed on the main boosting workflow. The parameter set for the untransformed target can then also be used for the transformed one.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>bout_boost_params <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>()</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>bout_boost_params </span></code></pre></div>
<pre><code>## Collection of 6 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[?]
##           trees          trees nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]
## 
## Model parameters needing finalization:
##    # Randomly Selected Predictors (&#39;mtry&#39;)
## 
## See `?dials::finalize` or `?dials::update.parameters` for more information.</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first a look at the default ranges</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">trees</span>()</span></code></pre></div>
<pre><code>## # Trees (quantitative)
## Range: [1, 2000]</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tree_depth</span>()</span></code></pre></div>
<pre><code>## Tree Depth (quantitative)
## Range: [1, 15]</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">learn_rate</span>()</span></code></pre></div>
<pre><code>## Learning Rate (quantitative)
## Transformer:  log-10 
## Range (transformed scale): [-10, -1]</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_reduction</span>()</span></code></pre></div>
<pre><code>## Minimum Loss Reduction (quantitative)
## Transformer:  log-10 
## Range (transformed scale): [-10, 1.5]</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sample_size</span>()</span></code></pre></div>
<pre><code>## # Observations Sampled (quantitative)
## Range: [?, ?]</code></pre>
<p>So <code>sample_size</code> must also be finalized. Again the lower bound on the number of trees will be raised to 100. The other scales are really sensible and will be left as is.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>bout_boost_params <span class="ot">&lt;-</span> bout_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">trees =</span> <span class="fu">trees</span>(<span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">2000</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(burnout_train)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>bout_boost_params</span></code></pre></div>
<pre><code>## Collection of 6 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[+]
##           trees          trees nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>bout_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_dials_object</span>(<span class="st">&quot;sample_size&quot;</span>)</span></code></pre></div>
<pre><code>## Proportion Observations Sampled (quantitative)
## Range: [0.1, 1]</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>bout_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_dials_object</span>(<span class="st">&quot;mtry&quot;</span>)</span></code></pre></div>
<pre><code>## # Randomly Selected Predictors (quantitative)
## Range: [1, 10]</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use the same object for the tuning of the boosted model</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with the transformed target</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>bout_boost_params_trans <span class="ot">&lt;-</span> bout_boost_params</span></code></pre></div>
<p>Now also these parameter objects are ready to be used. The next step is to define the metrics used for evaluation while tuning.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define a metrics set used for evaluation of the hyperparameters</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>regr_metrics <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(rmse, mae)</span></code></pre></div>
<p>Now the actual tuning begins. The models will be tuned by a space filling grid search. As one has defined ranges for each parameter one can then use different algorithms to construct a grid of combinations of parameter values that tries to best fill the defined ranges by random sampling also in a high dimensional setting. In the following the function <code>tune::grid_latin_hypercube</code> is used for this. So basically one tries out all hyperparameter values for each fold and saves the performance of the performance metrics on the hold out fold. These metrics are then aggregated for each combination and with the resulting estimates of performance one can choose the final set of hyperparameters. The more hyperparameters the model has the more tuning rounds might be needed to refine the grid. Besides the classical grid search there are also iterative methods for tuning. These are mainly good to tune a single hyperparameter at once and not for a bunch of them simultaneously. They will not be used in the following.</p>
<p>The <strong>random forest model</strong> goes first with the tuning.</p>
<p>Here there are as seen above only two hyperparameters to be tuned thus 30 combinations should suffice.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 30 minutes</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  bout_rf_tune <span class="ot">&lt;-</span> bout_rf_wflow <span class="sc">%&gt;%</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  burnout_folds,</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> bout_rf_params <span class="sc">%&gt;%</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(<span class="at">size =</span> <span class="dv">30</span>, <span class="at">original =</span> <span class="cn">FALSE</span>),</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="co"># visualization of the tuning results (snapshot of the output below)</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bout_rf_tune) <span class="sc">+</span> <span class="fu">theme_light</span>()</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a><span class="co"># this functions shows the best combinations wrt the rmse metric of all the</span></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a><span class="co"># combinations in the grid</span></span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_rf_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/rf_burn_tune_plot.png" alt="Result of a spacefilling grid search for the random forest model." width="70%" />
<p class="caption">
(#fig:rfburntuneplot)Result of a spacefilling grid search for the random forest model.
</p>
</div>
<p>The visualization alongside the best performing results suggest that a value of <code>mtry</code>of 3 and 1000 <code>trees</code> should give good results. Thus one can finalize and fit this model.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>final_bout_rf_wflow <span class="ot">&lt;-</span> </span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>  bout_rf_wflow <span class="sc">%&gt;%</span> </span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">tibble</span>(</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="dv">1000</span>,</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">mtry =</span> <span class="dv">3</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(burnout_train)</span></code></pre></div>
<p>Now the main <strong>boosting model</strong>.</p>
<p>First tune only the number of trees in order to detect a number of
trees that is ‘large enough.’ Then tune the tree specific arguments.
If one tunes all parameters at the same time the grid grows to large. Moreover a tuning of the trees argument could encourage overfitting. See the book <em>Hands-on Machine Learning with R</em> for a detailed explenation of the tuning strategies.<span class="citation">[<a href="#ref-HandsOnMLwithR" role="doc-biblioref">2</a>]</span></p>
<p>The first tuning round:</p>
<p>To display the discussed impact of different maximum tree depths the first grid search for a good number of trees will besides the kind of standard value 6 also be performed for regression stumps i.e. a value of 1 for the maximum tree depth.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tuning grid just for the #trees one with max tree depth 6 and one with</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># only stumps for comparison</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>first_grid_boost_burn_depth6 <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">250</span>, <span class="dv">2000</span>, <span class="dv">250</span>),</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">9</span>,</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="dv">6</span>,</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fl">0.000001</span>,</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fl">0.01</span>,</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>first_grid_boost_burn_stumps <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">250</span>, <span class="dv">2000</span>, <span class="dv">250</span>),</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">9</span>,</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="dv">1</span>,</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fl">0.000001</span>,</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fl">0.01</span>,</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 1 minute</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_first_stumps <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  burnout_folds,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> first_grid_boost_burn_stumps,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bout_boost_tune_first_stumps) <span class="sc">+</span> <span class="fu">theme_light</span>()</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_first_stumps)</span></code></pre></div>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 1 minute</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_first_depth6 <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  burnout_folds,</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> first_grid_boost_burn_depth6,</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plot output is shown in the figure below</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bout_boost_tune_first_depth6) <span class="sc">+</span> <span class="fu">theme_light</span>()</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_first_depth6)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/burn_boost_tune_first.png" alt="Result of the first grid search for the XGBoost model (max depth 6)." width="70%" />
<p class="caption">
(#fig:boostburntuneplot1)Result of the first grid search for the XGBoost model (max depth 6).
</p>
</div>
<p>Compare the two different first tuning rounds:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># output of this code shown in the figure below</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>bout_boost_tune_first_stumps <span class="sc">%&gt;%</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    bout_boost_tune_first_depth6 <span class="sc">%&gt;%</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="dv">6</span>)</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> trees, <span class="at">y =</span> mean, <span class="at">col =</span> <span class="fu">factor</span>(tree_depth))) <span class="sc">+</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> mean <span class="sc">-</span> std_err, <span class="at">ymax =</span> mean <span class="sc">+</span> std_err)) <span class="sc">+</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">col =</span> <span class="st">&quot;Max tree depth&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Number of trees&quot;</span>,</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">option =</span> <span class="st">&quot;C&quot;</span>) <span class="sc">+</span></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> .metric) <span class="sc">+</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_light</span>()</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/bout_tuning_stumpsVs6.png" alt="Comparison of the initial grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates." width="70%" />
<p class="caption">
(#fig:bouttuningstumpsVs6)Comparison of the initial grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates.
</p>
</div>
<p>This shows that the model with just stumps converges much slower or may even never reach the low level of when using deeper trees. One reason for this could be that these stumps do not account for interactions and of course the same number of deep trees encode much more information than the same number of stumps.</p>
<p>So 1500 trees should suffice here. Thus the workflow and model will be adjusted accordingly.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix the number of trees by redefining the boosted model with a </span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed number of trees.</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>bout_boost_model <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(<span class="at">trees =</span> <span class="dv">1500</span>,</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">loss_reduction =</span> <span class="fu">tune</span>(),</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">sample_size =</span> <span class="fu">tune</span>(),</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stop_iter =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a><span class="co"># update the workflow</span></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>bout_boost_wflow <span class="ot">&lt;-</span></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>  bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_model</span>(bout_boost_model)</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>bout_boost_params <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>() <span class="sc">%&gt;%</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(burnout_train)</span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a><span class="co"># reduced hyperparameter space</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>bout_boost_params</span></code></pre></div>
<pre><code>## Collection of 5 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]</code></pre>
<p>Now perform the first major grid search over all the other hyperparameters.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now tune all the remaining hyperparameters with a large space filling grid</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 1.5 hours</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_second <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> burnout_folds,</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> bout_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">size =</span> <span class="dv">200</span>),</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_second, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>Now refine the grid i.e. the parameter space according to the results of the last grid search and perform a third one.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now tune all the remaining hyperparameters with a refined space filling grid</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 2 hours</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_third <span class="ot">&lt;-</span> bout_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> burnout_folds,</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> bout_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">update</span>(</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">mtry =</span> <span class="fu">mtry</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">9</span>)),</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">tree_depth =</span> <span class="fu">tree_depth</span>(<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>)),</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>          <span class="at">learn_rate =</span> <span class="fu">learn_rate</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.7</span>, <span class="sc">-</span><span class="fl">1.3</span>)),</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>          <span class="at">loss_reduction =</span> <span class="fu">loss_reduction</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="sc">-</span><span class="dv">3</span>)),</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>          <span class="at">sample_size =</span> <span class="fu">sample_prop</span>(<span class="fu">c</span>(<span class="fl">0.4</span>, <span class="fl">0.9</span>))</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>        ) <span class="sc">%&gt;%</span></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>          <span class="at">size =</span> <span class="dv">200</span>),</span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_third, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>With this final grid search one is ready to finalize the model.</p>
<p>The results of the three grid searches suggest that no column-subsampling should be applied, the maximum tree depth should be 4, the learning rate should be small but not extremely small (<span class="math inline">\(\sim 0.02\)</span>), the loss reduction regularization effect is not needed here (very small) and the sample size for each tree should be set to 0.8.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>final_bout_boost_wflow <span class="ot">&lt;-</span> </span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>  bout_boost_wflow <span class="sc">%&gt;%</span> </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">tibble</span>(</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mtry =</span> <span class="dv">9</span>,</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="dv">4</span>,</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">learn_rate =</span> <span class="fl">0.02</span>,</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss_reduction =</span> <span class="fl">0.0000003</span>,</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> <span class="fl">0.8</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(burnout_train)</span></code></pre></div>
<p>Now tune the <strong>transformed outcome boosting model</strong>.</p>
<p>Again start with the <code>trees</code>.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>first_grid_boost_burn_trans <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">250</span>, <span class="dv">2000</span>, <span class="dv">250</span>),</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">9</span>,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="dv">6</span>,</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fl">0.000001</span>,</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fl">0.01</span>,</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 1 minute</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_first_trans <span class="ot">&lt;-</span> bout_boost_wflow_trans <span class="sc">%&gt;%</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  burnout_folds,</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> first_grid_boost_burn_trans,</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plot output in the figure below</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bout_boost_tune_first_trans) <span class="sc">+</span> <span class="fu">theme_light</span>()</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_first_trans)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/burn_boost_tune_first_trans.png" alt="Result of the first grid search for the XGBoost model with transformed outcome." width="70%" />
<p class="caption">
(#fig:burnboosttunefirsttrans)Result of the first grid search for the XGBoost model with transformed outcome.
</p>
</div>
<p>Again 1500 trees should easily suffice.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix the number of trees by redefining the boosted model with a </span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed number of trees.</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>bout_boost_model_trans <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(<span class="at">trees =</span> <span class="dv">1500</span>,</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">loss_reduction =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">sample_size =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stop_iter =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a><span class="co"># update the workflow</span></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>bout_boost_wflow_trans <span class="ot">&lt;-</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>  bout_boost_wflow_trans <span class="sc">%&gt;%</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_model</span>(bout_boost_model_trans)</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>bout_boost_params_trans <span class="ot">&lt;-</span> bout_boost_wflow_trans <span class="sc">%&gt;%</span></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(burnout_train)</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a><span class="co"># reduced hyperparameter space</span></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>bout_boost_params_trans</span></code></pre></div>
<pre><code>## Collection of 5 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]</code></pre>
<p>Now perform the first major grid search over all the other hyperparameters and the second one overall.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now tune all the remaining hyperparameters with a large space filling grid</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 2 hours</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>  bout_boost_tune_second_trans <span class="ot">&lt;-</span> bout_boost_wflow_trans <span class="sc">%&gt;%</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> burnout_folds,</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> bout_boost_params_trans <span class="sc">%&gt;%</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">size =</span> <span class="dv">200</span>),</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bout_boost_tune_second_trans, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>From the tuning results one can conclude that in this setting actually the same hyperparameter setting as for the raw target variable seems appropriate. So one finalizes the workflow with the same hyperparameters and fits the model.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>final_bout_boost_wflow_trans <span class="ot">&lt;-</span> </span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>  bout_boost_wflow_trans <span class="sc">%&gt;%</span> </span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">tibble</span>(</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mtry =</span> <span class="dv">9</span>,</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="dv">4</span>,</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">learn_rate =</span> <span class="fl">0.02</span>,</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss_reduction =</span> <span class="fl">0.0000003</span>,</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> <span class="fl">0.8</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(burnout_train)</span></code></pre></div>
<p>Now all models w.r.t. the burnout data set are fitted.</p>
</div>
<div id="evaluate-and-understand-the-model" class="section level3" number="5.1.4">
<h3 number="5.1.4"><span class="header-section-number">5.1.4</span> Evaluate and understand the model</h3>
<p>First a visualization of the variable importance. The variable importance is basically calculated by measuring the gain w.r.t. the loss of each feature in the single trees and splits.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>This shows that indeed the <code>mental_fatigue_score</code> is the most influential predictor by far followed by the <code>ressource_allocation</code> and <code>designation</code> features. These results are not at all surprising as the EDA exactly came to these conclusions. Especially the very few appearances of the features connected to <code>company_type</code> and <code>date_of_joining</code> are most likely just some minor overfitting.</p>
<p>Now compare the variable importance of the model with the raw and the transformed target variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>One can observe the exact same ordering. The only difference seems to be a higher influence of the designation variable for the transformed target. This is not surprising as the resource allocation and designation variable are highly correlated.</p>
<p>Now have a look at the performance of the main boosting model w.r.t. missing values in the most influential variable.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-87-1.png" width="672" /><img src="boosting_methods_files/figure-html/unnamed-chunk-87-2.png" width="672" /></p>
<p>The performance for the missing values is indeed not that precise but still quite good. There is at least no huge outlier detectable here. While at first glance the fact that outliers are handled naturally by the model is not astonishing it really is one the core strengths of the model to deal with messy data that includes missing and sparse data. So there is no need for imputation or a second fallback model for missing values.</p>
<p>Now it is interesting to check which model performed the best on the test data set. The results can be viewed below in tabular and graphical form.</p>
<table>
<caption>(#tab:perfBurn)Performance on the test data</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">mae</th>
<th align="right">rmse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">boost_trans_pred</td>
<td align="right">0.0467</td>
<td align="right">0.0592</td>
</tr>
<tr class="even">
<td align="left">boost_pred</td>
<td align="right">0.0467</td>
<td align="right">0.0593</td>
</tr>
<tr class="odd">
<td align="left">rf_pred</td>
<td align="right">0.0474</td>
<td align="right">0.0612</td>
</tr>
<tr class="even">
<td align="left">mfs_scored_pred</td>
<td align="right">0.0628</td>
<td align="right">0.0870</td>
</tr>
<tr class="odd">
<td align="left">intercept_pred</td>
<td align="right">0.1593</td>
<td align="right">0.1980</td>
</tr>
</tbody>
</table>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>It can be observed that already the really simple baseline model that just scales the mental fatigue score (so it reflects just a single linear influence) has a really low MAE and RMSE. Still both random forest as well as the boosting model can further improve this metric but the huge linear influence of the mental fatigue score obviously leaves not much room for improvement. By the way the simple linear model with just this one predictor has already an <span class="math inline">\(R^2\)</span> of more than 0.92. XGBoost manages to get a slightly better performance on the test data set but this would only be nice in a machine learning competition as in real life this difference would negligible. Actually in this use case for a real life application a very easy explainable linear model might be somewhat better than a complex model like XGBoost as the difference in performance is not too big. Nevertheless in my opinion the predictor <code>mental_fatigue_score</code> should be treated with extreme care as in a real life situation the collection of this score could be as costly or hard as the one of the outcome. There might be even latent variables that are highly linearly correlated to both scores. But this data was not intended to be used in a real life application but was shared at a machine learning competition and actually many of the best submissions there used XGBoost models. The transformation of the outcome variable did actually not change the predictive power of the model as the result for both the model with the raw target as well as the one with the transformed one performed equally good. Below one can see a scatterplot comparing the individual predictions of these two models on the test set which perfectly underlines the hypothesis that the models are basically the same as there is almost no variation around the identity slope.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>So all in all for this data set it was not a way better predictive performance (if one is not in an artificial machine learning setup) that was the core strength of the tree-based gradient boosting model but the minimal pre-processing and exploratory work (for example no interactions have to be detected manually or be tested for significance) that was needed and the natural handling of missing values to achieve the model. This of course came to a quite high computational price. The famous predictive performance could probably be displayed better when having a data set with more complex non-linear patterns.</p>
<p>This finishes the analysis of the burnout data set. But no worries there is still one data set and boosting model left to explore namely the insurance data set.</p>
</div>
</div>
<div id="insurance-data" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> Insurance data</h2>
<div id="baseline-models-1" class="section level3" number="5.2.1">
<h3 number="5.2.1"><span class="header-section-number">5.2.1</span> Baseline models</h3>
<p>The first thing is to set up the trivial <strong>baseline models</strong>.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the trivial intercept only model:</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>ins_predict_trivial_mean <span class="ot">&lt;-</span> <span class="cf">function</span>(new_data) {</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="fu">mean</span>(ins_train<span class="sc">$</span>log10_charges), <span class="fu">nrow</span>(new_data))</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the trivial linear model without any interactions (as we do not have </span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="co"># missing values does not have to be dealt with them)</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>ins_baseline_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(log10_charges <span class="sc">~</span> age <span class="sc">+</span> sex <span class="sc">+</span> bmi <span class="sc">+</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>                        children <span class="sc">+</span> smoker <span class="sc">+</span> region,</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> ins_train <span class="sc">%&gt;%</span> </span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.character), as.factor)))</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ins_baseline_lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log10_charges ~ age + sex + bmi + children + smoker + 
##     region, data = ins_train %&gt;% mutate(across(where(is.character), 
##     as.factor)))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46395 -0.08466 -0.01955  0.02643  0.93557 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      3.0685262  0.0357167  85.913  &lt; 2e-16 ***
## age              0.0149190  0.0004316  34.566  &lt; 2e-16 ***
## sexmale         -0.0251970  0.0119527  -2.108 0.035260 *  
## bmi              0.0052551  0.0010243   5.130 3.44e-07 ***
## children         0.0413979  0.0050235   8.241 5.00e-16 ***
## smokeryes        0.6841462  0.0147963  46.238  &lt; 2e-16 ***
## regionnorthwest -0.0178125  0.0172137  -1.035 0.301003    
## regionsoutheast -0.0613744  0.0172229  -3.564 0.000382 ***
## regionsouthwest -0.0512729  0.0172868  -2.966 0.003084 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1945 on 1061 degrees of freedom
## Multiple R-squared:  0.764,  Adjusted R-squared:  0.7622 
## F-statistic: 429.4 on 8 and 1061 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The predictions of these baseline models on the test data set will be compared with the predictions of the tree-based models that will be constructed.</p>
</div>
<div id="model-specification-1" class="section level3" number="5.2.2">
<h3 number="5.2.2"><span class="header-section-number">5.2.2</span> Model specification</h3>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Models:</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest model for comparison</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>ins_rf_model <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>(<span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>                             <span class="at">mtry =</span> <span class="fu">tune</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost model</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>ins_boost_model <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(<span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>                               <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>                               <span class="at">loss_reduction =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>                               <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>                               <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>                               <span class="at">sample_size =</span> <span class="fu">tune</span>(),</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stop_iter =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Workflows: model + recipe</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>ins_rf_wflow <span class="ot">&lt;-</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ins_rf_model) <span class="sc">%&gt;%</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ins_rec_rf)</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>ins_boost_wflow <span class="ot">&lt;-</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(ins_boost_model) <span class="sc">%&gt;%</span></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(ins_rec_boost)</span></code></pre></div>
</div>
<div id="tuning-1" class="section level3" number="5.2.3">
<h3 number="5.2.3"><span class="header-section-number">5.2.3</span> Tuning</h3>
<p>For the hyperparameter tuning one needs validation sets to monitor the models on unseen data. To do this 5-fold cross validation (CV) is used here.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Resampling object</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>ins_folds <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(ins_train, <span class="at">v =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Now the parameter objects will be fixed for the grid searches analogously to above.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>ins_rf_params <span class="ot">&lt;-</span> ins_rf_wflow <span class="sc">%&gt;%</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>() <span class="sc">%&gt;%</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">trees =</span> <span class="fu">trees</span>(<span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">2000</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(ins_train)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>ins_rf_params <span class="sc">%&gt;%</span> <span class="fu">pull_dials_object</span>(<span class="st">&quot;trees&quot;</span>)</span></code></pre></div>
<pre><code>## # Trees (quantitative)
## Range: [100, 2000]</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>ins_rf_params <span class="sc">%&gt;%</span> <span class="fu">pull_dials_object</span>(<span class="st">&quot;mtry&quot;</span>)</span></code></pre></div>
<pre><code>## # Randomly Selected Predictors (quantitative)
## Range: [1, 8]</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>ins_boost_params <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>() <span class="sc">%&gt;%</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">trees =</span> <span class="fu">trees</span>(<span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">2000</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(ins_train)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>ins_boost_params</span></code></pre></div>
<pre><code>## Collection of 6 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[+]
##           trees          trees nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]</code></pre>
<p>The <strong>random forest model</strong> goes first with the tuning.</p>
<p>Here there are as seen above only two hyperparameters to be tuned thus 30 combinations should suffice.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 2 minutes</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>  ins_rf_tune <span class="ot">&lt;-</span> ins_rf_wflow <span class="sc">%&gt;%</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  ins_folds,</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> ins_rf_params <span class="sc">%&gt;%</span></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(<span class="at">size =</span> <span class="dv">30</span>, <span class="at">original =</span> <span class="cn">FALSE</span>),</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a><span class="co"># visualization of the tuning results (snapshot of the output below)</span></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(ins_rf_tune) <span class="sc">+</span> <span class="fu">theme_light</span>()</span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a><span class="co"># this functions shows the best combinations wrt the rmse metric of all the</span></span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a><span class="co"># combinations in the grid</span></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_rf_tune, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/rf_ins_tune_plot.png" alt="Result of a spacefilling grid search for the random forest model." width="70%" />
<p class="caption">
(#fig:rfInstuneplot)Result of a spacefilling grid search for the random forest model.
</p>
</div>
<p>The visualization alongside the best performing results suggest that a value of <code>mtry</code>of 4 and 1000 <code>trees</code> should give good results. Thus one can finalize and fit this model.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>final_ins_rf_wflow <span class="ot">&lt;-</span> </span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>  ins_rf_wflow <span class="sc">%&gt;%</span> </span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">tibble</span>(</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="dv">1000</span>,</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">mtry =</span> <span class="dv">4</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(ins_train)</span></code></pre></div>
<p>Now the <strong>boosting model</strong>.</p>
<p>Again one starts to tune over the number of trees first (this time we also use three different tree depths to visualize the impact of this parameter too)</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>first_grid_boost_ins <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">250</span>, <span class="dv">2000</span>, <span class="dv">250</span>),</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">8</span>,</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>),</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fl">0.000001</span>,</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fl">0.01</span>,</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly half a minute</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>  ins_boost_tune_first <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  ins_folds,</span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> first_grid_boost_ins,</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_boost_tune_first)</span></code></pre></div>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the tuning results (output in the figure below)</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>ins_boost_tune_first <span class="sc">%&gt;%</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> trees, <span class="at">y =</span> mean, <span class="at">col =</span> <span class="fu">factor</span>(tree_depth))) <span class="sc">+</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> mean <span class="sc">-</span> std_err, <span class="at">ymax =</span> mean <span class="sc">+</span> std_err)) <span class="sc">+</span></span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">col =</span> <span class="st">&quot;Max tree depth&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Number of trees&quot;</span>,</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">option =</span> <span class="st">&quot;C&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> .metric) <span class="sc">+</span></span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_light</span>()</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/boost_ins_tune_plot1.png" alt="Comparison of the initial grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates." width="70%" />
<p class="caption">
(#fig:boostInstuneplot1)Comparison of the initial grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates.
</p>
</div>
<p>This visualization clearly shows that one has to have a closer look at the region around 500 trees. More than 500 trees might lead to overfitting as seen here.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>second_grid_boost_ins <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">250</span>, <span class="dv">750</span>, <span class="dv">50</span>),</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">8</span>,</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>),</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fl">0.000001</span>,</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fl">0.01</span>,</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 15 seconds</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>  ins_boost_tune_second <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  ins_folds,</span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> second_grid_boost_ins,</span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_boost_tune_second)</span></code></pre></div>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the tuning results (output in the figure below)</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>ins_boost_tune_second <span class="sc">%&gt;%</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> trees, <span class="at">y =</span> mean, <span class="at">col =</span> <span class="fu">factor</span>(tree_depth))) <span class="sc">+</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> mean <span class="sc">-</span> std_err, <span class="at">ymax =</span> mean <span class="sc">+</span> std_err)) <span class="sc">+</span></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">col =</span> <span class="st">&quot;Max tree depth&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Number of trees&quot;</span>,</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">option =</span> <span class="st">&quot;C&quot;</span>) <span class="sc">+</span></span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> .metric) <span class="sc">+</span></span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_light</span>()</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/boost_ins_tune_plot2.png" alt="Comparison of the second grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates." width="70%" />
<p class="caption">
(#fig:boostInstuneplot2)Comparison of the second grid search for the number of trees w.r.t. the maximum tree depth. Actually with tiny confidence bands around the estimates.
</p>
</div>
<p>So from this visualization one can conclude that a number of trees of 600 should suffice to get a decent model. So the workflow will be updated and the number of trees fixed. After that the first major space filling grid search will be performed. Moreover with the previous visualizations in mind one can reduce the bound of the parameter space of the maximum tree depth quite a bit from 15 to 9. A minimum tree depth of 2 seems also appropriate.
Before one does that mainly for a view on the influence of the learning rate one can tune one round only with the learning rate and some tree numbers for comparison.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tune mainly learn rate</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>lrate_grid_boost_ins <span class="ot">&lt;-</span> <span class="fu">crossing</span>(</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">seq</span>(<span class="dv">200</span>, <span class="dv">1500</span>, <span class="dv">100</span>),</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">8</span>,</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> <span class="fu">c</span>(<span class="dv">3</span>),</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss_reduction =</span> <span class="fu">c</span>(<span class="fl">0.000001</span>),</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">learn_rate =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_size =</span> <span class="dv">1</span></span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 20 seconds</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>  ins_boost_tune_lrate <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span>  ins_folds,</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> lrate_grid_boost_ins,</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_boost_tune_lrate)</span></code></pre></div>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the tuning results (output in the figure below)</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>ins_boost_tune_lrate <span class="sc">%&gt;%</span></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> trees, <span class="at">y =</span> mean, <span class="at">col =</span> <span class="fu">factor</span>(learn_rate))) <span class="sc">+</span></span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> mean <span class="sc">-</span> std_err, <span class="at">ymax =</span> mean <span class="sc">+</span> std_err)) <span class="sc">+</span></span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Number of trees&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">col =</span> <span class="st">&quot;Learning rate&quot;</span>,</span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">option =</span> <span class="st">&quot;C&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> .metric) <span class="sc">+</span></span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_light</span>()</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/boost_ins_tune_lrate.png" alt="Comparison of the grid search for the number of trees w.r.t. the learning rate. Actually with tiny confidence bands around the estimates." width="70%" />
<p class="caption">
(#fig:boostInstuneplotlrate)Comparison of the grid search for the number of trees w.r.t. the learning rate. Actually with tiny confidence bands around the estimates.
</p>
</div>
<p>This showcases the fact that indeed a learning rate that is too small can blow up the computational costs.</p>
<p>Now fix the number of the <code>trees</code> hyperparameter.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix the number of trees by redefining the boostin model with a </span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fixed number of trees.</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>ins_boost_model <span class="ot">&lt;-</span> <span class="fu">boost_tree</span>(<span class="at">trees =</span> <span class="dv">600</span>,</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">learn_rate =</span> <span class="fu">tune</span>(),</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">loss_reduction =</span> <span class="fu">tune</span>(),</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">sample_size =</span> <span class="fu">tune</span>(),</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stop_iter =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a><span class="co"># update the workflow</span></span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>ins_boost_wflow <span class="ot">&lt;-</span></span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>  ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update_model</span>(ins_boost_model)</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a>ins_boost_params <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">parameters</span>() <span class="sc">%&gt;%</span></span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">tree_depth =</span> <span class="fu">tree_depth</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">9</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize</span>(ins_train)</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a><span class="co"># reduced hyperparameter space</span></span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a>ins_boost_params</span></code></pre></div>
<pre><code>## Collection of 5 parameters for tuning
## 
##      identifier           type    object
##            mtry           mtry nparam[+]
##      tree_depth     tree_depth nparam[+]
##      learn_rate     learn_rate nparam[+]
##  loss_reduction loss_reduction nparam[+]
##     sample_size    sample_size nparam[+]</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now tune all the remaining hyperparameters with a large space filling grid</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 10 minutes</span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>  ins_boost_tune_major1 <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> ins_folds,</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> ins_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">size =</span> <span class="dv">300</span>),</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_boost_tune_major1, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/tuning_ins_parallelcoord.png" alt="Visualize the tuning results with a parallel coordinate plot. The y axis represents the scaled range for each of the hyperparameter spaces." width="70%" />
<p class="caption">
(#fig:insparallelcoord)Visualize the tuning results with a parallel coordinate plot. The y axis represents the scaled range for each of the hyperparameter spaces.
</p>
</div>
<p>The most prominent suggestions of the visualizations and the table of the best performing ones are combinations of hyperparameters with a very low <code>loss_reduction</code> , a low <code>learn_rate</code>, a medium to high value of <code>mtry</code>, a not too big <code>tree_depth</code> as well as a rather high value for the <code>sample_size</code>.
These observations will be used to refine the search space and perform once more a space filling grid search.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now tune all the remaining hyperparameters with a refined space filling grid</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="co"># took roughly 15 minutes</span></span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>({</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>  ins_boost_tune_major2 <span class="ot">&lt;-</span> ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tune_grid</span>(</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">resamples =</span> ins_folds,</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">grid =</span> ins_boost_params <span class="sc">%&gt;%</span></span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">update</span>(</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">mtry =</span> <span class="fu">mtry</span>(<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">8</span>)),</span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">tree_depth =</span> <span class="fu">tree_depth</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>)),</span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a>          <span class="at">learn_rate =</span> <span class="fu">learn_rate</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.7</span>, <span class="sc">-</span><span class="fl">1.5</span>)),</span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>          <span class="at">loss_reduction =</span> <span class="fu">loss_reduction</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="sc">-</span><span class="fl">1.5</span>)),</span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a>          <span class="at">sample_size =</span> <span class="fu">sample_prop</span>(<span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.9</span>))</span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a>        ) <span class="sc">%&gt;%</span></span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a>        <span class="fu">grid_latin_hypercube</span>(</span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a>          <span class="at">size =</span> <span class="dv">300</span>),</span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a>      <span class="at">metrics =</span> regr_metrics</span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb118-22"><a href="#cb118-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-23"><a href="#cb118-23" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(ins_boost_tune_major2, <span class="at">metric =</span> <span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<p>The results from this grid search were quite clear. One will further use a <code>mtry</code> value of 7, a <code>tree_depth</code> of 3, a <code>learn_rate</code> of 0.02, a <code>loss_reduction</code> of 0.03 and a <code>sample_size</code> of 0.8.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>final_ins_boost_wflow <span class="ot">&lt;-</span> </span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>  ins_boost_wflow <span class="sc">%&gt;%</span> </span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">tibble</span>(</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mtry =</span> <span class="dv">7</span>,</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="dv">3</span>,</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">learn_rate =</span> <span class="fl">0.02</span>,</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss_reduction =</span> <span class="fl">0.03</span>,</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> <span class="fl">0.8</span></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(ins_train)</span></code></pre></div>
<p>From here no computational intensive task will be performed so one stops the cluster.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stop cluster</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span></code></pre></div>
</div>
<div id="evaluate-and-understand-the-model-1" class="section level3" number="5.2.4">
<h3 number="5.2.4"><span class="header-section-number">5.2.4</span> Evaluate and understand the model</h3>
<p>First a visualization of the variable importance. The variable importance is again calculated by measuring the gain w.r.t. the loss of each feature in the single trees and splits.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<p>Here one can clearly see that the trends that were observable during the EDA are reflected here again. The most influential variable is indeed the smoker variable, closely followed by the age variable. Also the bmi and the number of children seem to be relevant. To get an even better sense for where which variable was mostly used one can have a look at the visualization below which shows the final model in a compressed form as a single tree. At each node the most important (again by the feature importance score like above) 5 variables are listed with their respective raw importance scores in brackets. The ‘Leaf’ variable just corresponds to the case where trees ended at this point.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization of the ensemble of trees as a single collective unit</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>final_ins_boost_fit <span class="ot">&lt;-</span> final_ins_boost_wflow <span class="sc">%&gt;%</span></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull_workflow_fit</span>()</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>final_ins_boost_fit<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xgb.plot.multi.trees</span>(<span class="at">render =</span> T)</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a><span class="co"># output as figure below as this returns a htmlwidget which is not that</span></span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a><span class="co"># intuitively handled by latex (pdf output)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="_pictures/ins_multitree_xgb.png" alt="Visualization of the final ensemble of trees as a single collective unit." width="70%" />
<p class="caption">
(#fig:insmultitree)Visualization of the final ensemble of trees as a single collective unit.
</p>
</div>
<p>It is quite interesting to see that most of the time the smoker feature was used just once in the first node but not multiple times. This suggests a quite linear influence of smokers and of course the binary character of the feature can support this behavior. Age on the other hand is represented in multiple depths with high importance which suggests a non-linear influence which was also visible during the EDA. Interestingly the children variable for example mostly is used at depth 3 for a split.</p>
<p>Also a residual plot of the XGBoost predictions on the test data set can shed some light on the way the model works. This 3D plot is an interactive html widget which works not in the pdf output but just on the website.</p>
<div id="htmlwidget-6f623c916c3e6ba3fc27" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-6f623c916c3e6ba3fc27">{"x":{"visdat":{"678c140d61d6":["function () ","plotlyVisDat"]},"cur_data":"678c140d61d6","attrs":{"678c140d61d6":{"x":{},"z":{},"y":{},"text":{},"hoverinfo":"text","color":{},"symbol":{},"colors":["#CC4678FF","#0D0887FF"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"symbols":["x","circle"],"type":"scatter3d","mode":"markers","opacity":0.9,"size":2,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Age"},"yaxis":{"title":"BMI"},"zaxis":{"title":"Raw residuals"}},"title":"Raw residuals of the insurance XGBoost model","legend":{"title":{"text":"Smoker"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[18,32,31,52,23,19,24,18,64,28,40,31,53,58,57,31,54,55,41,52,37,18,20,53,48,54,41,18,63,40,53,26,24,41,47,59,44,19,55,58,53,19,52,28,58,49,52,34,26,57,52,64,49,34,57,33,49,56,57,64,62,48,52,38,19,52,25,60,35,40,35,39,24,57,62,18,63,54,31,37,22,51,38,33,20,25,46,34,19,19,18,62,25,19,27,28,59,29,24,51,48,39,48,18,19,33,53,40,24,56,47,33,62,18,45,21,58,24,31,47,64,38,61,44,45,31,22,39,64,18,46,36,60,63,35,51,25,57,55,51,40,50,33,41,23,61,26,60,44,22,44,24,34,62,26,18,19,31,24,46,52,35,39,34,22,19,50,28,21,61,54,64,36,60,32,32,57,52,23,18,49,50,26,41,52,35,36,20,32,43,32,37,19,22,59,41,56,58,58,45,18,28,55,43,35,22,18,58,20,18,18,51,57,23,18],"z":[-559.895768193102,-285.798447994573,166.160130995318,-1640.34546472001,-841.651523903832,358.447628157622,-169.701965867938,-685.081443005199,16349.4056686668,-851.692829443274,-630.121902461979,649.245475345141,-452.425878910602,-880.912683262219,-910.731849348191,-674.572891311772,-163.325438080859,-2205.40390170798,36.2723567525627,-1208.78049151623,-941.930253981824,-113.909497029493,-191.837734497546,-753.384252870612,-2553.60361276063,-1250.74403447937,-123.883329375256,-595.89018115075,341.697428494565,-1218.13011500114,-1031.72258178435,-70.6149855363142,22468.5031571188,-373.123834813202,-1588.93863486007,-660.158989675843,-1458.67365176814,-3.91786930278226,-893.050836708253,-809.210589437957,7369.62281165481,-2606.77076780426,14041.3679410583,14764.7693435901,-597.996093318228,-1580.31807113277,-2056.82324085295,-1291.89613148762,18095.4773045501,-1719.98399330942,-1754.44106426169,1030.66515862553,-1048.59123360993,-427.982526652509,-797.531410734979,7295.34928621457,-1223.46757021579,-266.564344534811,-312.410136150553,1307.38303704728,13019.2141432638,-1422.45690857707,-2868.01579405431,81.2832558119817,-366.305118199377,-2138.8987348861,-274.861084923185,-1493.32988046182,-470.80782272924,-809.911007089193,-1322.42280813002,-1134.61649916576,-508.49528427246,-1134.90393368421,-2044.85137737716,-385.544020862301,-2618.68783885339,301.600056090625,-103.73665774984,-660.974884838872,-133.356362446797,-1285.87555137442,-571.525150974553,-289.278966909756,-1149.03383917436,14656.0284826427,-1073.90284508596,-1038.02324878726,-442.033568384102,-499.027511288761,-443.822468965725,16735.9909808734,67.3341905862767,-189.006796546968,-432.275690601863,-1557.52250871349,-838.544728485411,-252.249682422645,-737.826652866634,-914.959418759438,-884.52521405699,-699.718741028778,-1531.72950749062,-664.541092690685,-189.562796546968,-741.404083151854,-1349.60262615873,-693.386120146081,-387.804914584638,-826.896535287045,-1108.83551031424,-745.994750846805,-478.024898887792,-364.807492050718,-1309.16907944847,-241.059658766796,-516.088293490186,11587.8603010242,-938.896810272773,-1089.59547811426,516.90512356317,-211.530320187307,14257.0397157496,3875.86341226283,-1971.4171332944,-293.119337664407,186.178458810755,-812.039778157851,43.1967593313657,-725.978883562773,314.191336495951,-1020.05304791368,-1018.397174133,-1230.58334056061,-923.488425507269,-206.322216716055,14836.506636078,-1156.96702894438,-914.597768840076,-1266.22282907153,-632.500519165698,-1556.36669025607,-282.964617493845,-512.550192151025,-1347.71425127897,19.5674783858667,-488.456637490524,-532.143219107114,-1012.05218233062,-104.062123364966,-1212.32022903771,-120.250240677897,-1084.79113764454,-620.882156794403,136.521664733659,-1175.3865312323,-229.579594716033,-730.538419795329,9087.55011919952,-290.766328712811,13682.8364774795,-1367.11256404613,131.918320950323,-491.83622891452,172.58588479824,-161.314058819569,-665.737158187845,-1459.35424436821,-135.139800021246,-1745.89624219914,-52.2766259278251,528.317876464553,-318.739453299797,-206.737440092335,-715.755614900831,-511.282084148733,-925.349584204841,-961.382813943577,-776.757414875316,-982.541502990347,-1308.3291535601,-1120.1848119651,-271.895676021659,539.238964873791,15765.235406239,-978.920710768336,-95.3123198050644,-445.884874425795,-98.4091044249517,-983.708397936819,-486.430387347862,-662.698381734972,12483.2458632989,-100.634486318195,23496.4629312962,-579.585257359172,-1366.74477791316,-880.483090145364,136.916002460091,-869.632988496947,-631.362709349541,180.949020693814,16017.4165304662,-989.115460874464,-1239.17838050253,-28.6927859825091,-685.317753667974,-728.137358275964,-217.945051653008,-100.830546113186,8849.71320341624,-1148.4343694331,-1029.95396393204,7952.25535406233,-503.287510573287],"y":[33.77,28.88,25.74,30.78,23.845,28.6,26.6,35.625,24.7,25.935,36.19,28.5,28.1,32.01,34.01,26.885,30.8,38.28,31.6,32.205,23.37,23.75,28.975,35.9,29.7,39.6,32.2,34.43,31.8,41.23,26.6,29.92,23.21,31.635,25.46,27.83,38.06,20.9,25.365,25.2,38.06,20.615,26.4,27.5,34.865,35.86,33.25,25.27,29.64,40.945,36.7,34.5,41.47,29.26,23.18,35.245,30.78,32.3,22.23,30.115,31.46,31.445,38.38,21.12,17.48,34.1,30.59,33.11,28.9,29.6,38.6,29.6,23.4,30.495,38.095,24.09,41.325,21.47,23.6,30.875,31.35,22.42,28.025,38.9,31.79,41.325,33.44,34.21,35.53,30.495,37.29,36.86,23.465,20.7,25.175,26.98,28.785,26.03,29.3,39.7,30.78,26.22,31.13,33.66,20.3,18.5,26.41,41.69,23.655,33.725,29.545,32.9,37.4,40.28,24.035,28.975,22.77,33.63,27.645,32.3,39.7,19.475,36.1,36.48,39.805,29.26,23.18,31.92,35.97,30.14,30.8,34.43,24.32,33.1,23.465,34.2,32.23,28.1,33.535,25.4,29.9,36.2,33.44,28.8,27.36,44,27.265,35.1,32.34,28.31,27.5,33.99,35.815,39.16,46.53,26.18,22.61,39.49,26.79,24.795,36.765,27.1,34.32,23.56,20.235,40.5,31.6,29.26,34.6,38.38,23,26.41,28.595,18.335,27.835,31.5,31.54,47.74,32.7,31.35,29.925,26.22,30,32.6,24.86,35.815,22.135,30.59,41.1,34.58,35.2,34.105,27.93,32.11,34.8,23.94,34.43,30.305,23.3,27.83,33.33,24.3,37.715,29.9,27.61,30.4,30.03,25.175,22,26.125,28.31,30.03,25.74,33.4,36.85],"text":["Age: 18 BMI: 33.77","Age: 32 BMI: 28.88","Age: 31 BMI: 25.74","Age: 52 BMI: 30.78","Age: 23 BMI: 23.845","Age: 19 BMI: 28.6","Age: 24 BMI: 26.6","Age: 18 BMI: 35.625","Age: 64 BMI: 24.7","Age: 28 BMI: 25.935","Age: 40 BMI: 36.19","Age: 31 BMI: 28.5","Age: 53 BMI: 28.1","Age: 58 BMI: 32.01","Age: 57 BMI: 34.01","Age: 31 BMI: 26.885","Age: 54 BMI: 30.8","Age: 55 BMI: 38.28","Age: 41 BMI: 31.6","Age: 52 BMI: 32.205","Age: 37 BMI: 23.37","Age: 18 BMI: 23.75","Age: 20 BMI: 28.975","Age: 53 BMI: 35.9","Age: 48 BMI: 29.7","Age: 54 BMI: 39.6","Age: 41 BMI: 32.2","Age: 18 BMI: 34.43","Age: 63 BMI: 31.8","Age: 40 BMI: 41.23","Age: 53 BMI: 26.6","Age: 26 BMI: 29.92","Age: 24 BMI: 23.21","Age: 41 BMI: 31.635","Age: 47 BMI: 25.46","Age: 59 BMI: 27.83","Age: 44 BMI: 38.06","Age: 19 BMI: 20.9","Age: 55 BMI: 25.365","Age: 58 BMI: 25.2","Age: 53 BMI: 38.06","Age: 19 BMI: 20.615","Age: 52 BMI: 26.4","Age: 28 BMI: 27.5","Age: 58 BMI: 34.865","Age: 49 BMI: 35.86","Age: 52 BMI: 33.25","Age: 34 BMI: 25.27","Age: 26 BMI: 29.64","Age: 57 BMI: 40.945","Age: 52 BMI: 36.7","Age: 64 BMI: 34.5","Age: 49 BMI: 41.47","Age: 34 BMI: 29.26","Age: 57 BMI: 23.18","Age: 33 BMI: 35.245","Age: 49 BMI: 30.78","Age: 56 BMI: 32.3","Age: 57 BMI: 22.23","Age: 64 BMI: 30.115","Age: 62 BMI: 31.46","Age: 48 BMI: 31.445","Age: 52 BMI: 38.38","Age: 38 BMI: 21.12","Age: 19 BMI: 17.48","Age: 52 BMI: 34.1","Age: 25 BMI: 30.59","Age: 60 BMI: 33.11","Age: 35 BMI: 28.9","Age: 40 BMI: 29.6","Age: 35 BMI: 38.6","Age: 39 BMI: 29.6","Age: 24 BMI: 23.4","Age: 57 BMI: 30.495","Age: 62 BMI: 38.095","Age: 18 BMI: 24.09","Age: 63 BMI: 41.325","Age: 54 BMI: 21.47","Age: 31 BMI: 23.6","Age: 37 BMI: 30.875","Age: 22 BMI: 31.35","Age: 51 BMI: 22.42","Age: 38 BMI: 28.025","Age: 33 BMI: 38.9","Age: 20 BMI: 31.79","Age: 25 BMI: 41.325","Age: 46 BMI: 33.44","Age: 34 BMI: 34.21","Age: 19 BMI: 35.53","Age: 19 BMI: 30.495","Age: 18 BMI: 37.29","Age: 62 BMI: 36.86","Age: 25 BMI: 23.465","Age: 19 BMI: 20.7","Age: 27 BMI: 25.175","Age: 28 BMI: 26.98","Age: 59 BMI: 28.785","Age: 29 BMI: 26.03","Age: 24 BMI: 29.3","Age: 51 BMI: 39.7","Age: 48 BMI: 30.78","Age: 39 BMI: 26.22","Age: 48 BMI: 31.13","Age: 18 BMI: 33.66","Age: 19 BMI: 20.3","Age: 33 BMI: 18.5","Age: 53 BMI: 26.41","Age: 40 BMI: 41.69","Age: 24 BMI: 23.655","Age: 56 BMI: 33.725","Age: 47 BMI: 29.545","Age: 33 BMI: 32.9","Age: 62 BMI: 37.4","Age: 18 BMI: 40.28","Age: 45 BMI: 24.035","Age: 21 BMI: 28.975","Age: 58 BMI: 22.77","Age: 24 BMI: 33.63","Age: 31 BMI: 27.645","Age: 47 BMI: 32.3","Age: 64 BMI: 39.7","Age: 38 BMI: 19.475","Age: 61 BMI: 36.1","Age: 44 BMI: 36.48","Age: 45 BMI: 39.805","Age: 31 BMI: 29.26","Age: 22 BMI: 23.18","Age: 39 BMI: 31.92","Age: 64 BMI: 35.97","Age: 18 BMI: 30.14","Age: 46 BMI: 30.8","Age: 36 BMI: 34.43","Age: 60 BMI: 24.32","Age: 63 BMI: 33.1","Age: 35 BMI: 23.465","Age: 51 BMI: 34.2","Age: 25 BMI: 32.23","Age: 57 BMI: 28.1","Age: 55 BMI: 33.535","Age: 51 BMI: 25.4","Age: 40 BMI: 29.9","Age: 50 BMI: 36.2","Age: 33 BMI: 33.44","Age: 41 BMI: 28.8","Age: 23 BMI: 27.36","Age: 61 BMI: 44","Age: 26 BMI: 27.265","Age: 60 BMI: 35.1","Age: 44 BMI: 32.34","Age: 22 BMI: 28.31","Age: 44 BMI: 27.5","Age: 24 BMI: 33.99","Age: 34 BMI: 35.815","Age: 62 BMI: 39.16","Age: 26 BMI: 46.53","Age: 18 BMI: 26.18","Age: 19 BMI: 22.61","Age: 31 BMI: 39.49","Age: 24 BMI: 26.79","Age: 46 BMI: 24.795","Age: 52 BMI: 36.765","Age: 35 BMI: 27.1","Age: 39 BMI: 34.32","Age: 34 BMI: 23.56","Age: 22 BMI: 20.235","Age: 19 BMI: 40.5","Age: 50 BMI: 31.6","Age: 28 BMI: 29.26","Age: 21 BMI: 34.6","Age: 61 BMI: 38.38","Age: 54 BMI: 23","Age: 64 BMI: 26.41","Age: 36 BMI: 28.595","Age: 60 BMI: 18.335","Age: 32 BMI: 27.835","Age: 32 BMI: 31.5","Age: 57 BMI: 31.54","Age: 52 BMI: 47.74","Age: 23 BMI: 32.7","Age: 18 BMI: 31.35","Age: 49 BMI: 29.925","Age: 50 BMI: 26.22","Age: 26 BMI: 30","Age: 41 BMI: 32.6","Age: 52 BMI: 24.86","Age: 35 BMI: 35.815","Age: 36 BMI: 22.135","Age: 20 BMI: 30.59","Age: 32 BMI: 41.1","Age: 43 BMI: 34.58","Age: 32 BMI: 35.2","Age: 37 BMI: 34.105","Age: 19 BMI: 27.93","Age: 22 BMI: 32.11","Age: 59 BMI: 34.8","Age: 41 BMI: 23.94","Age: 56 BMI: 34.43","Age: 58 BMI: 30.305","Age: 58 BMI: 23.3","Age: 45 BMI: 27.83","Age: 18 BMI: 33.33","Age: 28 BMI: 24.3","Age: 55 BMI: 37.715","Age: 43 BMI: 29.9","Age: 35 BMI: 27.61","Age: 22 BMI: 30.4","Age: 18 BMI: 30.03","Age: 58 BMI: 25.175","Age: 20 BMI: 22","Age: 18 BMI: 26.125","Age: 18 BMI: 28.31","Age: 51 BMI: 30.03","Age: 57 BMI: 25.74","Age: 23 BMI: 33.4","Age: 18 BMI: 36.85"],"hoverinfo":["text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text"],"type":"scatter3d","mode":"markers","opacity":0.9,"name":"no","marker":{"color":"rgba(204,70,120,1)","size":[55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55],"sizemode":"area","symbol":"x","line":{"color":"rgba(204,70,120,1)"}},"textfont":{"color":"rgba(204,70,120,1)","size":55},"error_y":{"color":"rgba(204,70,120,1)","width":55},"error_x":{"color":"rgba(204,70,120,1)","width":55},"line":{"color":"rgba(204,70,120,1)","width":55},"frame":null},{"x":[31,45,20,19,42,63,56,47,53,50,44,36,63,64,61,40,24,21,39,57,54,35,48,31,18,42,50,53,27,37,47,44,34,27,30,18,20,43,22,47,48,23,18,23,43,52,52,24,18,62,32,30,62],"z":[718.439595977252,-2474.32791442279,163.862855918855,-562.445485375792,-616.081182383175,1041.55323564354,3143.71837891788,-1850.34353618616,-329.494262305605,193.887932563564,-1236.68064644067,-616.402794832837,1147.52147126736,807.364109688744,594.406661754227,-1232.82071366354,16605.0826519252,-1285.19746053166,29.6819771721894,2743.04078714739,19918.060429483,626.492913010443,423.586209927445,13979.0506891743,-3493.20571614619,11456.4007187653,-2421.89033677694,-242.84702924508,-479.029109853149,6797.69454657003,78.8573345539335,1461.587870512,-226.944800443358,-3189.22618434969,-3021.13937201994,-1018.16082160738,-649.879939689017,-1729.18011680482,11659.663655504,-1866.54167909923,-148.166383813448,-3022.11817953986,168.85072278915,-1824.65965524794,-2388.22020395241,17923.5920514336,1999.42107871245,-2993.49281159062,-3952.66077544965,2796.88985886247,-342.732420896598,-1510.46944299202,614.74905595959],"y":[36.3,22.895,28.025,28.3,24.64,27.74,33.63,25.41,22.61,32.3,20.235,22.6,26.98,33.88,35.86,32.775,28.5,25.7,28.3,42.13,47.41,34.105,40.565,38.095,33.535,28.31,27.6,20.9,28.5,47.6,38.94,30.2,27.835,20.045,22.99,21.565,30.685,24.7,52.58,36.19,25.85,31.4,27.36,28.49,25.27,34.485,41.8,29.83,21.66,30.875,28.12,23.655,26.695],"text":["Age: 31 BMI: 36.3","Age: 45 BMI: 22.895","Age: 20 BMI: 28.025","Age: 19 BMI: 28.3","Age: 42 BMI: 24.64","Age: 63 BMI: 27.74","Age: 56 BMI: 33.63","Age: 47 BMI: 25.41","Age: 53 BMI: 22.61","Age: 50 BMI: 32.3","Age: 44 BMI: 20.235","Age: 36 BMI: 22.6","Age: 63 BMI: 26.98","Age: 64 BMI: 33.88","Age: 61 BMI: 35.86","Age: 40 BMI: 32.775","Age: 24 BMI: 28.5","Age: 21 BMI: 25.7","Age: 39 BMI: 28.3","Age: 57 BMI: 42.13","Age: 54 BMI: 47.41","Age: 35 BMI: 34.105","Age: 48 BMI: 40.565","Age: 31 BMI: 38.095","Age: 18 BMI: 33.535","Age: 42 BMI: 28.31","Age: 50 BMI: 27.6","Age: 53 BMI: 20.9","Age: 27 BMI: 28.5","Age: 37 BMI: 47.6","Age: 47 BMI: 38.94","Age: 44 BMI: 30.2","Age: 34 BMI: 27.835","Age: 27 BMI: 20.045","Age: 30 BMI: 22.99","Age: 18 BMI: 21.565","Age: 20 BMI: 30.685","Age: 43 BMI: 24.7","Age: 22 BMI: 52.58","Age: 47 BMI: 36.19","Age: 48 BMI: 25.85","Age: 23 BMI: 31.4","Age: 18 BMI: 27.36","Age: 23 BMI: 28.49","Age: 43 BMI: 25.27","Age: 52 BMI: 34.485","Age: 52 BMI: 41.8","Age: 24 BMI: 29.83","Age: 18 BMI: 21.66","Age: 62 BMI: 30.875","Age: 32 BMI: 28.12","Age: 30 BMI: 23.655","Age: 62 BMI: 26.695"],"hoverinfo":["text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text","text"],"type":"scatter3d","mode":"markers","opacity":0.9,"name":"yes","marker":{"color":"rgba(13,8,135,1)","size":[55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55],"sizemode":"area","symbol":"circle","line":{"color":"rgba(13,8,135,1)"}},"textfont":{"color":"rgba(13,8,135,1)","size":55},"error_y":{"color":"rgba(13,8,135,1)","width":55},"error_x":{"color":"rgba(13,8,135,1)","width":55},"line":{"color":"rgba(13,8,135,1)","width":55},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Here one can see where the model has the most difficulties. Interesting enough the model has the biggest outliers with non smokers of either quite low or quite high age with a more or less not that extreme BMI. This could be due to chronic diseases especially for the younger ones but also some randomness that life holds. For example a car accident could cause such high charges. Of course it could be also due to not observed latent variables.</p>
<p>Which model performed the best on the test data set is the next interesting question. The results can be viewed below in tabular and graphical form.</p>
<table>
<caption>(#tab:perfIns)Performance on the test data</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">mae</th>
<th align="right">rmse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">boost_pred</td>
<td align="right">0.0849</td>
<td align="right">0.1581</td>
</tr>
<tr class="even">
<td align="left">rf_pred</td>
<td align="right">0.0820</td>
<td align="right">0.1586</td>
</tr>
<tr class="odd">
<td align="left">baseline_lm</td>
<td align="right">0.1171</td>
<td align="right">0.1877</td>
</tr>
<tr class="even">
<td align="left">intercept_pred</td>
<td align="right">0.3227</td>
<td align="right">0.4012</td>
</tr>
</tbody>
</table>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<p>Once without the intercept only model.</p>
<p><img src="boosting_methods_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<p>So overall the XGBoost model again delivered the best performance on the test data set w.r.t. the main optimization metric RMSE. The baseline linear model that takes all features without interactions into account also goes a long way and the performance w.r.t. the RMSE metric is even comparable to the tree-based models. So again the data did not leave much room to explore more complex non-linear patterns. Whether the rather small difference in performance is worth considering such a model is of course up to the use case. Again the XGBoost model displayed that it reaches a very competitive performance with minimal pre-processing, integrated feature selection and in this case the computational effort was quite small. The major pros and cons of the tree-based gradient boosting models will be shortly reviewed in the next and final section.</p>
<!--chapter:end:04-modeling.Rmd-->
</div>
</div>
</div>
<div id="conclusion" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>Now after having a solid theoretical understanding of gradient boosting methods for regression and the implementation XGBoost as well as seeing the application of such models to two data sets one can wrap up this project by discussing the major pros and cons of this approach with this specific implementation in mind.</p>
<div id="pros" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Pros</h2>
<ul>
<li><p>Minimal pre-processing</p></li>
<li><p>Flexible enough to detect complex non-linear patterns</p></li>
<li><p>Handling of missing values</p></li>
<li><p>Integrated feature selection</p></li>
<li><p>Good generalization due to lots of regularization options</p></li>
<li><p>Strong predictive power</p></li>
</ul>
</div>
<div id="cons" class="section level2" number="6.2">
<h2 number="6.2"><span class="header-section-number">6.2</span> Cons</h2>
<ul>
<li><p>Not as explainable as for example a linear model</p></li>
<li><p>Computationally demanding (especially the hyperparameter tuning)</p></li>
<li><p>Preferably lots of observations</p></li>
</ul>
<p>These pros and cons show that tree-based gradient boosting is a very powerful learning algorithm but still not suitable to any application. Due to its very robust nature and good handling of missing values and features of different scales it is one of the dominant algorithms in data mining alongside models like random forest. But in critical applications with just few observations such models should be treated with care as they are not that explainable. Still if one handles tabular data in order to perform a regression or classification task one should take the pros and cons that were mentioned above into account and in many cases a consideration of a tree-based gradient boosting model like XGBoost is advisable.</p>
<p><strong>Thanks for sticking with it!</strong></p>
<!--chapter:end:05-conclusion.Rmd-->
</div>
</div>
<div id="references" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-elements" class="csl-entry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Hastie</span>, T., <span class="smallcaps">Tibshirani</span>, R. and <span class="smallcaps">Friedman</span>, J. (2009). <em>The elements of statistical learning (12th printing)</em>. Springer New York.</div>
</div>
<div id="ref-HandsOnMLwithR" class="csl-entry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline"><span class="smallcaps">Boehmke</span>, B. and <span class="smallcaps">Greenwell</span>, B. (2019). <em>Hands-on machine learning with r</em>.</div>
</div>
<div id="ref-pacbounds" class="csl-entry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline"><span class="smallcaps">Wolf</span>, M. M. (2020). Lecture notes in mathematical foundations of supervised learning.</div>
</div>
<div id="ref-xgboostPaper" class="csl-entry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline"><span class="smallcaps">Chen</span>, T. and <span class="smallcaps">Guestrin</span>, C. (2016). XGBoost: <span>A</span> scalable tree boosting system. <em>CoRR</em> <strong>abs/1603.02754</strong>.</div>
</div>
<div id="ref-xgboost_package" class="csl-entry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span class="smallcaps">Chen</span>, T., <span class="smallcaps">He</span>, T., <span class="smallcaps">Benesty</span>, M., <span class="smallcaps">Khotilovich</span>, V., <span class="smallcaps">Tang</span>, Y., <span class="smallcaps">Cho</span>, H., <span class="smallcaps">Chen</span>, K., <span class="smallcaps">Mitchell</span>, R., <span class="smallcaps">Cano</span>, I., <span class="smallcaps">Zhou</span>, T., <span class="smallcaps">Li</span>, M., <span class="smallcaps">Xie</span>, J., <span class="smallcaps">Lin</span>, M., <span class="smallcaps">Geng</span>, Y. and <span class="smallcaps">Li</span>, Y. (2021). <em>Xgboost: Extreme gradient boosting</em>.</div>
</div>
<div id="ref-tidymodels" class="csl-entry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline"><span class="smallcaps">Kuhn</span>, M. and <span class="smallcaps">Wickham</span>, H. (2020). <em>Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles.</em></div>
</div>
<div id="ref-tidyverse" class="csl-entry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline"><span class="smallcaps">Wickham</span>, H., <span class="smallcaps">Averick</span>, M., <span class="smallcaps">Bryan</span>, J., <span class="smallcaps">Chang</span>, W., <span class="smallcaps">McGowan</span>, L. D., <span class="smallcaps">François</span>, R., <span class="smallcaps">Grolemund</span>, G., <span class="smallcaps">Hayes</span>, A., <span class="smallcaps">Henry</span>, L., <span class="smallcaps">Hester</span>, J., <span class="smallcaps">Kuhn</span>, M., <span class="smallcaps">Pedersen</span>, T. L., <span class="smallcaps">Miller</span>, E., <span class="smallcaps">Bache</span>, S. M., <span class="smallcaps">Müller</span>, K., <span class="smallcaps">Ooms</span>, J., <span class="smallcaps">Robinson</span>, D., <span class="smallcaps">Seidel</span>, D. P., <span class="smallcaps">Spinu</span>, V., <span class="smallcaps">Takahashi</span>, K., <span class="smallcaps">Vaughan</span>, D., <span class="smallcaps">Wilke</span>, C., <span class="smallcaps">Woo</span>, K. and <span class="smallcaps">Yutani</span>, H. (2019). Welcome to the <span class="nocase">tidyverse</span>. <em>Journal of Open Source Software</em> <strong>4</strong> 1686.</div>
</div>
<div id="ref-viridis" class="csl-entry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline"><span class="smallcaps">Garnier</span>, S. (2018). <em>Viridis: Default color maps from ’matplotlib’</em>.</div>
</div>
<div id="ref-ggtext" class="csl-entry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline"><span class="smallcaps">Wilke</span>, C. O. (2020). <em>Ggtext: Improved text rendering support for ’ggplot2’</em>.</div>
</div>
<div id="ref-viridisLite" class="csl-entry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline"><span class="smallcaps">Garnier</span>, S. (2018). <em>viridisLite: Default color maps from ’matplotlib’ (lite version)</em>.</div>
</div>
<div id="ref-patchwork" class="csl-entry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline"><span class="smallcaps">Pedersen</span>, T. L. (2020). <em>Patchwork: The composer of plots</em>.</div>
</div>
<div id="ref-visdat" class="csl-entry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline"><span class="smallcaps">Tierney</span>, N. (2017). Visdat: Visualising whole data frames. <em>JOSS</em> <strong>2</strong> 355.</div>
</div>
<div id="ref-lubridate" class="csl-entry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline"><span class="smallcaps">Grolemund</span>, G. and <span class="smallcaps">Wickham</span>, H. (2011). Dates and times made easy with <span class="nocase">lubridate</span>. <em>Journal of Statistical Software</em> <strong>40</strong> 1–25.</div>
</div>
<div id="ref-latexplots" class="csl-entry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline"><span class="smallcaps">Meschiari</span>, S. (2021). <em>latex2exp: Use LaTeX expressions in plots</em>.</div>
</div>
<div id="ref-ggally" class="csl-entry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline"><span class="smallcaps">Schloerke</span>, B., <span class="smallcaps">Cook</span>, D., <span class="smallcaps">Larmarange</span>, J., <span class="smallcaps">Briatte</span>, F., <span class="smallcaps">Marbach</span>, M., <span class="smallcaps">Thoen</span>, E., <span class="smallcaps">Elberg</span>, A. and <span class="smallcaps">Crowley</span>, J. (2021). <em>GGally: Extension to ’ggplot2’</em>.</div>
</div>
<div id="ref-plotly" class="csl-entry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline"><span class="smallcaps">Sievert</span>, C. (2020). <em>Interactive web-based data visualization with r, plotly, and shiny</em>. Chapman; Hall/CRC.</div>
</div>
<div id="ref-ranger_package" class="csl-entry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline"><span class="smallcaps">Wright</span>, M. N. and <span class="smallcaps">Ziegler</span>, A. (2017). <span class="nocase">ranger</span>: A fast implementation of random forests for high dimensional data in <span>C++</span> and <span>R</span>. <em>Journal of Statistical Software</em> <strong>77</strong> 1–7.</div>
</div>
<div id="ref-vipPack" class="csl-entry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline"><span class="smallcaps">Greenwell</span>, B. M. and <span class="smallcaps">Boehmke</span>, B. C. (2020). Variable importance plots—an introduction to the vip package. <em>The R Journal</em> <strong>12</strong> 343–66.</div>
</div>
<div id="ref-doParallel_package" class="csl-entry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline"><span class="smallcaps">Corporation</span>, M. and <span class="smallcaps">Weston</span>, S. (2019). <em>doParallel: Foreach parallel adaptor for the ’parallel’ package</em>.</div>
</div>
<div id="ref-kableextra_package" class="csl-entry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline"><span class="smallcaps">Zhu</span>, H. (2021). <em>kableExtra: Construct complex table with ’kable’ and pipe syntax</em>.</div>
</div>
</div>
<!--chapter:end:06-references.Rmd-->
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
