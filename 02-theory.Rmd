# Theory {#theory}

## The powerful idea of gradient boosting

As roughly mentioned in the introduction section \@ref(intro) the main idea of boosting is to sequentially build 'weak' learners that form a powerful ensemble model. It is not totally clear from which field these methods emerged but some claim that the work of Freund and Schapire with respect to PAC learning in the 1990s were instrumental for their growth.[@elements] PAC learning can be considered one field within the broader field of learning theory that tries to find generalization bounds for algorithms that are probably approximately correct (PAC).[@pacbounds] This section will first cover the general setup of gradient boosting as the most prominent method to train forward stagewise additive models. Secondly tree based gradient boosting and finally a very efficient and robust tree based gradient boosting algorithm namely XGBoost will be discussed in detail.

### Forward Stagewise Additive Modeling

In the setting of the dataset $\mathcal{D} = \{(y_i,x_i)\ | i \in [N]\}$ with $x_i \in \mathbb{R}^m$ and $y_i \in \mathbb{R}$ boosting is fitting the following additive, still quite general, model.

```{=tex}
\begin{equation}
  \hat{y_i} = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}
  (\#eq:additiveModel)
\end{equation}
```
Where $\mathcal{F}$ is the space of learning algorithms that will be narrowed down later on. Additive expansions like this are at the core of many other powerful machine learning algorithms like Neural Networks or Wavelets.[@elements]

The formulation \@ref(eq:additiveModel) leads to so called forward stagewise additive modeling which basically means that one sequentially adds $f \in \mathcal{F}$ to the current model $\phi_k$ without changing anything about the previous models.[@elements] The algorithm is shown below.

------------------------------------------------------------------------

**Algorithm 1**: Forward Stagewise Additive Modeling [@elements]

------------------------------------------------------------------------

1.  Initialize $\phi_0(x) = 0$

2.  For $k = 1$ to $K$ do:

    -   $(\beta_k,\gamma_k) = argmin_{\beta,\gamma}\sum_{i=1}^N L(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma))$
    -   $\phi_k(x) = \phi_{k-1}(x) + \beta_k f(x, \gamma_k)$

Where $\gamma$ parameterizes the learner $f \in \mathcal{F}$ and the $\beta_k$ are the expansion coefficients. $L$ should be a differentiable loss function.

------------------------------------------------------------------------

For example for the basic $L_2$ loss the expression to be minimized simplifies to the following:

$$
L_2(y_i,\phi_{k-1}(x_i) + \beta f(x_i,\gamma)) = (y_i - \phi_{k-1}(x_i) - \beta f(x_i,\gamma))^2
$$

As $y_i - \phi_{k-1}(x_i)$ is just the residual of the previous model, the next model that is added corresponds to the model that best approximates the residuals of the current model. Although the $L_2$ loss has many very nice properties like the above, it lacks robustness against outliers. Therefore two alternative losses for boosting in the regression setting are worth considering.

### Robust loss functions for regression

As the $L_2$ loss squares the residuals, observations with large absolute residuals are overly important in the minimization step. This effect can be reduced intuitively by just using the $L_1$ loss i.e. minimize over the sum over just the absolute residuals. To do this is indeed a valid approach and can reduce the influence of outliers greatly and thus make the final model more robust. Another good choice could be the **Huber** loss which tries to get the best of $L_1$ and $L_2$ loss.[@elements] 

```{=tex}
\begin{equation}
  L_{Huber}(y,f(x)) = \bigg\{\begin{array}{11}
  L_2(y,f(x)) & |y-f(x)| \leq \delta \\
  2\delta |y-f(x)| - \delta^2 & otherwise. \\
  \end{array}
  (\#eq:huberLoss)
\end{equation}
```

In Figure \@ref(fig:lossComp) is a comparison of the three different losses discussed so far.



```{r lossComp,echo=FALSE,fig.dim=c(6,4), out.width='70%', fig.align='center', fig.cap="Comparison of different regression loss functions[@elements]"}
knitr::include_graphics("_pictures/huber_loss.png")
```

These alternative loss criteria are more robust but make the fitting i.e. the minimization much more complex as they do not yield such simplifications like the $L_2$ loss.[@elements] The next step in the journey of exploring boosting is to narrow down the argument spaces of Algorithm 1 (Forward Stagewise Additive Modeling) and to specify a subset of the general space of learning algorithms. This subset will be the space of Classification and Regression Tree (CART) models and in this case as the focus is on a regression task the space of regression trees. This choice is by no means arbitrary as in practice tree based boosting algorithms have proven countless of times that they provide very robust and accurate models but still other learners might be chosen.[@elements] The next subsection will explore how one can actually fit such a Forward Stagewise Additive Model when using regression trees as the learner class.

## General gradient tree boosting

From now on there is the switch from the space of learning algorithms $\mathcal{F}$ to the space of regression trees $\mathcal{T}$. Such a regression tree can be formally expressed by:

```{=tex}
\begin{equation}
  t(x, \gamma, R) = \sum_{j=1}^J \gamma_j I(x \in R_j) \quad \text{for  } t \in \mathcal{T}
  (\#eq:treeDef)
\end{equation}
```

A nice graphical example can be found in \@ref(fig:exampleAdditiveTree).

```{r exampleAdditiveTree, echo=FALSE,fig.dim=c(6,4), out.width='70%', fig.align='center', fig.cap="Example of an additive tree ensamble[@xgboost_paper]"}
knitr::include_graphics("_pictures/boosting_easy.png")
```

With $R_j$ being $J$ distinct regions usually attained by recursive binary splitting. Moreover these regions correspond to the leafs of the tree and the number of leafs $J$ or the depth of the trees are most often hyperparameters (not trained). The $\gamma_j \in \mathbb{R}$ are the predictions for a given x if x is contained in the region $R_j$. While it is quite easy to get the $\gamma_j$ for the regions given, most often $\gamma_j = \frac{1}{|\{x \in R_j\}|} \sum_{\{x \in R_j\}} x$ , it is a much harder problem to get good distinct regions. The above mentioned recursive binary splitting is an approximation and works in a top down greedy fashion.[@elements] Having now the new space $\mathcal{T}$ for the general boosting model \@ref(eq:additiveModel) one can write down the optimization problem that has to be solved in each step of the forward stagewise process of fitting the model.

```{=tex}
\begin{equation}
  (\gamma^{(k)},R^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N L(y_i, \phi_{k-1}(x_i) + t(x_i,\gamma,R))
  (\#eq:oneStepTreeBoost)
\end{equation}
```

This can be estimated fast and quite straight forward if there is a simplification like the one seen for the $L_2$ loss. But in the more general case of an arbitrary differentiable convex loss function like the Huber loss techniques from numerical optimization are needed to derive fast algorithms.[@elements]

### Gradient boosting

So according to Algorithm 1 the goal is to minimize over the full loss of the training data $\mathcal{D}$ which is the sum over all observation losses.

$$
L(\phi) = \sum_{i=1}^N L(y_i, \phi(x_i))
$$

And thus the $\hat{\phi}$ additive boosting model we try to get is the following.

$$
\hat{\phi} = argmin_{\phi} L(\phi)
$$

Here $\phi$ can be viewed as a vector of dimension $N$ that contains the prediction according to $\phi$ of the corresponding observation. Numerical optimization then solves for $\hat{\phi}$ by a sum of vectors of the same dimension as the $\phi$.[@elements]

```{=tex}
\begin{equation}
  \phi_K = \sum_{k=0}^K h_k \quad h_k \in \mathbb{R}^N 
  (\#eq:numOptSol)
\end{equation}
```

While $h_0$ is just an initial guess the subsequent $h_k$ are again the prediction vectors of the corresponding model out of $\mathcal{T}$ i.e. $h_{k,i} = t_k(x_i)$ with $t \in \mathcal{T}$. This means that each $\phi_k = \phi_{k-1} + h_k$. The $h_k$ are calculated via the gradient which finally comes into play. As in every kind of gradient descent based optimization one minimizes the loss the most by going towards the direction of the steepest descent. For \@ref(eq:numOptSol) follows from its additive formulation and defining $g_k$ as the gradient of $L(\phi_k)$ evaluated for $\phi_{k-1}$ the update $h_k = -\lambda_k g_k$. As we assumed the loss to be differentiable we see that $g_k$ is well defined. Here $\lambda_k$ is the usual step length for gradient descent methods. It is the solution of the line search $\lambda_k = argmin_{\lambda} L(\phi_{k-1} - \lambda g_k)$. This $\lambda_k$ almost exactly corresponds to the $\beta_k$ in Algorithm 1 although here the optimization is for every region of the tree separately.[@elements]

With these insights it is clear that the tree predictions correspond to the negative gradient $-g_k$. Of course the predictions are not independent as the prediction is constant for each leaf of the tree. So the new optimization proposed by numerical optimization via gradient boosting is given in \@ref(eq:oneStepTreeBoostnew) below.

```{=tex}
\begin{equation}
  (\tilde{\gamma}^{(k)},\tilde{R}^{(k)}) = argmin_{\gamma,R} \sum_{i=1}^N [-g_{k,i} -  t(x_i,\gamma,R)] ^2
  (\#eq:oneStepTreeBoostnew)
\end{equation}
```

In words this just means fitting a regression tree by least squares to the negative gradients that were evaluated with the current predictions. The solution regions will not exactly match the ones from \@ref(eq:oneStepTreeBoost) but should be very similar.[@elements] After having estimated the regions one estimates the parameters $\gamma$ by solving the line search \@ref(eq:gammaLineSearch).

```{=tex}
\begin{equation}
  \tilde{\gamma}_{k,j} = argmin_{\gamma_{k,j}} \sum_{x \in R_{k,j}} L(y_i,\phi_{k-1}(x_i) + \gamma_{k,j})
  (\#eq:gammaLineSearch)
\end{equation}
```

here gradient for l2 l1 and huber

then algorithm

### gradient tree boosting algorithm from elements

### problem of right sized trees

### regularisation

### Off shelf performance


reference the figure: \@ref(fig:exampleAdditiveTree)

## XGBoost a highly efficient implementation

use the whole paper

We will cite the great paper![@xgboost_paper]

for details about hyperparameters use [@HandsOnMLwithR]
