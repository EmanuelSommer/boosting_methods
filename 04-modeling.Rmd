# Let's boost the models {#modeling}


For modeling two additional packages are required.[@xgboost_package, @ranger_package]

```{r loadModelPackages}
library(xgboost)
library(ranger)
```

First one has to set a metric for the evaluation of the final performance. This will be the mean average error (MAE) which is kind of the $L_1$ norm of performance metrics.

The first thing is to set up some trivial benchmark models.

```{r}
# the trivial intercept only model:
predict_trivial_mean <- function(new_data) {
  rep(mean(burnout_train$burn_rate), nrow(new_data))
}

# the trivial scoring of mental fatigue score (if missing intercept model)
predict_trivial_mfs <- function(new_data) {
  pred <- new_data[["mental_fatigue_score"]] / 10
  pred[is.na(pred)] <- mean(burnout_train$burn_rate)
  pred
}
```

Evaluate the performance of the two models on the training data. In the end of this section they will be applied to the test data set alongside the other models.

```{r}
# intercept only model
mae_vec(truth = burnout_train$burn_rate,
        estimate = predict_trivial_mean(burnout_train))
```
```{r}
# trivial mixed model
mae_vec(truth = burnout_train$burn_rate,
        estimate = predict_trivial_mfs(burnout_train))
```




To do

1.  register parallel backend

2.  create models and workflows

3.  create resampling objects

4.  choose tuning method (racing/iterative or grid search)

5.  tune models and select best ones

6.  compare final models to test set

7.  save final model

cite [@doParallel_package]

```{r registerCLuster, eval=FALSE}
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

# Put at the end:
stopCluster(cl)
```

for 2. set engine specific arguments in the set_engine() function use tune() for parameters that should be tuned (optional tune("id_name"))

```{r modelSpec}
# Models:
lm_model <- linear_reg() %>% 
  set_engine("lm")

rf_model <- rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

boost_model <- boost_tree() %>%
  set_engine("xgboost") %>% 
  set_mode("regression")
```

```{r workflowSpec}
# Workflows:

# lm_wflow <- 
#   workflow() %>% 
#   add_model(lm_model) %>% 
#   add_recipe()
```

for 3.

```{r resamplingObjects}
# Create Resampling objects
# set.seed(2)
# ames_folds <- vfold_cv(ames_train, v = 10)
```

for fitting resampling objects without tuning:

```{r fitResamplingWithoutTuning}
# set.seed(2)
# rf_res <- rf_wflow %>% 
#   fit_resamples(resamples = ames_folds)
# # collect the metrics during resampling with
# collect_metrics()
```

for 4. and following

```{r}
# show the parameters to be tuned with the range
# dials::parameters()
# get and modify the parameters to be tuned with:
# name()
# wflow_param %>% pull_dials_object("threshold")
# parameters(ames_rec) %>% 
#  update(threshold = threshold(c(0.8, 1.0)))

# finalize for data dependent params
# updated_param <- 
#   workflow() %>% 
#   add_model(rf_spec) %>% 
#   add_recipe(pca_rec) %>% 
#   parameters() %>% 
#   finalize(ames_train)
```

grids: grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2)) space filling: grid_latin_hypercube(size = 15, original = FALSE) then tune_grid() function instead of fit_resamples

```{r tuneGrids, eval=FALSE}
roc_res <- metric_set(roc_auc)
set.seed(99)
mlp_reg_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = mlp_param %>% grid_regular(levels = 3),
    metrics = roc_res
  )

autoplot(mlp_reg_tune) + theme(legend.position = "top")

show_best(mlp_reg_tune)

# for spacefilling
mlp_sfd_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = 20,
    # Pass in the parameter object to use the appropriate range: 
    param_info = mlp_param,
    metrics = roc_res
  )

select_best()
```

```{r finalizeWorkflow, eval=FALSE}
logistic_param <- 
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <- 
  mlp_wflow %>% 
  finalize_workflow(logistic_param)

final_mlp_fit <- 
  final_mlp_wflow %>% 
  fit(cells)
```

Racing:

```{r racingTune, eval=FALSE}
library(finetune) # if used to be cited

set.seed(99)
mlp_sfd_race <-
  mlp_wflow %>%
  tune_race_anova(
    cell_folds,
    grid = 20,
    param_info = mlp_param,
    metrics = roc_res,
    control = control_race(verbose_elim = TRUE)
  )
```

Here no iterative search as big parameter space, could be done after grid search or for single parameters. (If then Simulated Annealing)
