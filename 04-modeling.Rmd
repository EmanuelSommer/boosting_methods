# Let's boost the models {#modeling}


For modeling two additional packages are required.[@xgboost_package, @ranger_package]

```{r loadModelPackages}
library(xgboost)
library(ranger)
```

For parallel computation the `doParallel` package is used.[@doParallel_package]

```{r registerCLuster, eval=FALSE}
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

# Put at the end:
stopCluster(cl)
```

ADJUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUST

To do

1.  register parallel backend

2.  create models and workflows

3.  create resampling objects

4.  choose tuning method (racing/iterative or grid search)

5.  tune models and select best ones

6.  compare final models to test set

7.  save final model

ADJUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUST


## Burnout data

**Final metrics to be determined!!**

First one has to set a metric for the evaluation of the final performance. This will be the mean average error (MAE) which is kind of the $L_1$ norm of performance metrics.

The first thing is to set up some trivial benchmark models.

```{r}
# the trivial intercept only model:
bout_predict_trivial_mean <- function(new_data) {
  rep(mean(burnout_train$burn_rate), nrow(new_data))
}

# the trivial scoring of mental fatigue score (if missing intercept model)
bout_predict_trivial_mfs <- function(new_data) {
  pred <- new_data[["mental_fatigue_score"]] / 10
  pred[is.na(pred)] <- mean(burnout_train$burn_rate)
  pred
}
```

Evaluate the performance of the two models on the training data. In the end of this section they will be applied to the test data set alongside the other models.

```{r}
# intercept only model
mae_vec(truth = burnout_train$burn_rate,
        estimate = bout_predict_trivial_mean(burnout_train))
```
```{r}
# trivial mixed model
mae_vec(truth = burnout_train$burn_rate,
        estimate = bout_predict_trivial_mfs(burnout_train))
```




```{r modelSpecburn}
# Models:

# Random forest model for comparison
bout_rf_model <- rand_forest(trees = tune(),
                             mtry = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

bout_boost_model <- boost_tree(trees = tune(),
                               learn_rate = tune(),
                               loss_reduction = tune(),
                               tree_depth = tune(),
                               mtry = tune(),
                               sample_size = tune(),
                               stop_iter = 10) %>%
  set_engine("xgboost") %>% 
  set_mode("regression")
```

```{r workflowSpecburn}
# Workflows:

bout_rf_wflow <-
  workflow() %>%
  add_model(bout_rf_model) %>%
  add_recipe(burnout_rec_rf)

bout_boost_wflow <-
  workflow() %>%
  add_model(bout_boost_model) %>%
  add_recipe(burnout_rec_boost)
```



```{r resamplingObjects}
# Create Resampling objects
set.seed(2)
burnout_folds <- vfold_cv(burnout_train, v = 10)
```

```{r}
# Have a look at the hyperparameters that have to be tuned and finalize them
bout_rf_params <- bout_rf_wflow %>%
  parameters()

bout_rf_params 
```

This shows that the mtry hyperparameter has to be adjusted depending on the data. Moreover with the `dials::update` function one can manually set the ranges that should be used for tuning.

```{r}
# default range for tuning is 1 up to 2000 for the trees argument
# set the lower bound of the range to 200. Then finalize the
# parameters using the training data.
bout_rf_params <- bout_rf_params %>%
  update(trees = trees(c(200, 2000))) %>%
  finalize(burnout_train)
bout_rf_params
bout_rf_params %>% pull_dials_object("mtry")
```

Now this parameter object is ready for tuning and the same steps have to be performed on the boosting workflow.

```{r}
bout_boost_params <- bout_boost_wflow %>%
  parameters()

bout_boost_params 
```



for fitting resampling objects without tuning:

```{r fitResamplingWithoutTuning}
# set.seed(2)
# rf_res <- rf_wflow %>% 
#   fit_resamples(resamples = ames_folds)
# # collect the metrics during resampling with
# collect_metrics()
```

for 4. and following

```{r}
# show the parameters to be tuned with the range
# dials::parameters()
# get and modify the parameters to be tuned with:
# name()
# wflow_param %>% pull_dials_object("threshold")
# parameters(ames_rec) %>% 
#  update(threshold = threshold(c(0.8, 1.0)))

# finalize for data dependent params
# updated_param <- 
#   workflow() %>% 
#   add_model(rf_spec) %>% 
#   add_recipe(pca_rec) %>% 
#   parameters() %>% 
#   finalize(ames_train)
```

grids: grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2)) space filling: grid_latin_hypercube(size = 15, original = FALSE) then tune_grid() function instead of fit_resamples

```{r tuneGrids, eval=FALSE}
roc_res <- metric_set(roc_auc)
set.seed(99)
mlp_reg_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = mlp_param %>% grid_regular(levels = 3),
    metrics = roc_res
  )

autoplot(mlp_reg_tune) + theme(legend.position = "top")

show_best(mlp_reg_tune)

# for spacefilling
mlp_sfd_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = 20,
    # Pass in the parameter object to use the appropriate range: 
    param_info = mlp_param,
    metrics = roc_res
  )

select_best()
```

```{r finalizeWorkflow, eval=FALSE}
logistic_param <- 
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <- 
  mlp_wflow %>% 
  finalize_workflow(logistic_param)

final_mlp_fit <- 
  final_mlp_wflow %>% 
  fit(cells)
```

Racing:

```{r racingTune, eval=FALSE}
library(finetune) # if used to be cited

set.seed(99)
mlp_sfd_race <-
  mlp_wflow %>%
  tune_race_anova(
    cell_folds,
    grid = 20,
    param_info = mlp_param,
    metrics = roc_res,
    control = control_race(verbose_elim = TRUE)
  )
```

Here no iterative search as big parameter space, could be done after grid search or for single parameters. (If then Simulated Annealing)
