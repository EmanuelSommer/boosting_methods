# Let's boost the models {#modeling}

Beside the XGBoost model also a random forest model will be fitted here for comparison. 

For the modeling three additional packages are required.[@xgboost_package, @ranger_package, @vipPack]

```{r loadModelPackages, message=FALSE, warning=FALSE}
library(xgboost) # for the xgboost model
library(ranger) # for the random forest model
# + the vip package but it will not be loaded into
# the namespace
```

For parallel computations the `doParallel` package is used.[@doParallel_package]
This is most useful for the tuning part.

```{r registerCLuster, eval=TRUE, warning=FALSE, message=FALSE}
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)


```

First one has to set a **metric** for the evaluation of the final performance. This will be the mean average error (MAE) which is kind of the $L_1$ norm of performance metrics. Besides also the root mean squared error (RSME) will be covered but the optimization and tuning will focus on the more robust MAE.



ADJUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUST

To do

1.  register parallel backend DONE

2.  create models and workflows

3.  create resampling objects

4.  choose tuning method (racing/iterative or grid search)

5.  tune models and select best ones

6.  compare final models to test set

7.  save final model

ADJUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUST


## Burnout data

### Baseline models

The first thing is to set up the trivial **baseline models**.

```{r}
# the trivial intercept only model:
bout_predict_trivial_mean <- function(new_data) {
  rep(mean(burnout_train$burn_rate), nrow(new_data))
}

# the trivial scoring of mental fatigue score (if missing intercept model)
bout_predict_trivial_mfs <- function(new_data) {
  # these two scoring parameters are correspondint to the 
  # simple linear regression model containing only one predictor
  # i.e. mfs
  pred <- new_data[["mental_fatigue_score"]] * 0.097 - 0.1
  pred[is.na(pred)] <- mean(burnout_train$burn_rate)
  pred
}

```

The predictions of these baseline models on the test data set will be compared with the predictions of the tree-based models that will be constructed.

### Model specification

```{r modelSpecburn}
# Models:

# Random forest model for comparison
bout_rf_model <- rand_forest(trees = tune(),
                             mtry = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

bout_boost_model <- boost_tree(trees = tune(),
                               learn_rate = tune(),
                               loss_reduction = tune(),
                               tree_depth = tune(),
                               mtry = tune(),
                               sample_size = tune(),
                               stop_iter = 10) %>%
  set_engine("xgboost") %>% 
  set_mode("regression")
```

```{r workflowSpecburn}
# Workflows: model + recipe

bout_rf_wflow <-
  workflow() %>%
  add_model(bout_rf_model) %>%
  add_recipe(burnout_rec_rf)

bout_boost_wflow <-
  workflow() %>%
  add_model(bout_boost_model) %>%
  add_recipe(burnout_rec_boost)
```



### Tuning

For the hyperparameter tuning one needs validation sets to mointor the models on unseen data. To do this 5-fold cross validation (CV) is used here.

```{r resamplingObjects}
# Create Resampling object
set.seed(2)
burnout_folds <- vfold_cv(burnout_train, v = 5)
```

Now adjust or check the hyperparameter ranges that the tuning will use. First the Random forest model.

```{r}
# Have a look at the hyperparameters that have to be tuned and finalize them
bout_rf_params <- bout_rf_wflow %>%
  parameters()

bout_rf_params 
```

This shows that the mtry hyperparameter has to be adjusted depending on the data. Moreover with the `dials::update` function one can manually set the ranges that should be used for tuning.

```{r}
# default range for tuning is 1 up to 2000 for the trees argument
# set the lower bound of the range to 100. Then finalize the
# parameters using the training data.
bout_rf_params <- bout_rf_params %>%
  update(trees = trees(c(100, 2000))) %>%
  finalize(burnout_train)
bout_rf_params
bout_rf_params %>% pull_dials_object("mtry")
bout_rf_params %>% pull_dials_object("trees")
```

Now this parameter object is ready for tuning and the same steps have to be performed on the boosting workflow.

```{r}
bout_boost_params <- bout_boost_wflow %>%
  parameters()

bout_boost_params 
```

```{r}
# first a look at the default ranges
trees()
tree_depth()
learn_rate()
loss_reduction()
sample_size()
```

So `sample_size` must also be finalized. Again the lower bound on the number of trees will be raised to 100. The other scales are really sensible and will be left as is.

```{r}
bout_boost_params <- bout_boost_params %>%
  update(trees = trees(c(100, 2000))) %>%
  finalize(burnout_train)

bout_boost_params
bout_boost_params %>%
  pull_dials_object("sample_size")
bout_boost_params %>%
  pull_dials_object("mtry")
```
Now also this parameter object is ready to be used.


```{r}
# define a metrics set used for evaluation of the hyperparameters
regr_metrics <- metric_set(mae, rmse)
```

Here the actual tuning begins. Again the **random forest model** goes first.

```{r tuneRFburnout, cache=TRUE, eval=FALSE}
# took roughly 30 minutes
system.time({
  set.seed(2)
  bout_rf_tune <- bout_rf_wflow %>%
    tune_grid(
      resamples =  burnout_folds,
      grid = bout_rf_params %>%
        grid_latin_hypercube(size = 30, original = FALSE),
      metrics = regr_metrics
    )
})

# visualization of the tuning results (snapshot of the output below)
autoplot(bout_rf_tune) + theme_light()
# this functions shows the best combinations wrt the mae metric of all the
# combinations in the grid
show_best(bout_rf_tune, metric = "mae")
```


```{r rfburntuneplot,echo=FALSE,fig.height=4, out.width='70%', fig.align='center', fig.cap="Result of a spacefilling grid search for the random forest model."}
knitr::include_graphics("_pictures/rf_burn_tune_plot.png")
```

The visualization alongside the best performing results suggest that a value of `mtry`of 3 and 1000 `trees` should suffice. Thus one can finalize and fit this model.

```{r}
final_bout_rf_wflow <- 
  bout_rf_wflow %>% 
  finalize_workflow(tibble(
    trees = 1000,
    mtry = 3
  )) %>%
  fit(burnout_train)
```

Now the **boosting model**.

First tune only the number of trees in order to detect a number of
trees that is large enough. Then tune the tree specific arguments.
If one tunes all parameters at the same time the grid grows to large.

```{r}
# tuning grid just for the #trees
first_grid_boost_burn <- crossing(
  trees = seq(250, 2000, 250),
  mtry = 9,
  tree_depth = 6,
  loss_reduction = 0.000001,
  learn_rate = 0.01,
  sample_size = 1
)
```



```{r tuneBoostBurnfirst, eval=FALSE, cache=TRUE}
# took roughly 1 minute
system.time({
  set.seed(2)
  bout_boost_tune_first <- bout_boost_wflow %>%
    tune_grid(
      resamples =  burnout_folds,
      grid = first_grid_boost_burn,
      metrics = regr_metrics
    )
})

# plot output is shown in the figure below
autoplot(bout_boost_tune_first) + theme_light()
show_best(bout_boost_tune_first)
```

```{r boostburntuneplot1,echo=FALSE,fig.height=4, out.width='70%', fig.align='center', fig.cap="Result of the first grid search for the XGBoost model."}
knitr::include_graphics("_pictures/burn_boost_tune_first.png")
```

So 1500 trees should suffice here.

```{r}
# fix the number of trees by redefining the boosted model with a 
# fixed number of trees.
bout_boost_model <- boost_tree(trees = 1500,
                               learn_rate = tune(),
                               loss_reduction = tune(),
                               tree_depth = tune(),
                               mtry = tune(),
                               sample_size = tune(),
                               stop_iter = 10) %>%
  set_engine("xgboost") %>% 
  set_mode("regression")

# update the workflow
bout_boost_wflow <-
  bout_boost_wflow %>%
  update_model(bout_boost_model)

bout_boost_params <- bout_boost_wflow %>%
  parameters() %>%
  finalize(burnout_train)

# reduced hyperparameter space
bout_boost_params
```


Now perform the major grid search over all the other hyperparameters.

```{r tuneBoostBurnsecond, eval=FALSE, cache=TRUE}
# now tune all the remaining hyperparameters with a large space filling grid

# took roughly 1.5 hours
system.time({
  set.seed(2)
  bout_boost_tune_second <- bout_boost_wflow %>%
    tune_grid(
      resamples = burnout_folds,
      grid = bout_boost_params %>%
        grid_latin_hypercube(
          size = 200),
      metrics = regr_metrics
    )
})

show_best(bout_boost_tune_second, metric = "mae")
```


Refine the grid i.e. the parameter space according to the results of the last grid search.

```{r tuneBoostBurnthird, eval=FALSE, cache=TRUE}
# now tune all the remaining hyperparameters with a refined space filling grid

# took roughly 2 hours
system.time({
  set.seed(2)
  bout_boost_tune_third <- bout_boost_wflow %>%
    tune_grid(
      resamples = burnout_folds,
      grid = bout_boost_params %>%
        update(
          mtry = mtry(c(5,9)),
          tree_depth = tree_depth(c(4,5)),
          learn_rate = learn_rate(c(-1.7, -1.3)),
          loss_reduction = loss_reduction(c(-8,-3)),
          sample_size = sample_prop(c(0.4, 0.9))
        ) %>%
        grid_latin_hypercube(
          size = 200),
      metrics = regr_metrics
    )
})

show_best(bout_boost_tune_third, metric = "mae")

``` 

With this final grid search one is ready to finalize the model.

No column-subsampling will be applied, the maximum tree depth will be 4, the learning rate is small but not extremely small, the loss reduction regularization effect is obviously not needed here and the sample sized used for each tree is set to 0.8.

```{r}
final_bout_boost_wflow <- 
  bout_boost_wflow %>% 
  finalize_workflow(tibble(
    mtry = 9,
    tree_depth = 4,
    learn_rate = 0.02,
    loss_reduction = 0.0000003,
    sample_size = 0.8
  )) %>%
  fit(burnout_train)
```



From here one no computational intensive task will be performed so stop the cluster.

```{r}
# stop cluster
stopCluster(cl)
```

### Evaluate and understand the model

First a visualization of the variable importance. The variable importance is calculated by measuring the frequency of appearance of each feature in the single trees.

```{r}
final_bout_boost_wflow %>%
  pull_workflow_fit() %>%
  vip::vip() + 
  theme_light() 
```

This shows that indeed the `mental_fatigue_score` is the most influential predictor followed by the `ressource_allocation` and `designation` features.
Now the really interesting part is which model performed the best on the test data set. The results can be viewed below.


```{r, echo=FALSE}
# calculate the mae an the rmse for both models (random forest and xgboost)
bout_test_perf <- burnout_test %>%
  mutate(rf_pred = predict(final_bout_rf_wflow,
                           new_data = .)[[".pred"]],
         boost_pred = predict(final_bout_boost_wflow,
                              new_data = .)[[".pred"]],
         intercept_pred = bout_predict_trivial_mean(.),
         mfs_scored_pred = bout_predict_trivial_mfs(.)) %>%
  mutate(across(ends_with("pred"), function(col) {
    case_when(
      col < 0 ~ 0,
      col > 1 ~ 1,
      TRUE ~ col
      )
  })) %>%
  select(burn_rate, rf_pred, boost_pred, 
         intercept_pred, mfs_scored_pred) %>%
  pivot_longer(-burn_rate, names_to = "model") %>%
  group_by(model) %>%
  summarise(
    mae = mae_vec(
      truth = burn_rate,
      estimate = value
    ),
    rmse = rmse_vec(
      truth = burn_rate,
      estimate = value
    )
  ) %>%
  arrange(mae)

knitr::kable(bout_test_perf)

bout_test_perf %>%
  pivot_longer(-model) %>%
  mutate(model = as.factor(model),
         model = fct_recode(model, 
                             "XGBoost" = "boost_pred",
                            "Random Forest" = "rf_pred",
                            "Baseline scaled" = "mfs_scored_pred",
                            "Intercept only" = "intercept_pred"),
         model = fct_reorder(model,-value)) %>%
  ggplot(aes(x = value, y = model, col = name)) +
  geom_point(shape = 16, size = 3) +
  geom_segment(aes(y = model, yend = model, 
                   x = 0, xend = value),
               size = 1) +
  scale_color_viridis_d(option = "C") +
  labs(x = "",y = "", title = "Performance on the test data") +
  theme_light() +
  theme(legend.position = "None") +
  facet_wrap(~name)
```

It can be observed that already the really simple baseline model that just scales the mental fatigue score (so it reflects just a single linear influence) has a really low mae. Still both random forest as well as the boosting model can further improve this metric but the huge linear influence of the mental fatigue score obviously leaves not much room for improvement. By the way the simple linear model with just this one predictor has already an $R^2$ of more than 0.92. XGBoost manages to get a slightly better performance on the test data set but this would only be nice in a machine learning competition as in real life this difference would negligible. Actually in this use case for a real life application a very easy explainable linear model might be somewhat better than a complex model like XGBoost as the difference in performance is not too big. Nevertheless in my opinion the predictor `mental_fatigue_score` should be treated with extreme care as in a real life situation the collection of this score could be as costly or hard as the one of the outcome. There might be even latent variables that are highly linearly correlated to both scores. But this data was not intended to be used in a real life application but was shared at a machine learning competition and actually many of the best submissions used XGBoost models. 

this closes this 

## Insurance data






